---
title : Hotchips 35
category : chips
tags : [chips, AI, core]
---
今年hotchips 35上包含了AI, CORE, 以及optical switch内容， 比较有意思的有
* intel的66线程的risc-v core， 结合了gpgpu的多线程和risc-v的模块化，和我的一些想法不谋而合
* google和intel的optical switch， 这个很大可能会成为未来scale out首选
* samsung和hynix的PIM/PNM看起来很美好， 但是看起来在商业上是无解的， 不可能计算和存储的利润都被垄断
* AI未来发展应该的专有的更专有， 通用的更通用， 不会存在即通用又专有的空间。 
* AI不能没有大模型， 就像鱼不能没有水。人人都是大模型专家
* ARM的CSS直接把做ARM server的门槛降到零了， 就差直接喊快来买， 不会我帮你。卷的飞起。 阿里得要版权才行, 不然亏死
* intel的chiplet看起来比AMD的分配更好， 计算能力增加了, 对应的memory带宽也需要对应提升， 不可能出现复用。至于PCIE和Slow IO倒是可以复用
* RISC-V的发展迅猛， SiFive的IP看起来就是按照ARM来的
* scale, 还是scale, 不管是scale up还是scale out


# Moffett Antoum AI Inference Accelerator
这家公司中文名叫墨芯， 就是去年闹得沸沸扬扬的拳打脚踢NVIDIA的那家， 主打稀疏计算。 正常开场， 一般先讲一下AI发展路径，然后转到大模型的需求。大家套路都一样。
![Pasted image 20230830210446.png](/assets/images/hc35/Pasted image 20230830210446.png)
大家都知道的， 大模型相比以前视觉类任务, 参数量更大
![Pasted image 20230830210635.png](/assets/images/hc35/Pasted image 20230830210635.png)
不论如何，反正我家产品都是很适合的 哈哈哈 
![Pasted image 20230830210700.png](/assets/images/hc35/Pasted image 20230830210700.png)
主打的是稀疏计算
![Pasted image 20230830210815.png](/assets/images/hc35/Pasted image 20230830210815.png)
![Pasted image 20230830210842.png](/assets/images/hc35/Pasted image 20230830210842.png)
开发环境看起来比较完备， 可以用simulator来评估
![Pasted image 20230830210912.png](/assets/images/hc35/Pasted image 20230830210912.png)
SoC架构，1个scalar processor, 一个vector processor, 8个sparse processor。计算能力主要在稀疏上
![Pasted image 20230830210936.png](/assets/images/hc35/Pasted image 20230830210936.png)
SPU具体特性
![Pasted image 20230830211203.png](/assets/images/hc35/Pasted image 20230830211203.png)
SPU数据流, 从ddr到global buffer, 再到weight/activation buffer， 然后送到PE里计算
![Pasted image 20230830211231.png](/assets/images/hc35/Pasted image 20230830211231.png)
vector processor很宽， 512bit， 支持int8和fp16
![Pasted image 20230830211313.png](/assets/images/hc35/Pasted image 20230830211313.png)
还有一个计算转置，池化的special function, 这个标配
![Pasted image 20230830211333.png](/assets/images/hc35/Pasted image 20230830211333.png)
4个core之间可以通信
![Pasted image 20230830211354.png](/assets/images/hc35/Pasted image 20230830211354.png)
大模型里的稀疏性, 所以很适合
![Pasted image 20230830211412.png](/assets/images/hc35/Pasted image 20230830211412.png)
70W TDP@800MHz. 非稀疏的算力的确很一般
![Pasted image 20230830211431.png](/assets/images/hc35/Pasted image 20230830211431.png)
还是主打推理， 不同规格的产品
![Pasted image 20230830211521.png](/assets/images/hc35/Pasted image 20230830211521.png)
完整工具链， 没这个都不好意思说了
![Pasted image 20230830211557.png](/assets/images/hc35/Pasted image 20230830211557.png)
![Pasted image 20230830211618.png](/assets/images/hc35/Pasted image 20230830211618.png)
![Pasted image 20230830211641.png](/assets/images/hc35/Pasted image 20230830211641.png)
当然， 多卡扩展也是必须要支持的
![Pasted image 20230830211658.png](/assets/images/hc35/Pasted image 20230830211658.png)
8x Moffett S30 上的性能
![Pasted image 20230830211754.png](/assets/images/hc35/Pasted image 20230830211754.png)
![Pasted image 20230830211831.png](/assets/images/hc35/Pasted image 20230830211831.png)

# Lightelligence Hummingbird Low-Latency Optical Connection Engine
喜闻乐见的环节，AI要的太多太多
![Pasted image 20230830212110.png](/assets/images/hc35/Pasted image 20230830212110.png)
公司介绍
![Pasted image 20230830212147.png](/assets/images/hc35/Pasted image 20230830212147.png)
性能提升两个方面， 架构创新和摩尔
![Pasted image 20230830212216.png](/assets/images/hc35/Pasted image 20230830212216.png)
![Pasted image 20230830212300.png](/assets/images/hc35/Pasted image 20230830212300.png)
电信号传播的缺点，先抑后扬， 大家都这样
![Pasted image 20230830212322.png](/assets/images/hc35/Pasted image 20230830212322.png)
主题来了，用光网络来做片间互联
![Pasted image 20230830212357.png](/assets/images/hc35/Pasted image 20230830212357.png)
光网络优势，解放之前的拓扑的限制。优势在我
![Pasted image 20230830212423.png](/assets/images/hc35/Pasted image 20230830212423.png)
![Pasted image 20230830212439.png](/assets/images/hc35/Pasted image 20230830212439.png)
FPGA和激光组成的测试板. 可以做all-to-all broadcast.
![Pasted image 20230830212506.png](/assets/images/hc35/Pasted image 20230830212506.png)
光和电结合的方法
![Pasted image 20230830212536.png](/assets/images/hc35/Pasted image 20230830212536.png)
测试电路的架构
![Pasted image 20230830212654.png](/assets/images/hc35/Pasted image 20230830212654.png)
感觉偏题了， 计算core的微架构, 我不关心这个啊 
![Pasted image 20230830212713.png](/assets/images/hc35/Pasted image 20230830212713.png)
一些设计指标, 系统结构和性能数据
![Pasted image 20230830212735.png](/assets/images/hc35/Pasted image 20230830212735.png)
![Pasted image 20230830212753.png](/assets/images/hc35/Pasted image 20230830212753.png)
![Pasted image 20230830212810.png](/assets/images/hc35/Pasted image 20230830212810.png)
未来使用场景一些展望, 或者野望
![Pasted image 20230830212835.png](/assets/images/hc35/Pasted image 20230830212835.png)
![Pasted image 20230830212855.png](/assets/images/hc35/Pasted image 20230830212855.png)

# SiFive P870 RISC-V Processor
SiFive还是主导了RISC-V发展
![Pasted image 20230830172004.png](/assets/images/hc35/Pasted image 20230830172004.png)
SiFive的产品路线图
![Pasted image 20230830172230.png](/assets/images/hc35/Pasted image 20230830172230.png)
大芯片架构， 不过怎么L2还是4个core共享呢
![Pasted image 20230830172407.png](/assets/images/hc35/Pasted image 20230830172407.png)
pipeline
![Pasted image 20230830172436.png](/assets/images/hc35/Pasted image 20230830172436.png)
core指令流
![Pasted image 20230830172751.png](/assets/images/hc35/Pasted image 20230830172751.png)
从上往下分别介绍不同部分
![Pasted image 20230830172940.png](/assets/images/hc35/Pasted image 20230830172940.png)
![Pasted image 20230830173113.png](/assets/images/hc35/Pasted image 20230830173113.png)
![Pasted image 20230830173345.png](/assets/images/hc35/Pasted image 20230830173345.png)
![Pasted image 20230830173443.png](/assets/images/hc35/Pasted image 20230830173443.png)
![Pasted image 20230830173503.png](/assets/images/hc35/Pasted image 20230830173503.png)
![Pasted image 20230830173520.png](/assets/images/hc35/Pasted image 20230830173520.png)
共享的L2 cache, 感觉这块有点没跟上业界节奏
![Pasted image 20230830173556.png](/assets/images/hc35/Pasted image 20230830173556.png)
32-core一个示例
![Pasted image 20230830173628.png](/assets/images/hc35/Pasted image 20230830173628.png)
这是针对消费市场的大小核例子
![Pasted image 20230830173724.png](/assets/images/hc35/Pasted image 20230830173724.png)
针对汽车电子的，强调RAS
![Pasted image 20230830173753.png](/assets/images/hc35/Pasted image 20230830173753.png)
这IP真不是按照ARM的来的吗
![Pasted image 20230830174154.png](/assets/images/hc35/Pasted image 20230830174154.png)


# Ventana Veyron V1 RISC-V Data Center Processor
每个cpu die 16个 core, 要scale到192 core， 需要12个cpu die， 这IO die能同意吗
![Pasted image 20230830135727.png](/assets/images/hc35/Pasted image 20230830135727.png)
CPU die的一些特性
![Pasted image 20230830135854.png](/assets/images/hc35/Pasted image 20230830135854.png)
瞄准服务器市场， risc-v本身标准不完善，加了一些扩展。感觉这些都是坑
![Pasted image 20230830135926.png](/assets/images/hc35/Pasted image 20230830135926.png)
core microarchitecture.
![Pasted image 20230830140108.png](/assets/images/hc35/Pasted image 20230830140108.png)
![Pasted image 20230830140259.png](/assets/images/hc35/Pasted image 20230830140259.png)
pipeline
![Pasted image 20230830140403.png](/assets/images/hc35/Pasted image 20230830140403.png)
predict, fetch, 和decode 
![Pasted image 20230830140428.png](/assets/images/hc35/Pasted image 20230830140428.png)
load/ store
![Pasted image 20230830140847.png](/assets/images/hc35/Pasted image 20230830140847.png)
each 16 core cluster共享48MB L3 cache, 这个比arm还是大
![Pasted image 20230830141049.png](/assets/images/hc35/Pasted image 20230830141049.png)
![Pasted image 20230830141238.png](/assets/images/hc35/Pasted image 20230830141238.png)
吊打其他家上一代, 哈哈
![Pasted image 20230830141430.png](/assets/images/hc35/Pasted image 20230830141430.png)
参考实现
![Pasted image 20230830141517.png](/assets/images/hc35/Pasted image 20230830141517.png)


# Arm Neoverse V2
Neoverse的路线图, 这个很重要，要让人有信心
![Pasted image 20230830113244.png](/assets/images/hc35/Pasted image 20230830113244.png)
一些加强的部分
![Pasted image 20230830113317.png](/assets/images/hc35/Pasted image 20230830113317.png)
V2核的微架构
![Pasted image 20230830113354.png](/assets/images/hc35/Pasted image 20230830113354.png)
下面主要是各个不同部位的改进和相应的性能提升, 当然总结起来就是更大，更宽，更快, 所以更强。都是套路
![Pasted image 20230830113505.png](/assets/images/hc35/Pasted image 20230830113505.png)
![Pasted image 20230830115327.png](/assets/images/hc35/Pasted image 20230830115327.png)
![Pasted image 20230830115455.png](/assets/images/hc35/Pasted image 20230830115455.png)
![Pasted image 20230830115526.png](/assets/images/hc35/Pasted image 20230830115526.png)
![Pasted image 20230830115637.png](/assets/images/hc35/Pasted image 20230830115637.png)
![Pasted image 20230830115716.png](/assets/images/hc35/Pasted image 20230830115716.png)
![Pasted image 20230830115816.png](/assets/images/hc35/Pasted image 20230830115816.png)
V2@5nm vs V1@7nm
![Pasted image 20230830113145.png](/assets/images/hc35/Pasted image 20230830113145.png)
一整套的配合的IP
![Pasted image 20230830115930.png](/assets/images/hc35/Pasted image 20230830115930.png)
后面都是各种场景V2吊打V1, 看起来就是哥哥欺负弟弟
![Pasted image 20230830120028.png](/assets/images/hc35/Pasted image 20230830120028.png)
![Pasted image 20230830120124.png](/assets/images/hc35/Pasted image 20230830120124.png)
![Pasted image 20230830120213.png](/assets/images/hc35/Pasted image 20230830120213.png)
![Pasted image 20230830120235.png](/assets/images/hc35/Pasted image 20230830120235.png)
![Pasted image 20230830120258.png](/assets/images/hc35/Pasted image 20230830120258.png)
![Pasted image 20230830120320.png](/assets/images/hc35/Pasted image 20230830120320.png)
![Pasted image 20230830120427.png](/assets/images/hc35/Pasted image 20230830120427.png)
nvidia来了
![Pasted image 20230830120446.png](/assets/images/hc35/Pasted image 20230830120446.png)

## Arm Neoverse CSS
ARM为了赚钱， 不寒碜
![Pasted image 20230830161255.png](/assets/images/hc35/Pasted image 20230830161255.png)
哎 以后连线的活都找不到了 伤心
![Pasted image 20230830161315.png](/assets/images/hc35/Pasted image 20230830161315.png)
![Pasted image 20230830161521.png](/assets/images/hc35/Pasted image 20230830161521.png)
![Pasted image 20230830161837.png](/assets/images/hc35/Pasted image 20230830161837.png)
各种规格 应有尽有
![Pasted image 20230830162006.png](/assets/images/hc35/Pasted image 20230830162006.png)
解决方案
![Pasted image 20230830162102.png](/assets/images/hc35/Pasted image 20230830162102.png)
![Pasted image 20230830162122.png](/assets/images/hc35/Pasted image 20230830162122.png)
MSCP都安排上了，太贴心了
![Pasted image 20230830162301.png](/assets/images/hc35/Pasted image 20230830162301.png)
这个倒是常规套餐
![Pasted image 20230830162330.png](/assets/images/hc35/Pasted image 20230830162330.png)
CMN来了, 现在不用学了  我都打包给你
![Pasted image 20230830162404.png](/assets/images/hc35/Pasted image 20230830162404.png)
看， 能iEP都搞好了 还能什么呢  
![Pasted image 20230830162442.png](/assets/images/hc35/Pasted image 20230830162442.png)
阿里 : 咦  这不是倚天710吗 
![Pasted image 20230830162521.png](/assets/images/hc35/Pasted image 20230830162521.png)
看起来很眼熟
![Pasted image 20230830162554.png](/assets/images/hc35/Pasted image 20230830162554.png)
之前还是AIB， 与时俱进， 换成UCIe, 其实都是一个东西
![Pasted image 20230830162850.png](/assets/images/hc35/Pasted image 20230830162850.png)
看 很火的CXL也能支持的
![Pasted image 20230830162909.png](/assets/images/hc35/Pasted image 20230830162909.png)
![Pasted image 20230830162929.png](/assets/images/hc35/Pasted image 20230830162929.png)
给你看看floorplan
![Pasted image 20230830162949.png](/assets/images/hc35/Pasted image 20230830162949.png)
这是正经做法 
![Pasted image 20230830163036.png](/assets/images/hc35/Pasted image 20230830163036.png)
不止硬件 firmware, os都准备好了  真不来一套吗
![Pasted image 20230830163106.png](/assets/images/hc35/Pasted image 20230830163106.png)
依我看 保守了 3个月差不多 再多就是看不起人了
![Pasted image 20230830163257.png](/assets/images/hc35/Pasted image 20230830163257.png)

通篇看完， 就差直说阿里套餐 要不要

# Intel
## Intel on Changing its Xeon CPU Architecture
intel : 重核不吃香了 我知道了 我改
![Pasted image 20230830154234.png](/assets/images/hc35/Pasted image 20230830154234.png)
intel : cloud， cloud， 不就是要高吞吐， 多线程吗 我懂
![Pasted image 20230830154432.png](/assets/images/hc35/Pasted image 20230830154432.png)
这个感觉走对了 ， AMD的路子不对
![Pasted image 20230830155339.png](/assets/images/hc35/Pasted image 20230830155339.png)
intel : 我的互联不差的
![Pasted image 20230830155501.png](/assets/images/hc35/Pasted image 20230830155501.png)
计算能力要和内存带宽匹配
![Pasted image 20230830155725.png](/assets/images/hc35/Pasted image 20230830155725.png)
P-core一些更新
![Pasted image 20230830160045.png](/assets/images/hc35/Pasted image 20230830160045.png)
MCR DRAM是大家都可以用的吗
![Pasted image 20230830160239.png](/assets/images/hc35/Pasted image 20230830160239.png)
 CXL Type-3, 自家的东西 肯定是要支持的
![Pasted image 20230830160701.png](/assets/images/hc35/Pasted image 20230830160701.png)
IO die的架构
![Pasted image 20230830160823.png](/assets/images/hc35/Pasted image 20230830160823.png)
这个512M cache很猛
![Pasted image 20230830161014.png](/assets/images/hc35/Pasted image 20230830161014.png)
赶紧准备下单吧 什么 你要核多一点的？ 那看后面的吧
![Pasted image 20230830161039.png](/assets/images/hc35/Pasted image 20230830161039.png)

## Intel Xeon E-Cores for Next Gen Sierra Forest
来看看 E-core
![Pasted image 20230830163606.png](/assets/images/hc35/Pasted image 20230830163606.png)
![Pasted image 20230830163741.png](/assets/images/hc35/Pasted image 20230830163741.png)
![Pasted image 20230830163953.png](/assets/images/hc35/Pasted image 20230830163953.png)
![Pasted image 20230830164111.png](/assets/images/hc35/Pasted image 20230830164111.png)
intel : 不是说云原生吗 受够了 给你4MB L2, 看你还说我不是云原生
![Pasted image 20230830164224.png](/assets/images/hc35/Pasted image 20230830164224.png)
一些新的指令
![Pasted image 20230830164331.png](/assets/images/hc35/Pasted image 20230830164331.png)
intel : 都来看  我把HyperThread也去掉了  原不原生
![Pasted image 20230830164359.png](/assets/images/hc35/Pasted image 20230830164359.png)
intel : 这封装你学的来吗
![Pasted image 20230830164533.png](/assets/images/hc35/Pasted image 20230830164533.png)

## Direct Mesh-to-Mesh Optical Fabric
The key motivation behind this was the DARPA HIVE program for hyper-sparse data
![Pasted image 20230830204756.png](/assets/images/hc35/Pasted image 20230830204756.png)
When Intel profiled the workloads that DARPA was looking at, they found they were massively parallel. Still, they had poor cache line utilization and things like big long out-of-order pipelines were not well utilized.
![Pasted image 20230830204834.png](/assets/images/hc35/Pasted image 20230830204834.png)
 Intel has a 66-thread-per-core processor with 8 cores in a socket (528 threads?) The cache apparently is not well used due to the workload. This is a RISC ISA not x86.
 ![Pasted image 20230830205159.png](/assets/images/hc35/Pasted image 20230830205159.png)
 Here is the die architecture. Each core has multi-threaded pipelines.
 ![Pasted image 20230830205257.png](/assets/images/hc35/Pasted image 20230830205257.png)
 The high-speed I/O chips bridge the electrical to optical capabilities of the chip.
Here is the 10-port cut-through router being used.
![Pasted image 20230830205450.png](/assets/images/hc35/Pasted image 20230830205450.png)
Here is the on-die network where the routers are placed. Half of the 16 routers are there just to provide more bandwidth to the high-speed I/O. On-packaged EMIBs are being used for the physical connection layer.
![Pasted image 20230830205524.png](/assets/images/hc35/Pasted image 20230830205524.png)
Going off-die, each chip uses silicon photonics to drive its optical networking. With this, the connections between cores can happen directly between chips even if they are not in the same chassis without adding switches and NICs
![Pasted image 20230830205611.png](/assets/images/hc35/Pasted image 20230830205611.png)
These chips are being packaged as a multi-chip package with EMIB
![Pasted image 20230830205649.png](/assets/images/hc35/Pasted image 20230830205649.png)
Here is the optical performance.
![Pasted image 20230830205705.png](/assets/images/hc35/Pasted image 20230830205705.png)
In terms of power, this was done in an 8-core 75W CPU. More than half of the power here is being used by silicon photonics.
![Pasted image 20230830210111.png](/assets/images/hc35/Pasted image 20230830210111.png)
Here is the actual die photograph and confirmation that this is being done on TSMC 7nm
![Pasted image 20230830210206.png](/assets/images/hc35/Pasted image 20230830210206.png)
Here is what the package and test board looks like:
![Pasted image 20230830210228.png](/assets/images/hc35/Pasted image 20230830210228.png)
![Pasted image 20230830210244.png](/assets/images/hc35/Pasted image 20230830210244.png)



# AMD 
## AMD Siena - A Smaller EPYC for Telco and Edge
AMD Zen 4 was a big upgrade from Zen 3 with higher IPC, more clocks, and lower power
![Pasted image 20230830132822.png](/assets/images/hc35/Pasted image 20230830132822.png)
Zen 4c brought an even more compact Zen 4 core for Bergamo
AMD’s Socket SP5 strategy is to build out different chiplets and combine them with a common I/O Die.
![Pasted image 20230830132931.png](/assets/images/hc35/Pasted image 20230830132931.png)
Now, AMD is showing the fourth member of the 4th Gen AMD EPYC portfolio, Siena meant for the Telco Edge markets.
![Pasted image 20230830133535.png](/assets/images/hc35/Pasted image 20230830133535.png)
We get up to only 64 cores with 6x DDR5 DRAM channels. Siena is going to scale much lower than Genoa with a 70W to 225W TDP, albeit not as low as some of Intel’s Xeon D parts.
![Pasted image 20230830134753.png](/assets/images/hc35/Pasted image 20230830134753.png)

## AMD CCD and Memory Technology
an interesting CCD slide that shows some of the features of the I/O die
![Pasted image 20230830135029.png](/assets/images/hc35/Pasted image 20230830135029.png)
AMD also has one on its memory technologies including CXL
![Pasted image 20230830135430.png](/assets/images/hc35/Pasted image 20230830135430.png)

# Google TPUv4 and Optically Reconfigurable AI Network
![Pasted image 20230830174522.png](/assets/images/hc35/Pasted image 20230830174522.png)
We expect this week we will start hearing more about TPUv5. Google usually can do papers and presentations about one-generation old hardware
![Pasted image 20230830174641.png](/assets/images/hc35/Pasted image 20230830174641.png)
Here is the TPUv4 architecture diagram. Google builds these TPU chips not just to be a single accelerator, but to scale out and run as part of large-scale infrastructure.
![Pasted image 20230830194958.png](/assets/images/hc35/Pasted image 20230830194958.png)
Google TPUv4 versus TPUv3 stats
![Pasted image 20230830195133.png](/assets/images/hc35/Pasted image 20230830195133.png)
a SparseCore accelerator built into the TPUv4
![Pasted image 20230830195304.png](/assets/images/hc35/Pasted image 20230830195304.png)
TPUv4 SparseCore performance
![Pasted image 20230830195406.png](/assets/images/hc35/Pasted image 20230830195406.png)
The board itself has four TPUv4 chips and is liquid-cooled. Google said that they had to rework data centers and operations to change to liquid cooling, but the power savings are worth it. The valve on the right controls flow through the liquid cooling tubes. Google says it is like a fan speed controller, but for liquid.
![Pasted image 20230830195516.png](/assets/images/hc35/Pasted image 20230830195516.png)
Google has power entering from the top of rack like many data centers, but then it has a number of interconnects. Within a rack, Google can use electrical DACs, but outside of a rack, Google needs to use optical cables.
![Pasted image 20230830195647.png](/assets/images/hc35/Pasted image 20230830195647.png)
Each system has 64 racks with 4096 interconnected chips. For some sense, NVIDIA’s AI clusters at 256 nodes have half as many GPUs.
![Pasted image 20230830195728.png](/assets/images/hc35/Pasted image 20230830195728.png)
Each rack is a 4x4x4 cube (64 nodes) with optical circuit switching (OCS) between the TPUs. Within the rack, the connections are DACs. The faces of the cube are all optical.
![Pasted image 20230830195908.png](/assets/images/hc35/Pasted image 20230830195908.png)
Here is a look at the OCS. Instead of using an electrical switch, using the OCS gives a direct connection between chips. Google has internal 2D MEMS arrays, lenses, cameras and more. Avoiding all of the networking overhead allows sharing of data more efficiently.
![Pasted image 20230830200008.png](/assets/images/hc35/Pasted image 20230830200008.png)
Google said that it has over 16,000 connections and enough distance of fiber in the super pod that it can encircle the state of Rhode Island.
![Pasted image 20230830201134.png](/assets/images/hc35/Pasted image 20230830201134.png)
Because there is so much point-to-point communication, it requires a lot of fiber strands.
![Pasted image 20230830201215.png](/assets/images/hc35/Pasted image 20230830201215.png)
Beyond that each pool can be connected to larger pools
![Pasted image 20230830201236.png](/assets/images/hc35/Pasted image 20230830201236.png)
The OCS, because it is reconfigurable, can yield higher utilization of the nodes
![Pasted image 20230830201404.png](/assets/images/hc35/Pasted image 20230830201404.png)
Google can then change topologies by adjusting the optical routing.
![Pasted image 20230830201539.png](/assets/images/hc35/Pasted image 20230830201539.png)
the benefit of different topologies
![Pasted image 20230830201603.png](/assets/images/hc35/Pasted image 20230830201603.png)
This is important since Google says that the changes in model needs can drive system changes.
![Pasted image 20230830201633.png](/assets/images/hc35/Pasted image 20230830201633.png)
![Pasted image 20230830201652.png](/assets/images/hc35/Pasted image 20230830201652.png)
Google also increased the on-chip memory to 128MB to keep data access local
![Pasted image 20230830201718.png](/assets/images/hc35/Pasted image 20230830201718.png)
Here is Google’s comparison against the NVIDIA A100 on a performance-per-watt basis.
![Pasted image 20230830201748.png](/assets/images/hc35/Pasted image 20230830201748.png)
 the PaLM model training over 6144 TPUs in two pods.
 ![Pasted image 20230830201826.png](/assets/images/hc35/Pasted image 20230830201826.png)

# Cerebras Wafer-Scale Cluster
Cerebras started the presentation with a company update and that AI/ ML models are getting bigger (~40,000x in 5 years.)
![Pasted image 20230830202057.png](/assets/images/hc35/Pasted image 20230830202057.png)
![Pasted image 20230830202149.png](/assets/images/hc35/Pasted image 20230830202149.png)
![Pasted image 20230830202220.png](/assets/images/hc35/Pasted image 20230830202220.png)
Still, what models are practical to use depends on the ability to not just get gains at a chip level, but also at the cluster level.
![Pasted image 20230830202304.png](/assets/images/hc35/Pasted image 20230830202304.png)
Some of the challenges of current scale-out is just the communication needs to keep data moving to smaller compute and memory nodes.
![Pasted image 20230830202413.png](/assets/images/hc35/Pasted image 20230830202413.png)
Cerebras built a giant chip to get an order-of-magnitude improvement, but it also needs to scale out to clusters since one chip is not enough.
![Pasted image 20230830202446.png](/assets/images/hc35/Pasted image 20230830202446.png)
Traditional scale-out has challenges because it is trying to split a problem, data, and compute across so many devices.
![Pasted image 20230830202537.png](/assets/images/hc35/Pasted image 20230830202537.png)
On GPUs, that means using different types of parallelism to scale out to more compute and memory devices.
![Pasted image 20230830202704.png](/assets/images/hc35/Pasted image 20230830202704.png)
Cerebras is looking to scale cluster level memory and cluster level compute to decouple compute and memory scaling as is seen on GPUs.
![Pasted image 20230830202740.png](/assets/images/hc35/Pasted image 20230830202740.png)
Cerebras has 850,000 cores on the WSE-2 for its base
![Pasted image 20230830202819.png](/assets/images/hc35/Pasted image 20230830202819.png)
Cerebras houses the WSE-2 in a CS2 and then connects it to MemoryX. It then can stream data to the big chip.
![Pasted image 20230830202848.png](/assets/images/hc35/Pasted image 20230830202848.png)
It then has the SwarmX interconnect that does the data parallel scaling.
![Pasted image 20230830202916.png](/assets/images/hc35/Pasted image 20230830202916.png)
Weights are never stored on the wafer. They are just streamed in.
![Pasted image 20230830202936.png](/assets/images/hc35/Pasted image 20230830202936.png)
The SwarmX fabric scales weights and reduces gradients on the return.
![Pasted image 20230830203024.png](/assets/images/hc35/Pasted image 20230830203024.png)
Each MemoryX unit has 12x MemoryX nodes. States are stored in DRAM and in flash. Up to 1TB of DRAM and 500TB of flash.
![Pasted image 20230830203342.png](/assets/images/hc35/Pasted image 20230830203342.png)
MemoryX has to handle the sharding of the weights in a thoughtful way to make this work. Ordering the streaming helps perform an almost free transpose.
![Pasted image 20230830203500.png](/assets/images/hc35/Pasted image 20230830203500.png)
In MemoryX, there is a high-performance runtime in order to transfer data and perform computations.
![Pasted image 20230830203616.png](/assets/images/hc35/Pasted image 20230830203616.png)
SwarmX fabric uses 100GbE and RoCE RDMA to provide connectivity and Broadcast Reduce that happens on CPUs.
![Pasted image 20230830203851.png](/assets/images/hc35/Pasted image 20230830203851.png)
Every broadcast reduce node has 12 nodes with 6x 100GbE links. Five of them are used for a 1:4 broadcast plus a redundant link. That means 150Tbps of broadcast reduce bandwidth.
![Pasted image 20230830204042.png](/assets/images/hc35/Pasted image 20230830204042.png)
100GbE is interesting since it is now a very commoditized interconnect as compared to NVLink/ NVSwitch and InfiniBand.
![Pasted image 20230830204201.png](/assets/images/hc35/Pasted image 20230830204201.png)
 the SwarmX topology.
 ![Pasted image 20230830204255.png](/assets/images/hc35/Pasted image 20230830204255.png)
 The flexibility in the fabric can be used to effectively provision work across the cluster while supporting sub-cluster partitioning.
![Pasted image 20230830204332.png](/assets/images/hc35/Pasted image 20230830204332.png)
Cerebras was training large models on Andromeda quickly with 16x CS-2’s
![Pasted image 20230830204500.png](/assets/images/hc35/Pasted image 20230830204500.png)
programming a job for a single CS-2 scaled to 16x CS-2’s
![Pasted image 20230830204527.png](/assets/images/hc35/Pasted image 20230830204527.png)


# SK hynix AI Memory
Generative AI inference costs are enormous. It is not just the AI compute. It is also the power, interconnects, and memory that also drives a lot of the costs.
![Pasted image 20230830101721.png](/assets/images/hc35/Pasted image 20230830101721.png)
With large transformer models, memory is a major challenge. These models need a ton of data, and therefore are often memory capacity and bandwidth bound.
![Pasted image 20230830101829.png](/assets/images/hc35/Pasted image 20230830101829.png)
Here is a look at the GDDR6 memory where there are banks of memory each with its own 1GHz processing unit capable of 512GB/s of internal bandwidth.
![Pasted image 20230830102404.png](/assets/images/hc35/Pasted image 20230830102404.png)
![Pasted image 20230830102656.png](/assets/images/hc35/Pasted image 20230830102656.png)
Here is a look at the GDDR6 memory where there are banks of memory each with its own 1GHz processing unit capable of 512GB/s of internal bandwidth.
![Pasted image 20230830102731.png](/assets/images/hc35/Pasted image 20230830102731.png)
GEMV in memory for AI compute - Weight matrix data is sourced from banks while vector data comes from the global buffer.
![Pasted image 20230830103038.png](/assets/images/hc35/Pasted image 20230830103038.png)
AiM memory commands for the in-memory compute.
![Pasted image 20230830103223.png](/assets/images/hc35/Pasted image 20230830103223.png)

![Pasted image 20230830103400.png](/assets/images/hc35/Pasted image 20230830103400.png)
![Pasted image 20230830103445.png](/assets/images/hc35/Pasted image 20230830103445.png)
challenges
![Pasted image 20230830103855.png](/assets/images/hc35/Pasted image 20230830103855.png)
Here is how SK hynix is looking at doing the mapping from problems to AiM
![Pasted image 20230830104042.png](/assets/images/hc35/Pasted image 20230830104042.png)
The system architecture needs to handle scale-up and scale-out.
![Pasted image 20230830104259.png](/assets/images/hc35/Pasted image 20230830104259.png)
Key components of the AIM architecture are the AiM controller, Scalable Multicasting Interconnect, Router, Compute Unit (ALU), and an Instruction Sequencer.
![Pasted image 20230830104358.png](/assets/images/hc35/Pasted image 20230830104358.png)
SK hynix AiM uses a CISC-like instruction set to manage matrix vector accumulate function
![Pasted image 20230830104650.png](/assets/images/hc35/Pasted image 20230830104650.png)
The next step of this is optimization. With a new architecture, often there are nuances that can be exploited to get better performance.
![Pasted image 20230830104928.png](/assets/images/hc35/Pasted image 20230830104928.png)
SK hynix did not just talk about AiM in the abstract. Instead, it showed a proof of concept GDDR6 AiM solution using two FPGAs.
![Pasted image 20230830105454.png](/assets/images/hc35/Pasted image 20230830105454.png)
It also showed its software stack for AiM.
![Pasted image 20230830105518.png](/assets/images/hc35/Pasted image 20230830105518.png)
It does not sound like SK hynix is looking to sell these cards, instead, these seem like they are being used to prove out the concept.
![Pasted image 20230830105625.png](/assets/images/hc35/Pasted image 20230830105625.png)
SK hynix is still in the evaluation stage doing different types of analysis on the solution versus more traditional solutions.
![Pasted image 20230830105649.png](/assets/images/hc35/Pasted image 20230830105649.png)

# Samsung Processing in Memory Technology
One of the biggest costs in computing is moving data from different storage and memory locations to the actual compute engines.
![Pasted image 20230830105759.png](/assets/images/hc35/Pasted image 20230830105759.png)
 add more lanes or channels for different types of memory. That has its limits.
 ![Pasted image 20230830105950.png](/assets/images/hc35/Pasted image 20230830105950.png)
 CXL helps because it allows for things like re-purposing wires for PCIe to provide more memory bandwidth
 ![Pasted image 20230830110144.png](/assets/images/hc35/Pasted image 20230830110144.png)
GPT bottlenecks
![Pasted image 20230830110226.png](/assets/images/hc35/Pasted image 20230830110226.png)
GPT’s compute bound and memory bound workloads
![Pasted image 20230830110414.png](/assets/images/hc35/Pasted image 20230830110414.png)
![Pasted image 20230830110701.png](/assets/images/hc35/Pasted image 20230830110701.png)
Samsung shows how parts of the compute pipeline can be offloaded to processing-in-memory (PIM) modules.
![Pasted image 20230830110902.png](/assets/images/hc35/Pasted image 20230830110902.png)
Doing processing at the memory module, instead of the accelerator saves data movement lowering power consumption and interconnect costs.
![Pasted image 20230830111401.png](/assets/images/hc35/Pasted image 20230830111401.png)
While SK hynix was talking about GDDR6 for its solution, Samsung is showing its high-bandwidth memory HBM-PIM
![Pasted image 20230830111435.png](/assets/images/hc35/Pasted image 20230830111435.png)
Apparently, Samsung and AMD had MI100’s with HBM-PIM instead of just standard PIM so it could build a cluster so it could have what sounds like a 12-node 8-accelerator cluster to try out the new memory.
![Pasted image 20230830111518.png](/assets/images/hc35/Pasted image 20230830111518.png)
Here is how the T5-MoE model uses HBM-PIM in the cluster.
![Pasted image 20230830111603.png](/assets/images/hc35/Pasted image 20230830111603.png)
Here are the performance and energy efficiency gains.
![Pasted image 20230830112020.png](/assets/images/hc35/Pasted image 20230830112020.png)
A big part of this is also how to get the PIM modules to do useful work. That requires software work to program and utilize the PIM modules.
![Pasted image 20230830112210.png](/assets/images/hc35/Pasted image 20230830112210.png)
Samsung hopes to get this built-into standard programming modules.
![Pasted image 20230830112301.png](/assets/images/hc35/Pasted image 20230830112301.png)
Here is the OneMCC for memory-coupled computing to-be state, but this sounds like a future, rather than a current, state.
![Pasted image 20230830112408.png](/assets/images/hc35/Pasted image 20230830112408.png)
It looks like Samsung is showing off not just the HBM-PIM, but also a LPDDR-PIM. As with everything today, it needs a Generative AI label.
![Pasted image 20230830112439.png](/assets/images/hc35/Pasted image 20230830112439.png)
![Pasted image 20230830112459.png](/assets/images/hc35/Pasted image 20230830112459.png)
This LPDDR-PIM is only 102.4GB/s of internal bandwidth, but the idea is that keeping compute on the memory module means lower power by not having to transmit the data back to the CPU or xPU.
![Pasted image 20230830112534.png](/assets/images/hc35/Pasted image 20230830112534.png)
Here is the architecture with the PIM banks and DRAM banks on the module.
![Pasted image 20230830112613.png](/assets/images/hc35/Pasted image 20230830112613.png)
Here is what the performance and power analysis looks like on the possible LP5-PIM modules.
![Pasted image 20230830112744.png](/assets/images/hc35/Pasted image 20230830112744.png)
If HBM-PIM and LPDDR-PIM were not enough, Samsung is looking at putting compute onto CXL modules in the PNM-CXL.
![Pasted image 20230830112758.png](/assets/images/hc35/Pasted image 20230830112758.png)
The idea here is to not just put memory on CXL Type-3 modules. Instead, Samsung is proposing to put compute on the CXL module. This can be done either by adding a compute element to the CXL module and using standard memory or by using PIM on the modules and a more standard CXL controller.
![Pasted image 20230830112832.png](/assets/images/hc35/Pasted image 20230830112832.png)
Samsung has a concept 512GB CXL-PNM card with up to 1.1TB/s of bandwidth
![Pasted image 20230830112915.png](/assets/images/hc35/Pasted image 20230830112915.png)
Here is Samsung’s proposed CXL-PNM software stack.
![Pasted image 20230830112939.png](/assets/images/hc35/Pasted image 20230830112939.png)
Here are the expected energy savings and throughput for large-scale LLM workloads. CXL is usually going over wires also used for PCIe, so energy costs for transmitting data are very high. As a result, there are large gains by being able to avoid that data transfer.
![Pasted image 20230830113044.png](/assets/images/hc35/Pasted image 20230830113044.png)
