---
title : History of POWER | What makes POWER so powerful
categories : [history]
tags : [chip, cpu, architecture, history]
---

# 0. 概述
最近几年系统性的学习了解并梳理了近30多年的计算技术发展历史，稍有感悟。遂决定将之整理成文，目的有二，一来作为知识沉淀，二来通过整理再次审视历史，期望窥见未来发展方向。我将本系列命名为**鉴往知远**, 主要关注**计算与互联**。 本文为第一篇，主要回顾IBM POWER系列。

POWER (Performance Optimization With Enhanced RISC)架构起源于1990年IBM的RISC System/6000产品。1991年，Apple, IBM, 和Motorola一起合作开发了PowerPC架构。1997, Motorola和IBM合作将PowerPC用于嵌入式系统。2006, Freescale和IBM合作制定了POWER ISA 2.03。 2019年8月21, 开源了POWER Instruction Set Architecture (ISA)。PowerISA由必须的基础架构，4组可选特性，一组过时特性组成。OpenPOWER也允许自定义扩展。

本文通过系统性回顾整个POWER系列处理器，试图通过POWER系列处理器发展的历史脉络，来展现近30年计算架构的变迁，技术的演进，进而窥见计算技术发展的未来。

本文组织形式如下:
* 第一章简单介绍POWER指令集架构3.1B版本， 也是最新POWER10处理器使用的指令集。通过本章介绍，可以了解程序在POWER架构上运行的过程及预期结果，掌握异常处理，特权模型，以及POWER调试手段
* 第二章 简单回顾整个POWER系列处理器，总结各代处理器的面积，功耗，缓存，IO等基本内容
* 第三章，第四章分别描述POWER 1和POWER 2的整体架构，主要是了解这些古老系统结构
* 第五章具体描述POWER 3的微架构及各模块
* 第六章详细描述POWER 4微架构以及从POWER3单核演进到POWER4双核的变化
* 第七章详细描述POWER 5微架构以及从之前单线程到双线程的演进
* 第八章介绍POWER 6处理器微架构，了解从之前乱序执行变为顺序执行的取舍
* 第九章介绍POWER 7处理器微架构，以及内部缓存协议状态
* 第十章介绍POWER 8处理器微架构
* 第十一章主要介绍POWER 9处理器微架构，SMP互联
* 第十二章完整介绍POWER 10处理器微架构，SMP互联，片上加速器，中断
* 第十三章列出了主要的参考文献

# 1. POWER指令集架构
## 1.1 寄存器
* __Condition Register (CR)__ 是32寄存器，记录指令执行结果，供测试和条件分支指令使用
* __Link Register (LR)__ 是64位寄存器，保存__Branch Conditional to Link Register__ 指令跳转地址, 并且可以保存当__LK=1__ 时分支指令和__System Call Vectored__ 指令后的返回地址。
* __Count Register (CTR)__ 是64位寄存器。当执行的分支指令的__BO__ 编码时候可以作为`for`循环计数寄存器。__Count Register__ 也可以保存__Branch Conditional to Count Register__ 指令的跳转目标地址。
* __VR Save Register (VRSAVE)__ 是32位寄存器，软件作为SPR使用。
* __Fixed-Point Exception Register (XER)__ 是64位寄存器
	* 0:31 Reserved
	* 32 __Summary Overflow (SO)__ Summary Overflow置位当Overflow置位.
	* 33 __Overflow (OV)__ 指令执行溢出时Overflow置位
	* 34 __Carry (CA)__ 
	* 35:43 Reserved
	* 44 __Overflow32 (OV32)__ OV32 32位运行模式时溢出位
	* 45 __Carry32 (CA32)__ CA32 32位运行模式时溢出位
	* 46:56 Reserved
	* 57:63 指定__ Load String Indexed__  和__ Store String Indexed__  指令传输的字节数
* __FloatingPoint Status and Control Register(FPSCR)__ 控制浮点异常处理和浮点指令执行结果状态。32:55位是状态位, 56:63是控制位
![Pasted image 20230904172658.png](/assets/images/power/Pasted image 20230904172658.png)
* __Logical Partitioning Control Register (LPCR)__  __LPCR__ 控制资源的逻辑分区
* __Logical Partition Identification Register (LPIDR)__  __LPIDR__ 设置逻辑分区ID
## 1.2 计算模式
处理器提供两种执行模式, 64位模式和32位模式。 两种模式下，设置64位寄存器指令仍然影响所有64位。计算模式控制有效地址的解释, __Condition Register__ 和__XER__ 的设置, 当__LK=1__ 时__Link Register__ 被分支指令的设置 , 以及__Count Register__  被条件分支指令的使用。几乎所有指令都可以在两种模式下运行。在两种模式下，有效地址的计算都使用所有相关寄存器的64位(__General Purpose Registers__ , __Link Register__ , __Count Register__ 等) 并且产生64位结果。
## 1.3 指令格式
![Pasted image 20230904175410.png](/assets/images/power/Pasted image 20230904175410.png)

指令前缀格式
前缀指令由4字节前缀和4字节后缀组成。所有前缀的0:5都是*0b000001*.

### 1.3.1 分支指令
分支指令按照下面5种方式计算有效地址(EA):
1. 将分支指令地址加上位移 (当分支或条件分支的__AA=0__ 时).
2. 使用绝对地址 (当分支或条件分支的__AA=1__ 时).
3. 使用__Link Register__ 里的地址(__Branch Conditional to Link Register__ ).
4. 使用__Count Register__ 里的地址 (__Branch Conditional to Count Register__ ).
5. 使用__Target Address Register__ 里的地址 (__Branch Conditional to Target Address Register__ ).
![Pasted image 20230905092520.png](/assets/images/power/Pasted image 20230905092520.png)
![Pasted image 20230905092500.png](/assets/images/power/Pasted image 20230905092500.png)
### 1.3.2 条件寄存器指令
这些是操作条件寄存器__CR__ 的指令
![Pasted image 20230905093726.png](/assets/images/power/Pasted image 20230905093726.png)
![Pasted image 20230905093750.png](/assets/images/power/Pasted image 20230905093750.png)
![Pasted image 20230905094031.png](/assets/images/power/Pasted image 20230905094031.png)
![Pasted image 20230905094050.png](/assets/images/power/Pasted image 20230905094050.png)
![Pasted image 20230905094419.png](/assets/images/power/Pasted image 20230905094419.png)
### 1.3.3 系统调用指令
系统调用指令主要用于切换特权模式
* 当__LEV=1__ 时，唤起hypervisor
* 当__LEV=2__ 和__SMFCTRL.E = 1__ 时, 唤起ultravisor
* 当__LEV=2__ 和__SMFCTRL.E = 0__ 时, 唤起hypervisor。但是，这种方式是编程错误
![Pasted image 20230905095119.png](/assets/images/power/Pasted image 20230905095119.png)
### 1.3.4 定点加载存储指令
* 有效地址(EA)索引的字节，半字，字，双字被加载到__RT__ 寄存器
* __RS__ 寄存器里字节，半字，字，双字被存储到有效地址(EA)索引空间
![Pasted image 20230905101900.png](/assets/images/power/Pasted image 20230905101900.png)
![Pasted image 20230905102304.png](/assets/images/power/Pasted image 20230905102304.png)
![Pasted image 20230905102532.png](/assets/images/power/Pasted image 20230905102532.png)
![Pasted image 20230905102612.png](/assets/images/power/Pasted image 20230905102612.png)
![Pasted image 20230905102630.png](/assets/images/power/Pasted image 20230905102630.png)
![Pasted image 20230905102726.png](/assets/images/power/Pasted image 20230905102726.png)
![Pasted image 20230905102804.png](/assets/images/power/Pasted image 20230905102804.png)
### 1.3.5 定点算术指令
* __addic__ , __addic__ , __subfic__ , __addc__ , __subfc__ , __adde__ , __subfe__ , __addme__ , __subfme__ , __addze__ , 和__subfze__ 指令设置__CR.CA__ , 在64位模式下反映位0的进位，在32位模式反映位32的进位
* 对于XO形式的`Multiply Low`和`Divide`指令, __CR.SO/OV/OV32__ 设置依赖计算模式, 反映__mulld__ , __divd__ , __divde__ , __divdu__ 和__divdeu__ 的64位溢出, __mullw__ , __divw__ , __divwe__ , __divwu__ 和__divweu__ 低32位的溢出.
### 1.3.6 定点比较指令
定点比较指令将寄存器__RA__ 和如下值比较
1. 符号扩展SI
2. 无符号扩展UI
3. __RB__ 寄存器的值
__cmpi__ 和__cmp__ 是有符号比较, __cmpli__ 和__cmpl__ 是无符号比较.
![Pasted image 20230905103727.png](/assets/images/power/Pasted image 20230905103727.png)
### 1.3.7 定点逻辑指令
定点逻辑指令对64位进行按位操作
![Pasted image 20230905104113.png](/assets/images/power/Pasted image 20230905104113.png)
### 1.3.8 定点旋转和移位指令
定点旋转和移位指令对通用寄存器值进行旋转和移位操作，从位0开始
![Pasted image 20230905104311.png](/assets/images/power/Pasted image 20230905104311.png)
### 1.3.9 Binary Coded Decimal (BCD) 辅助指令
Binary Coded Decimal辅助指令操作BCD(__cbcdtd__ 和__addg6s__ )和十进制浮点操作数
![Pasted image 20230905104845.png](/assets/images/power/Pasted image 20230905104845.png)

## 1.4 特权模型
__Machine State Register (MSR)__ 是64位寄存器，定义了线程的特权状态。
线程的特权状态由__MSR.S__ , __MSR.HV__ 和__MSR.PR__ 组成，意义如下:
![Pasted image 20230905144055.png](/assets/images/power/Pasted image 20230905144055.png)
__MSR.SF__ 控制线程32/64位计算模式.
## 1.5 存储模型
Storage control attributes are associated with units of storage that are multiples of the page size. Each storage access is performed according to the storage control attributes of the specified storage location, as described below. The storage control attributes are the following.
* __Write Through Required__ The store does not cause the block to be considered to be modified in the data cache.
* __Caching Inhibited__ An access to a Caching Inhibited storage location is performed in main storage.
* __Memory Coherence Required__ An access to a Memory Coherence Required storage location is performed coherently
* __Guarded__  A data access to a Guarded storage location is performed only if either
	* the access is caused by an instruction that is known to be required by the sequential execution model
	* the access is a load and the storage location is already in a cache.

These attributes have meaning only when an effective address is translated by the processor performing the storage access.
The storage model for the ordering of storage accesses is weakly consistent. This model provides an opportunity for improved performance over a model that has stronger consistency rules, but places the responsibility on the program to ensure that ordering or synchronization instructions are properly placed when storage is shared by two or more programs.

## 1.6 内存管理及虚拟化
The translation mode is selected by the __Host Radix__ bit found in the __Partition Table Entry__ . The __Host Radix__  bit indicates whether the partition is using __ HPT(Hashed Page Table)__   or __ Radix Tree__  translation. Given the overall process, __ MSR.HV/PR/IR/DR__  determine where and how the process is entered.
### 1.6.1 Ultravisor/Hypervisor Offset Real Mode Address
If __MSR.HV = 1__  and __ EA.0 = 0__ , the access is controlled by the contents of the __Ultravisor Real Mode Offset Register__  or the __Hypervisor Real Mode Offset Register__ , depending on the value of __MSR.S__ , as follows.
* When __MSR.S=1__ , bits 4:63 of the effective address for the access are ORed with the 60-bit offset represented by the contents of the __URMOR__ , and the 60-bit result is used as the real address for the access
* When __MSR.S=0__ , bits 4:63 of the effective address for the access are ORed with the 60-bit offset represented by the contents of the __HRMOR__ , and the 60-bit result is used as the real address for the access.

### 1.6.2 Segment Translation
Conversion of a 64-bit effective address to a virtual address is done by searching the Segment Lookaside Buffer (SLB) as shown. If no matching translation is found in the SLB, __ LPCR.UPRT=1__ , and either __ MSR.HV=0__  or __ LPID=0__ , the Segment Table is searched. The Segment Lookaside Buffer (SLB) specifies the mapping between Effective Segment IDs (ESIDs) and Virtual Segment IDs (VSIDs).
![Pasted image 20230905155844.png](/assets/images/power/Pasted image 20230905155844.png)
In Paravirtualized HPT mode, conversion of a 78-bit virtual address to a real address is done by searching the Page Table. The Hashed Page Table (HTAB) is a variable-sized data structure that specifies the mapping between virtual page numbers and real page numbers. The HTAB contains Page Table Entry Groups (PTEGs). A PTEG contains 8 Page Table Entries (PTEs) of 16 bytes each; each PTEG is thus 128 bytes long. PTEGs are entry points for searches of the Page Table.
Below shows Translation of 78-bit virtual address to 60-bit real address.
![Pasted image 20230906084212.png](/assets/images/power/Pasted image 20230906084212.png)
### 1.6.3 Radix Tree Translation
A __Radix Tree root descriptor (RTRD)__  specifies the size of the address being translated, the size of the root table, and its location. Below shows Four level Radix Tree walk translating a 52b EA with __ NLS=13__  in the root PDE and __ NLS=9__  in the other PDEs.
![Pasted image 20230905170905.png](/assets/images/power/Pasted image 20230905170905.png)
Radix Tree Page Directory Entry
![Pasted image 20230905171134.png](/assets/images/power/Pasted image 20230905171134.png)
![Pasted image 20230905171203.png](/assets/images/power/Pasted image 20230905171203.png)
Radix Tree Page Table Entry

#### 1.6.3.1Nested Translation
When __MSR.HV=0__  and translation is enabled, each guest real address must undergo partition-scoped translation using the hypervisor’s Radix Tree for the partition. Below shows Radix on Radix Page Table search for a 52-bit EA depicting memory reads 1-24 numbered in sequence
![Pasted image 20230905172603.png](/assets/images/power/Pasted image 20230905172603.png)
### 1.6.4 Secure Memory Protection
When __SMFCTRL.E=1__ , Secure Memory Protection is enabled. Each location in main storage has a Secure Memory property __mem.SM__ . __mem.SM=1__  indicates secure memory. __mem.SM=0__  indicates ordinary memory. Generally, only secure partitions and the ultravisor may access secure memory for explicit and implicit accesses.

## 1.7 异常和中断
Power指令集架构提供了中断机制，允许线程能够处理外部信号，错误或指令执行异常。系统复位和机器检查中断是不可覆盖的，其他中断可覆盖且处理器状态可保留。当中断发生时， __SRR0__ , __HSRR0__ 或__USRR0__ 指向正在执行且未完成的指令。
中断可分为是否是执行指令引起或其他系统异常。系统异常包括：
* System Reset
* Machine Check
* External
* Decrementer
* Directed Privileged Doorbell
* Hypervisor Decrementer
* Hypervisor Maintenance
* Hypervisor Virtualization
* Directed Hypervisor Doorbell
* Directed Ultravisor Doorbell
* Performance Monitor
其他都是指令中断

### 1.7.1 中断寄存器
根据处理器所在特权状态，可以分为:
* __Machine Status Save/Restore Registers__  中断发生时，处理器状态被保存在__Machine Status Save/Restore registers__ (__SRR0__ 和__SRR1__ )。
* __Hypervisor Machine Status Save/Restore Registers__  中断发生时，处理器状态被保存在__Hypervisor Machine Status Save/Restore registers__ (__HSRR0__  and __ HSRR1__ )。
* __Ultravisor Machine Status Save/Restore Registers__  中断发生时，处理器状态被保存在__Ultravisor Machine Status Save/Restore registers__ (__HSRR0__  and __ HSRR1__ )。

### 1.7.2 中断处理
shows all the types of interrupts and the values assigned to the __ MSR__  for each. Below shows the effective address of the interrupt vector for each interrupt type. Interrupt processing consists of saving a small part of the thread’s state in certain registers, identifying the cause of the interrupt in other registers, and continuing execution at the corresponding interrupt vector location.
1. __SRR0__ , __HSRR0__ , or __USRR0__  is loaded with an instruction address that depends on the type of interrupt;
2. Bits 33:36 and 42:47 of __SRR1__ , __HSRR1__ , or __USRR1__  are loaded with information specific to the interrupt type.
3. Bits 0:32, 37:41, and 48:63 of __SRR1__ , __HSRR1__ , or __USRR1__  are loaded with a copy of the corresponding bits of the __ MSR__ .
4. The __MSR__  is set. In particular, __MSR__  bits __IR__  and __DR__  are set as specified by __LPCR.AIL__  or __LPCR.HAIL__  as appropriate and __MSR.SF__  is set to 1, selecting 64-bit mode. The new values take effect beginning with the first instruction executed following the interrupt.
5. Instruction fetch and execution resumes, using the new __ MSR__  value, at the effective address specific to the interrupt type. An offset may be applied to get the effective addresses, as specified by __LPCR.AIL__  or __LPCR.HAIL__  as appropriate
![Pasted image 20230906141329.png](/assets/images/power/Pasted image 20230906141329.png)
## 1.8 调试
调试功能允许硬件和软件通过追踪指令流，比较数据地址，单步执行等进行调试：
* __Come From Address Register__ 
	* __Come From Address Register (CFAR)__ 是64位寄存器， 当执行__rfebb__ , __rfid__ , 或__rfscv__ 执行时，寄存器值设置为当前执行的有效地址。
* __Completed Instruction Address Breakpoint__ 
	* __ Completed Instruction Address Breakpoint__ 提供了发现完成执行特定地址指令的机制。地址比较是基于有效地址(EA)。__Completed Instruction Address Breakpoint__ 机制是由__Completed Instruction Address Breakpoint Register (CIABR)__ 控制。
* __Data Address Watchpoint__ 
	* __Data Address Watchpoint__ 提供了发现多个双字有效地址(EA)加载存储访问的机制。至少两个独立地址范围可以指定。每个__Data Address Watchpoint__ 是由一对SPRs控制：__Data Address Watchpoint Register(DAWRn)__ 和__Data Address Watchpoint Register Extension (DAWRXn)__ 

# 2. POWER处理器概述
* 1975年，IBM Thomas J. Watson Research Center发明了第一个RISC机器，801。801原始设计目标是1 IPC，研究重点是定义一个能够每周期执行多个指令，即超标量的架构。研究的结果是第二代RISC架构，称为"AMERICA architecture"
* 1986年，IBM 位于Austin, Texas的开发RT System的实验室, 基于AMERICA architecture开始开发产品。这个研究最终实现了IBM RISC System/6000* (RS/6000)，即IBM's **POWER**架构
* 1990年，IBM发布RISC System/6000, 包括9种当时工业界最快最强大的工作站。RISC System/6000使用IBM研发的精简指令集，一个新版本的IBM实现UNIX操作系统Advanced Interactive Executive (AIX)。这就是**POWER 1**
* 1991年，Apple，IBM和Motorola宣布研发一种用于个人电脑和低端工作站的精简指令集处理器，可以运行IBM AIX和Macintosh。这就是**PowerPC**
* 1993年，IBM发布了自1990年以来RISC System/6000产品线最大的更新，包括四款使用PowerPC 601处理器的工作站，和三款使用IBM多芯片微处理器**POWER2**的高端工作站
* 1999年，IBM发布了基于**POWER3**微处理器的RS/6000 SP超级计算机。**POWER3**能每秒执行20亿次运算，比使用**POWER2** 的Deep Blue计算机快2倍。Deep Blue在1997年打败象棋世界冠军Garry Kasparov
* 2001年,  **POWER4**实现单芯片双核心，共享第二级缓存，以及第三级缓存控制器
* 2004年，**POWER5**实现双线程，并集成内存控制器
* 2007年，**POWER6**实现4.5G的高频。为了实现高频从之前的乱序执行简化成了顺序执行
* 2010年，**POWER7**实现单片8核四线程，片上集成32M 3级缓存
* 2016年，**POWER8**实现单片12核八线程，片外128M 4级缓存，具备NVLink接口
* 2017年，**POWER9**增加了更多IO和带宽
* 2021年，**POWER10*实现单片15核八线程，增加互联能力

下表总结了各代POWER处理器的各方面数据，可以从中看到POWER系列的发展路径。


||POWER1|POWER2|POWER3|POWER4|POWER5|POWER6|POWER7|   |POWER8|   |POWER9|   |   |POWER10|
|Date|1990|1993|1997|2001|2004|2007|2010|2012|2014|2016|2017|2018|2020|2021|
|Technology|1.0um|0.35um|0.22um|180nm SOI|130nm SOI, 90nm|65nm SOI|45nm SOI|32nm SOI|22nm|22nm|GF 14nm HP|GF 14nm HP|GF 14nm HP|Samsung 7nm|
|Transistors|8.9M||15M|174M|276M|790M|1.2B|2.1B|1.2B||8B|8B|8B|18B|
|Area(mm^2)|12.7x12.7(ICU,FXU,FPU)  <br>11.3x11.3(DCU,SCU)|12.7x12.7 (ICU,FXU,FPU)  <br>11.7x9.55 (DCU, SCU)||267|389|341|567|567|650|650|728(25.3x28.8)|728(25.3x28.8)|728(25.3x28.8)|602|
|TDP(W)|NA|NA|NA||||||120-190|120-190|190|190|190||
|Frequency(GHz)|0.02-0.03|0.055-0.0715|0.2-0.45|1.1-1.3|1.5|4-4.5|||2.0-3.3|2.0-3.3|2.75-3.8|2.75-3.8|2.75-3.8|3.5-4|
|No. Inst Decode|3|8|4|8|8|8|6|6|8|8|||||
|No. Inst Issue|2|6|4|5|5|7|8|8|10|10|||||
|COREs|1|1|1|2|2|2(in order)|8|8|12|12|12|12|12|15|
|Threads|ST|ST|ST|ST|SMT2|SMT2|SMT4|SMT4|SMT8|SMT8|SMT8|SMT8|SMT8|SMT8|
|L1 Cache|I$: 8K 2 way  <br>D$: 64K 4 way|I$: 32K  <br>D$: 4x32K|I$: 32K 128 way 128B cacheline  <br>D$: 64K 128 way 4 banks|I$: 64K direct map  <br>D$: 32K 2 way|I$: 64K 2 way  <br>D$: 32K 4 way|I$: 64K 4 way  <br>D$: 64K 8 way|I$: 32K 4 way  <br>D$: 32K 8 way|I$: 32K 4 way  <br>D$: 32K 8 way|I$: 32K  <br>D$: 64K|I$:32KB 8-way  <br>D$:32KB 8-way (SMT4)|I$:32KB 8-way  <br>D$:32KB 8-way (SMT4)|||I$:48KB 6-way  <br>D$:32KB 8-way(SMT4)|
|L2 Cache|NA|512K-3M direct map|1M-16M offchip|3x480K 8 way|3x640K 10 way|2x4M 8 way|256K/Core 8 way|256K/Core 8 way|512K/Core 8 way|1MB 8-way inclusive|1MB 8-way inclusive|||2MB 8-way|
|L3 Cache|NA|NA|NA|8 way cache directory  <br>32 M off chip  <br>512B cache line|3x12M 12 way  <br>256B cache line|32M 16 way  <br>128B off chip|4M/Core  <br>32MB eDRAM|10M/Core  <br>80MB eDRAM|8M/Core 96M eDRAM 8 way  <br>128M offchip L4|10MB/SMT8 Core 20-way  <br>120MB  <br>eDRAM|10MB/SMT8 Core 20-way  <br>120MB  <br>eDRAM|120MB||8MB/SMT8 Core 16-way  <br>120MB|
|IO|||||PCIE G2 3GB/s|10GB/s|PCIE G2@20GB/s  <br>SMP 6x60GB/s|PCIE G2@20GB/s  <br>SMP 6x60GB/s|PCIE Gen3 x32  <br>CAPI 1.0  <br>SMP 6.4G x3|PCIe Gen3  <br>CAPI 1.0  <br>NVLINK 25GT/s 160GB/s|PCIe Gen4 x48  <br>CAPI 2.0  <br>OpenCAPI3.0  <br>NVLINK2.0  <br>25GT/s x48  <br>300GB/s|PCIe Gen4 x48  <br>CAPI 2.0  <br>OpenCAPI3.0  <br>NVLINK2.0  <br>25GT/s x48  <br>300GB/s|PCIe Gen4 x48  <br>CAPI 2.0  <br>OpenCAPI4.0  <br>NVLINK2.0  <br>25GT/s x48  <br>300GB/s|PCIe Gen5 x64  <br>PowerAXON 16 x8@32GT/s  <br>SMP Interconnect 14 x9@32GT/s|
|Memory|8-256M|128bit 64M-2G||0-16G|15GB/s|30GB/s|100GB/s|100GB/s|230GB/s 9.6G DMI|230GB/s|150GB/s|8 DMI DDR4 ports@230GB/s|16 x8@25GT/s OMI  <br>650GB/s|16 x8@32GT/s  <br>1TB/s|
|SMP(sockets)|NA||2|16|32|32|32|32|32|32|||||
|Comments|RISC architecture|SMP|64 bit|hypervisor mode|integrated memory ctrl|private L2  <br>decimal floating point  <br>vector multimedia ext|eDRAM L3 cache|on die acceleration|big data optimized|high bandwidth GPU attach|scale out  <br>direct-attach DDR4 memory|scale up  <br>memory buffers|memory attached via OMI|single chip module , 16 sockets  <br>dual chip module, 4 sockets|


# 3. POWER 1
RS/6000架构在当时一个主要特性就是集成了浮点算法单元，同时将不同功能单元独立，主要包括：
* 定点单元
* 浮点单元
* 分支单元

下图展示了RISC system/6000架构逻辑结构
![Pasted image 20230912135927.png](/assets/images/power/Pasted image 20230912135927.png)
RISC System/6000架构定义了分离的指令和数据缓存。这些缓存都是写入。指令缓存主要和分支单元耦合，而数据缓存由定点和浮点单元共享。
* 分支单元  分支单元主要负责取指，地址翻译和检查， 中断处理。除非对应定点或浮点单元上指令队列已满或对定点或浮点单元上数据存在依赖性，分支单元能不断对下一个指令进行取指，解码，并执行相应指令，或者将对应定点或浮点指令分发到对应定点或浮点单元。分支单元每周期可以至少获取3条指令，分别对应定点单元，浮点单元，及分支单元。并且每周期可以分发一个定点和浮点指令。**POWER 1**不需要分支延迟槽。
* 定点单元 定点单元除了处理所有定点算术指令外，还需要为浮点单元及自身计算数据地址。因此，定点单元需要负责调度浮点单元和数据缓存之间数据交换。浮点单元寄存器只负责接收或提供数据，因此，浮点单元加载和存储操作是消耗定点单元周期数。
* 浮点单元 浮点单元支持ANSI/IEEE Standard 754-1985. RISC System/6000浮点运算是双精度计算。因此，单精度浮点运算也会被转换为双精度进行运算。

下图展示了**POWER 1**的有效地址(EA)转换过程，32位有效地址首先经过段寄存器转换为52位虚拟地址(VA), 然后52位虚拟地址(VA)经过TLB翻译成32位真实地址(RA)：
![Pasted image 20230912141125.png](/assets/images/power/Pasted image 20230912141125.png)


# 4. Power 2
High Performance MCM Chip Set for Desktop Systems
* 4 Data Cache Unit Chips
	* 128 Kbytes of DCache
* 32 Kbyte ICache
* 512 Kbyte - 2 Mbyte L2 Cache
* 4 Word Memory interface
	* Minimum Memory Configuration of two memory cards
	* 64 Mbyte - 2048 Mbyte
* Ceramic Multi-Chip Module (MCM) CPU Package
![Pasted image 20230816174929.png](/assets/images/power/Pasted image 20230816174929.png)
Cost Reduced Chip Set for Desktop Systems
* 2 Data Cache Unit Chips
	* 64 Kbytes of DCache
* 32 Kbyte ICache
* 512 Kbyte - 1 Mbyte L2 Cache
* 2 Word Memory Interface
	* Minimum Memory Configuration of one memory card
	* 32 Mbyte - 512 Mbyte
* Single Chip Solder Ball Connect (SBC) CPU Package
![Pasted image 20230816175250.png](/assets/images/power/Pasted image 20230816175250.png)
## 4.1 Core Features
* 6 Instruction Dispatch
* 8 Operations/cycle
* Large, multi-ported Data Cache
* High bandwidth buses
* Dual Fixed Point, Floating Point, Branch Units
![Pasted image 20230816175511.png](/assets/images/power/Pasted image 20230816175511.png)
* Optimized L2 Cache Subsystem
	* 512 KB,1 MB,2 MB Second Level Cache
	* Direct-Mapped, 128 byte line
	* Store-through - Overlapped write to L2 and Memory
	* Industry standard Burst SRAM
		* 2-1-1-1 Cache Hit Read and Write Timing
	* Run at CPU clock speed
	* Single bit correct, double bit detect ECC for all L2 Cache Accesses
* Storage Control Unit L2 Cache Features
	* Programmable Second Level Cache and Main Memory Size
	* Programmable Bus Width
	* Integrated L2 Cache Tag RAM
	* Overlapped L2 Tag lookup/compare with DRAM access
	* Single cycle L2 Tag iookup
	* DRAM access never started on L2 Cache Hit
	* No Memory cycle penalty for L2 Cache Miss
	* Direct Store Segment Load/Store to L2 directory and data

![Pasted image 20230816193204.png](/assets/images/power/Pasted image 20230816193204.png)


# 5. POWER 3
POWER3是第一个支持32位和64位PowerPC ISA的处理器。下图展示了**POWER 3** 的die shot
![Pasted image 20230912085657.png](/assets/images/power/Pasted image 20230912085657.png)

POWER3由7个功能单元组成：
* Instruction processing unit (IPU)
* Instruction ﬂow unit (IFU)
* Fixed-point unit (FXU)
* Floating-point unit (FPU)
* Load/store unit (LSU)
* Data cache unit (DCU)
* Bus interface unit (BIU)

下图展示了*POWER3* 处理器功能模块图：
![Pasted image 20230912085752.png](/assets/images/power/Pasted image 20230912085752.png)

* *Instruction processing unit* 和*instruction ﬂow unit* *IPU* 和*IFU* 负责取值，缓存以及指令分发和完成整个生命流程。IPU有32KB指令缓存和*cache reload buffer(CRB)* 。 指令缓存缓存行大小是128B，因此一共有256行，组织成128路组相联，并且单周期访问。*CRB* 保存从内存读取最新缓存行。另外实现虚拟地址转换，实现了256-entry 2路组相联*instruction translation lookaside buffer (ITLB)* 和16-entry *instruction segment lookaside buffer (ISLB)* 。每周期可以取8条指令，分发4条指令，并且完成4条指令。为了调高吞吐，指令顺序分发，大部分可以乱序执行和结束，顺序完成。指令分发到不同功能单元指令队列并且由32-entry *completion queue*记录。这些功能单元指令队列确保对应功能单元有足够指令可以选择来执行，并且阻塞的指令不会阻碍IFU的指令分发。*completion queue* 确保处理器的架构状态的正确性，强制指令顺序完成和中断以及异常的正常处理。**POWER3** 采用两种机制来提高分支预测的准确性。首先，通过跟踪所有并发的带条件码的指令，处理器能够在指令分发时就确定分支结果。另外，对于在指令分发时无法确定的分支，会进行投机。当带条件码指令完成并且分支结果投机错误，分支指令之后所有指令会取消并重新分发正确指令。
* *Fixed-point execution units* **POWER3** 包含3个定点执行单元：2个单周期执行单元和一个多周期执行单元。单周期执行单元执行算术，移位，逻辑，比较，*trap* 和 *count leading zero* 指令。其他比如乘法，除法等都由多周期执行单元执行。两个单周期执行单元共享一个 *six-entry* 指令队列, 多周期执行单元使用一个*three-entry* 的指令队列。不同于**POWER 2** 包含两个执行定点和加载存储的对称的执行单元，**POWER3** 有两个专门的存储加载单元。独立的定点执行单元和存储加载单元对于类似Monte Carlo仿真这样整数操作占比大的应用很显然有比较大性能提升，但是即使是对浮点运算也很重要。像在*sparse-matrix-vector multiply* 中，整数索引必须先转换成字节偏移。
* *Floating-point execution units* *FPU* 包含两个对称的执行融合乘加流水的执行单元。所有浮点指令都需要经过乘法和加法阶段。对于浮点乘，加法阶段0作为一个操作数。对于浮点加，乘法阶段1作为一个操作数。
* *Load/store execution units* 所有的存储加载操作都由两个存储加载执行单元完成。加载指令将数据从内存转移到定点或浮点重命名寄存器，存储指令将数据从寄存器转移到内存。存储指令使用一个*16 entries store buffer* 来提高性能. 当存储指令获取到数据之后就可以完成，而不必等到写进数据缓存。两个加载存储执行单元共享一个 *six-entry* 指令队列。 *LSU* 乱序执行允许加载指令超过存储指令同时记录数据依赖性。存储指令的顺序在执行阶段和存储队列中维护。
* *Data cache unit* *DCU* 主要由数据内存管理单元*MMU* ， L1缓存和数据预取单元组成
	* *Memory management unit* *MMU* 主要负责数据的地址转换，包括一个*16-entry segment lookaside buffer (SLB)* 和两个镜像的*256-entry两路组相联*data translation lookaside buffers (DTLB) 以便支持两个存储加载执行单元。*MMU* 支持1T物理内存，64位有效地址和80位的虚拟地址。
	* *L1 data cache* L1数据缓存是单周期访问的64 KB，由4个bank，一共512个128B缓存行组成。每个bank是128路组相联。每个bank，由地址A55/A56决定，根据双字地址(A60)地址分成2个子bank。 *least-recently-used (LRU)* 替换算法不适用于128路组相联缓存，因此，**POWER3** 在L1指令缓存和L1数据缓存中采用*round-robin* 替换方案。为了适应不同位宽和频率以便重建缓存行，需要在传输路径上加入*linewidth buffers* 。L1数据缓存和BIU之间, 每个bank有一个*cache- reload buffers (CRBs)* 和一个*cache-storeback buffers (CSBs)*。 加载操作命中*CRB* 直接提供数据，不必等到缓存行加载。*CSB* 和 *BIU* 之间是64-byte 接口。L1数据缓存可以支持4个缓存缺失，可以有效掩盖内存延迟。当加载操作没有命中L1数据缓存，指令会被放入*six-entry load-miss queue(LMQ)* , 同时*BIU*发出数据加载传输， 后续加载指令可以继续执行。只有当第五个加载操作没有命中L1数据缓存，并且*LMQ* 里已经有4个未命中加载指令，加载操作才会被暂停，直到*LMQ* 里任意一个加载指令执行完成。
	* 数据预取 **POWER3**处理器一大创新是硬件数据预取功能。**POWER3**处理器根据监控到的缓存行缺失及匹配到的模式进行数据预取。当一个模式或数据流被探测到，处理器会对缓存行进行投机预取，假设数据会马上被使用。数据预取对于掩藏内存延迟非常关键。预取的数据流在地址上是连续的缓存行，要么是递增，要么是递减的。 **POWER3**处理器预取引擎包括一个*ten- entry stream ﬁlter* 和一个 *stream prefetcher* 。 *stream ﬁlter* 观察所有数据缓存缺失的真实地址(RA)，检测潜在的预取数据流。*stream ﬁlter* 根据加载指令的地址操作数，猜测下一个缓存行的真实地址是增加还是减少，并记录到*FIFO ﬁlter* 。当发生新的缓存缺失时，如果发生缓存缺失的真实地址和猜测的地址一致，这样就检测到一个数据流。如果*stream prefetcher* 当前少于4个数据预取流，当前数据流会被接受，并且预测下一个缓存行加载操作会通过*BIU* 发出。一旦数据流被放入*stream prefetcher* , 数据流保持活跃直到数据流到达终点或者有新的缓存缺失进入*stream ﬁlter*。数据预取引擎会尝试保持预取2个缓存行，前一个缓存行会被放到L1 缓存，后一个缓存行会被放到BIU里一个预取缓冲区。因此，数据预取引擎可以并发预取4个数据流，每个2个缓存行，一共8个预取操作。数据预取引擎监控所有加载操作的地址操作数，当LSU结束当前缓存行并开始加载下一个缓存行，数据预取引擎将预取缓冲区数据传输到L1，并预取下一个缓存行到预取缓冲区。

下图展示了预取引擎框图
![Pasted image 20230912094139.png](/assets/images/power/Pasted image 20230912094139.png)

* *Bus interface unit* **BIU**提供*IPU*，*DCU*，预取引擎和L2缓存之间连接。数据接口位宽128位。
* *L2 cache* **POWER3** 支持1MB-16MB的L2缓存，可以是组相连或直接映射。总线和L2以32B位宽相连，一个128B缓存行需要4个周期传输。

# 6. POWER 4
**POWER 4** 是一个双核处理器，通过一个*core interface unit(CIU)* 共享一个统一的片上二级缓存。
* *CIU* 是3个L2控制器和2个处理器之间的*crossbar switch* 。每个L2控制器每周期能提供32B数据。*CIU* 通过一个8B的接口接受来自处理器的存储操作。
* 每一个处理器有一个相关联的*noncacheable unit(NC unit)*，负责处理指令串行功能和非缓存存储加载操作。
* L3控制器和目录在**POWER4** 片上，但是实际L3位于片外。
* *fabric controller* 负责L2和L3之间的数据流以及**POWER 4** SMP通信。
* *GX controller* 控制器负责IO。
每个处理器芯片包括四种类型IO:
* 和同一个模块内其他**POWER4** 芯片通信，4个16-byte接口。物理是线上，这4个逻辑总线有6个总线组成，3个输入，3个输出。
* 和不同模块的**POWER4** 芯片通信，2个8-byte的总线，一个输入，一个输出。
* 片外L3接口，2个16-byte总线，一个输入，一个输出，运行在1/3处理器频率。
* 2个4-byte GX总线, 一个输入，一个输出，运行在1/3处理器频率。

下图展示了**POWER4** 的逻辑框图:
![Pasted image 20230912171951.png](/assets/images/power/Pasted image 20230912171951.png)

The chip also has a dedicated intra chip and inter-chip communication fabric to support a maximum of 32 processors in a SMP system.
The internal micro-architecture of the core is a speculative, superscalar, out–of–order(OOO) execution design. it can fetch, decode and crack up to eight instructions per cycle, issue up to five operations per cycle and maintain a sustained completion rate of five operations per cycle — most instructions crack to one operation, a few crack to more than one operation. With large register rename pools, other OOO resources and long pipelines, the P4 can have over two hundred instructions in flight. To exploit instruction level parallelism there are eight execution units each of which can have an instruction issued each cycle, though this rate can not be sustained due to the formation of five instruction groups.
To minimise the resources used to keep track of such a large number of instructions in flight, the P4 decodes and schedules instructions into instruction groups of five. The group is encoded such that each slot in the group can only be scheduled on specific execution unit pipelines, and the fifth slot may only contain a branch instruction or a no–op. This group then has rename and other OOO resources associated with it and is tracked through the entire execution pipeline. Finally when every in struction in the group has completed the entire group is committed, hence the sustained completion rate of five instructions per cycle.

To achieve very high potential clock rates the P4 has a sixteen cycle instruction pipeline, which can cause very long miss–predicted branch stalls, so the designers introduced a very large branch prediction unit with sophisticated subroutine and other indirection logging. The P4 uses three branch history tables: the local predictor table has 16k 1–bit predictors, the global predictor table also has 16k 1–bit predictors — indexed by hashing the result of the last 11 branches with the current branch address and finally a selector table of 16k 1–bit predictors — indexed the same as the global branch table.

As a part of IBM’s commitment to reliability, availability and serviceability, the designers have introduced into the P4 the ability to logically partition an SMP system into a number of logical subsystems. Each processor has a logical partition ID, a real mode offset register (RMOR), a real mode limit register (RMLR) and a hypervisor RMOR. These registers and a few others can only be modified in a new privileged mode called the hypervisor mode. Once the RMOR/RMLR registers are set by the hypervisor the processor will be incapable of accessing memory out of that range of physical memory. With these tools the operating system can divide a large SMP into a series of logical partitions that can be individually failed —without bringing the entire system down – until a service technician can be scheduled. This technique has other uses too, such as processor virtualisation.

![Pasted image 20230809154420.png](/assets/images/power/Pasted image 20230809154420.png)

## 6.1 POWER4 Core
Below figure shows a high-level block diagram of a POWER4 processor. The internal microarchitecture of the core is a speculative superscalar out-of-order execution design. Up to eight instructions can be issued each cycle, with a sustained completion rate of five instructions. Register-renaming pools and other out-of-order resources coupled with the pipeline structure allow the design to have more than 200 instructions in flight at any given time. In order to exploit instruction-level parallelism, there are eight execution units, each capable of being issued an instruction each cycle. Two identical floating-point execution units, each capable of starting a fused multiply and add each cycle are provided. In order to feed the dual floating-point units, two load/store units, each capable of performing address-generation arithmetic, are provided. Additionally, there are dual fixed-point execution units, a branch execution unit, and an execution unit to perform logical operations on the condition register.
![Pasted image 20230912172922.png](/assets/images/power/Pasted image 20230912172922.png)
### Branch prediction
POWER4 uses a multilevel branch-prediction scheme to predict whether or not a conditional branch instruction is taken. Additionally, branch target addresses are predicted for those instructions that branch to an address specified in either the count register or the link register. In POWER4, up to eight instructions are fetched each cycle from the direct-mapped 64KB instruction cache. The branch-prediction logic scans the fetched instructions, looking for up to two branches each cycle. Depending upon the branch type found, various branch-prediction mechanisms engage to help predict the branch direction or the target address of the branch or both. Branch direction for unconditional branches is not predicted. All conditional branches are predicted, even if the condition register bits upon which they are dependent are known at instruction fetch time. Branch target addresses for the PowerPC branch-to-link-register (bclr) and branch-tocount-register (bcctr) instructions are predicted using a hardware-implemented link stack and count cache mechanism, respectively. Target addresses for absolute and relative branches are computed directly as part of the branch scan function. POWER4 uses a set of three branch-history tables to predict the direction of branch instructions.
	* The first table, called the local predictor, is similar to a traditional branch-history table (BHT). It is a 16 384-entry array indexed by the branch instruction address producing a 1-bit predictor that indicates whether the branch direction should be taken or not taken.
	* The second table, called the global predictor, predicts the branch direction on the basis of the actual path of execution to reach the branch. The path of execution is identified by an 11-bit vector, one bit per group of instructions fetched from the instruction cache. The global history vector is hashed, using a bitwise exclusive or with the address of the branch instruction. The result indexes into a 16 384-entry global history table to produce another 1-bit branch-direction predictor.
	* a third table, called the selector table, keeps track of which of the two prediction schemes works better for a given branch and is used to select between the local and the global predictions. The 16 384-entry selector table is indexed exactly the same way as the global history table to produce the 1-bit selector. This combination of branch-prediction tables has been shown to produce very accurate predictions across a wide range of workload types.
Dynamic branch prediction can be overridden by software. It is accomplished by setting two previously reserved bits in conditional branch instructions, one to indicate a software override and the other to predict the direction.
POWER4 uses a link stack to predict the target address for a branch-to-link instruction that it believes corresponds to a subroutine return. By setting the hint bits in a branch-to-link instruction, software communicates to the processor whether a branch-to-link instruction represents a subroutine return, a target address that is likely to repeat, or neither. When instruction-fetch logic fetches a branch-and-link instruction (either conditional or unconditional) predicted as taken, it pushes the address of the next instruction onto the link stack. When it fetches a branch-to-link instruction with taken prediction and with hint bits indicating a subroutine return, the link stack is popped, and instruction fetching starts from the popped address.
POWER4 uses a 32-entry, tagless, direct-mapped cache, called a count cache, to predict the repetitive targets, as indicated by the software hints. Each entry in the count cache can hold a 62-bit address. When a branch-to-link or branch-to-count instruction is executed, for which the software indicates that the target is repetitive and therefore predictable, the target address is written in the count cache. When such an instruction is fetched, the target address is predicted using the count cache.

### 6.1.1 Instruction Fecth

once instruction-fetch address register (IFAR) is loaded, the I-cache is accessed and retrieves up to eight instructions per cycle. Each line in the I-cache can hold 32 PowerPC instructions, i.e., 128 bytes. Each line is divided into four equal sectors. Since I-cache misses are infrequent, to save area, the I-cache has been designed to contain a single port that can be used to read or write one sector per cycle. The I-cache directory (IDIR) is indexed by the effective address and contains 42 bits of real address per entry. On an I-cache miss, instructions are returned from the L2 in four 32-byte transmissions. The L2 normally returns the critical sector (the sector containing the specific word address that references the cache line) in one of the first two beats. Instruction-fetch logic forwards these demandoriented instructions into the pipeline as quickly as possible. In addition, the cache line is written into one entry of the instruction-prefetch buffer so that the I-cache itself is available for successive instruction fetches. The instructions are written to the I-cache during cycles when instruction fetching is not using the I-cache, as is the case when another I-cache miss occurs. In this way, writes to the I-cache are hidden and do not interfere with normal instruction fetching.
the EA, RA pair is stored in a 128-entry, two-way set-associative array, called the effective-to-real address translation (ERAT) table. POWER4 implements separate ERATs for instruction-cache (IERAT) and datacache (DERAT) accesses. Both ERATs are indexed using the effective address. A common 1024-entry four-way setassociative TLB is implemented for each processor.
When the instruction pipeline is ready to accept instructions, the IFAR content is sent to the I-cache, IDIR, IERAT, and branch-prediction logic. The IFAR is updated with the address of the first instruction in the next sequential sector. In the next cycle, instructions are received from the I-cache and forwarded to an instruction queue from which the decode, crack, and group formation logic. Also received in the same cycle are the RA from the IDIR, the EA, RA pair from the IERAT, and the branchdirection-prediction information. The IERAT is checked to ensure that it has a valid entry and that its RA matches the contents of the IDIR. If the IERAT has an invalid entry, the EA must be translated from the TLB and SLB. Instruction fetching is then stalled. Assuming that the IERAT is valid, if the RA from the IERAT matches the contents of the IDIR, an I-cache hit is validated. Using the branch-prediction logic, the IFAR is reloaded and the process is repeated. Filling the instruction queue in front of the decode, crack, and group formation logic allows instruction fetching to run ahead of the rest of the system and queue work for the remainder of the system. In this way, when there is an I-cache miss, there often are additional instructions in the instruction queue to be processed, thereby not freezing the pipeline.
If there is an I-cache miss, several different scenarios are possible. First, the instruction-prefetch buffers are examined to see whether the requested instructions are there, and, if so, logic steers these instructions into the pipeline as though they came from the I-cache and will also write the critical sector into the I-cache. If the instructions are not found in the instruction-prefetch buffer, a demand-fetch reload request is sent to the L2. The L2 processes this reload request with high priority. When it is returned from the L2, an attempt will be made to write it into the I-cache. In addition to these demand-oriented instructionfetching mechanisms, POWER4 prefetches instructioncache lines that might soon be referenced into its instruction-prefetch buffer, which is capable of holding four entries of 32 instructions each. The instructionprefetch logic monitors demand instruction-fetch requests and initiates prefetches for the next one (if there is a hit in the prefetch buffer) or two (if there is a miss in the prefetch buffer) sequential cache lines after verifying that they are not already in the I-cache. When these requests return cache lines, the returned lines are stored in the instruction-prefetch buffer so that they do not pollute the demand-oriented I-cache. The I-cache contains only cache lines that have had a reference to at least one instruction.

### 6.1.2 Decode, crack, and group formation
A group contains up to five internal instructions referred to as IOPs. In the decode stages, the instructions are placed sequentially in a group—the oldest instruction is placed in slot 0, the next oldest one in slot 1, and so on. Slot 4 is reserved solely for branch instructions. If required, no-ops are inserted to force the branch instruction to be in the fourth slot. If there is no branch instruction, slot 4 contains a no-op. Only one group of instructions can be dispatched in a cycle, and all instructions in a group are dispatched together. (By dispatch, we mean the movement of a group of instructions into the issue queues.) Groups are dispatched in program order. Individual IOPs are issued from the issue queues to the execution units out of program order. Results are committed when the group completes. A group can complete when all older groups have completed and when all instructions in the group have finished execution. Only one group can complete in a cycle.
For correct operation, certain instructions are not allowed to execute speculatively. To ensure that the instruction executes nonspeculatively, it is not executed until it is the next one to complete. This mechanism is called completion serialization. To simplify the implementation, such instructions form single instruction groups. Examples of completion serialization instructions include loads and stores to guarded space and contextsynchronizing instructions such as the move-to-machinestate-register instruction that is used to alter the state of the machine. In order to implement out-of-order execution, many of the architected registers are renamed, but not all. To ensure proper execution of these instructions, any instruction that sets a nonrenamed register terminates a group.

### 6.1.3 Group dispatch and instruction issue
Instruction groups are dispatched into the issue queues one group at a time. As a group is dispatched, control information for the group is stored in the group completion table (GCT). The GCT can store up to 20 groups. The GCT entry contains the address of the first instruction in the group. As instructions finish execution, that information is registered in the GCT entry for the group. Information is maintained in the GCT until the group is retired, i.e., either all of its results are committed, or the group is flushed from the system. Each instruction slot feeds separate issue queues for the floating-point units, the branch-execution unit, the CR execution unit, the fixed-point execution units, and the load/store execution units. The fixed-point and load/store execution units share common issue queues. Below Table summarizes the depth of each issue queue and the number of queues available for each type of queue.
![Pasted image 20230913150523.png](/assets/images/power/Pasted image 20230913150523.png)
Instructions are dispatched into the top of an issue queue. As they are issued from the queue, the remaining instructions move down in the queue. In the case of two queues feeding a common execution unit, the two queues are interleaved. The oldest instruction that has all of its sources set in the common interleaved queue is issued to the execution unit. Before a group can be dispatched, all resources to support that group must be available. If they are not, the group is held until the necessary resources are available. To successfully dispatch, the following resources are assigned:
* GCT entry: One GCT entry is assigned for each group. It is released when the group retires.
* Issue queue slot: An appropriate issue queue slot must be available for each instruction in the group. It is released when the instruction in it has successfully been issued to the execution unit. Note that in some cases this is not known until several cycles after the instruction has been issued. As an example, a fixed-point operation dependent on an instruction loading a register can be speculatively issued to the fixed-point unit before it is known whether the load instruction resulted in an L1 data cache hit. Should the load instruction miss in the cache, the fixed-point instruction is effectively pulled back and sits in the issue queue until the data on which it depends is successfully loaded into the register.
* Rename register: For each register that is renamed and set by an instruction in the group, a corresponding renaming resource must be available. Below Table summarizes the renaming resources available to each POWER4 processor. The renaming resource is released when the next instruction writing to the same logical resource is committed.
![Pasted image 20230913154714.png](/assets/images/power/Pasted image 20230913154714.png)
* Load reorder queue (LRQ) entry: An LRQ entry must be available for each load instruction in the group. These entries are released when the group completes. The LRQ contains 32 entries.
* Store reorder queue (SRQ) entry: An SRQ entry must be available for each store instruction in the group. These entries are released when the result of the store is successfully sent to the L2, after the group completes. The SRQ contains 32 entries.

### 6.1.4 Load/store unit operation
Associated with each SRQ entry is a store data queue (SDQ) entry. The SDQ entry maintains the desired result to be stored until the group containing the store instruction is committed. Once committed, the data maintained in the SDQ is written to the caches. Additionally, three particular hazards must be avoided:
* Load hit store: A younger load that executes before an older store to the same memory location has written its data to the caches must retrieve the data from the SDQ. As loads execute, they check the SRQ to see whether there is any older store to the same memory location with data in the SDQ. If one is found, the data is forwarded from the SDQ rather than from the cache. If the data cannot be forwarded (as is the case if the load and store instructions operate on overlapping memory locations and the load data is not the same as or contained within the store data), the group containing the load instruction is flushed; that is, it and all younger groups are discarded and refetched from the instruction cache. If we can tell that there is an older store instruction that will write to the same memory location but has yet to write its result to the SDQ, the load instruction is rejected and reissued, again waiting for the store instruction to execute.
* Store hit load: If a younger load instruction executes before we have had a chance to recognize that an older store will be writing to the same memory location, the load instruction has received stale data. To guard against this, as a store instruction executes it checks the LRQ; if it finds a younger load that has executed and loaded from memory locations to which the store is writing, the group containing the load instruction and all younger groups are flushed and refetched from the instruction cache. To simplify the logic, all groups following the store are flushed. If the offending load is in the same group as the store instruction, the group is flushed, and all instructions in the group form single instruction groups.
* Load hit load: Two loads to the same memory location must observe the memory reference order and prevent a store to the memory location from another processor between the intervening loads. If the younger load obtains old data, the older load must not obtain new data. This requirement is called sequential load consistency. To guard against this, LRQ entries for all loads include a bit which, if set, indicates that a snoop has occurred to the line containing the loaded data for that entry. When a load instruction executes, it compares its load address against all addresses in the LRQ. A match against a younger entry which has been snooped indicates that a sequential load consistency problem exists. To simplify the logic, all groups following the older load instruction are flushed. If both load instructions are in the same group, the flush request is for the group itself. In this case, each instruction in the group when refetched forms a single instruction group in order to avoid this situation the second time around.
## 6.2 POWER4 Core Pipeline
The MP cycle is the mapper cycle, in which all dependencies are determined, resources assigned, and the group dispatched into the appropriate issue queues. During the ISS cycle, the IOP is issued to the appropriate execution unit, reads the appropriate register to retrieve its sources during the RF cycle, and executes during the EX cycle, writing its result back to the appropriate register during the WB cycle. At this point, the instruction has finished execution but has not yet been completed. It cannot complete for at least two more cycles, the Xfer and CP cycle, assuming that all older groups have completed and all other instructions in the same group have also finished.
Instructions waiting in the instruction queue after being fetched from the instruction cache wait prior to the D1 cycle. This is the case if instructions are fetched (up to eight per cycle) faster than they can be formed into groups. Similarly, instructions can wait prior to the MP cycle if resources are not available to dispatch the entire group into the issue queues. Instructions wait in the issue queues prior to the ISS cycle. Similarly, they can wait to complete prior to the CP cycle.
The pipeline for the two load/store units is identical and is shown as the LD/ST pipeline in Figure 4. After accessing the register file, load and store instructions generate the effective address in the EA cycle. The DERAT and, for load instructions, the data cache directory and the data cache, are all accessed during the DC cycle. If a DERAT miss should occur, the instruction is rejected; i.e., it is kept in the issue queue. Meanwhile, a request is made to the TLB to reload the DERAT with the translation information. The rejected instruction is reissued again a minimum of seven cycles after it was first issued. If the DERAT still does not contain the translation information, the instruction is rejected again. This process continues until the DERAT is reloaded. If a TLB miss occurs (i.e., we do not have translation information in the TLB), the translation is initiated speculatively. However, the TLB is not updated until we are certain that the instruction failing translation will be executed. To ensure that this occurs, the TLB updates are held off until the group that contains the instruction failing translation is the next group to complete. Two different page sizes, 4 KB and 16 MB, are supported.
In the case of loads, if the directory indicates that the L1 data cache contains the cache line, the requested bytes from the returned data are formatted (the fmt cycle) and written into the appropriate register. They are also available for use by dependent instructions during this cycle. In anticipation of a data cache hit, dependent instructions are issued so that their RF cycle lines up with the writeback cycle of the load instructions. If a cache miss is indicated, a request is initiated to the L2 to retrieve the line. Requests to the L2 are stored in the load miss queue (LMQ). The LMQ can hold up to eight requests to the L2. If the LMQ is full, the load instruction missing in the data cache is rejected and reissued again in seven cycles, and the process is repeated. If there is already a request to the L2 for the same line from another load instruction, the second request is merged into the same LMQ entry. If this is the third request to the same line, the load instruction is rejected, and processing continues as above. All reloads from the L2 check the LMQ to see whether there is an outstanding request yet to be honored against a just-returned line. If there is, the requested bytes are forwarded to the register to complete the execution of the load instruction. After the line has been reloaded, the LMQ entry is freed for reuse.
In the case of store instructions, rather than write data to the data cache, the data is stored in the SDQ as described above. Once the group containing the store instruction is completed, an attempt is made to write the data into the data cache. If the cache line containing the data is already in the L1 data cache, the changed data is written to the data cache. If it is not, the line is not reloaded from the L2. In both cases, the changed data is written to the L2. The coherency point for POWER4 is the L2 cache. Additionally, all data in the L1 data cache are also in the L2 cache. If data has to be cast out of the L2, the line is marked invalid in the L1 data cache if it is resident there.
![Pasted image 20230912173004.png](/assets/images/power/Pasted image 20230912173004.png)

## 6.3 POWER4 L2 Cache
The unified second-level cache is shared across the two processors on the POWER4 chip. Below shows a logical view of the L2 cache.
![Pasted image 20230912173042.png](/assets/images/power/Pasted image 20230912173042.png)
The L2 is implemented as three identical slices, each with its own controller. Cache lines are hashed across the three controllers. Each slice contains four SRAM partitions, each capable of supplying 16 bytes of data every other cycle. The four partitions can supply 32 bytes per cycle, taking four consecutive cycles to transfer a 128-byte line to the processor. The data arrays are ECC-protected (single error correct, double-error detect). Both wordline and bitline redundancy are implemented. The L2 cache directory is implemented in two redundant eight-way set-associative parity-protected arrays. The redundancy, in addition to providing a backup capability, also provides two nonblocking read ports to permit snoops to proceed without causing interference to load and store requests.
A pseudo-LRU replacement algorithm is implemented as a standard 7-bit tree structure. Since the L1 is a store-through design, store requests to the L2 are at most eight bytes per request. The L2 implements two four-entry 64-byte queues for gathering individual stores and minimizing L2 requests for stores. The majority of control for L2 cache management is handled by four coherency processors in each controller. A separate coherency processor is assigned to handle each request to the L2. Requests can come from either of the two processors (for either an L1 data-cache reload or an instruction fetch) or from one of the store queues. Each coherency processor has associated with it a cast-out processor to handle deallocation of cache lines to accommodate L2 reloads on L2 misses. The coherency processor does the following:
* Controls the return of data from the L2 (hit) or from the fabric controller (miss) to the requesting processor via the CIU.
* Updates the L2 directory as needed.
* Issues fabric commands for L2 misses on fetch requests and for stores that do not hit in the L2 in the M, Me, or Mu state (described below).
* Controls writing into the L2 when reloading because of fetch misses in the L2 or when accepting stores from the processors.
* Initiates back-invalidates to a processor via the CIU resulting from a store from one processor that hits a cache line marked as resident in the other processor’s L1 data cache.
Included in each L2 controller are four snoop processors responsible for managing coherency operations snooped from the fabric. When a fabric operation hits on a valid L2 directory entry, a snoop processor is assigned to take the appropriate action. Depending on the type of operation, the inclusivity bits in the L2 directory, and the coherency state of the cache line, one or more of the following actions may result:
* Sending a back-invalidate request to the processor(s) to invalidate a cache line in its L1 data cache.
* Reading the data from the L2 cache.
* Updating the directory state of the cache line.
* Issuing a push operation to the fabric to write modified data back to memory.
* Sourcing data to another L2 from this L2.
In addition to dispatching a snoop processor, the L2 provides a snoop response to the fabric for all snooped operations. When a fabric operation is snooped by the L2, the directory is accessed to determine whether the targeted cache line is resident in the L2 cache and, if so, its coherency state. Coincident with the snoop directory lookup, the snooped address is compared with the addresses of any currently active coherency, cast-out, and snoop processors to detect address-collision scenarios. The address is also compared to the per-processor reservation address registers. On the basis of this information, the snoop response logic determines the appropriate snoop response to send back.
The L2 cache controller also acts as the reservation station for the two processors on the chip in support of the load [double] word and reserve indexed (lwarx/ldarx) and the store [double] word conditional (stwcx/stdcx) instructions. One address register for each processor is used to hold the reservation address. The reservation logic maintains a reservation flag per processor to indicate when a reservation is set. The flag is set when a lwarx or ldarx instruction is received from the processor; it is reset when certain invalidating type operations are snooped, including a store to the reservation address from other processors in the system, or when a stwcx or stdcx instruction succeeds. (A successful store occurs if the reservation flag was not reset by another operation. The success or failure of the conditional store instruction is communicated to the program by setting a bit in the condition register.) The L2 cache implements an enhanced version of the MESI coherency protocol, supporting seven states as follows:
* I (invalid state): The data is invalid. This is the initial state of the L2 entered from a power-on reset or a snoop invalidate hit.
* SL (shared state, can be source to local requesters): The data is valid. The cache line may also be valid in other L2 caches. From this state, the data can be sourced to another L2 on the same module via intervention. This state is entered as a result of a processor L1 data-cache load request or instruction-fetch request that misses in the L2 and is sourced from another cache or from memory when not in other L2s.
* S (shared state): The data is valid. The cache line may also be valid in other L2 caches. In this state, the data cannot be sourced to another L2 via intervention. This state is entered when a snoop-read hit from another processor on a chip on the same module occurs and the data and tag were in the SL state.
* M (modified state): The data is valid. The data has been modified and is exclusively owned. The cache line cannot be valid in any other L2. From this state the data can be sourced to another L2 in a chip on the same or remote module via intervention. This state results from a store operation performed by one of the processors on the chip.
* Me (exclusive state): The data is valid. The data is not considered modified but is exclusive to this L2. The cache line cannot be valid in any other L2. Cast-out of an Me line requires only invalidation of the tag; i.e., data need not be written back to memory. This state is entered as a result of one of the processors on the chip asking for a reservation via the lwarx or ldarx instruction when data is sourced from memory or for a cache line being prefetched into the L2 that was sourced from memory. (Sourcing data from the L3 in the O state is equivalent to sourcing it from memory.)
* Mu (unsolicited modified state): The data is valid. The data is considered to have been modified and is exclusively owned. The cache line cannot be valid in any other L2. This state is entered as a result of one of the processors on the chip asking for a reservation via the lwarx or ldarx instruction when data is sourced from another L2 in the M state or for a cache line being prefetched into the L2 that was sourced from another L2 in the M state.
* T (tagged state): The data is valid. The data is modified with respect to the copy in memory. It has also been sourced to another cache; i.e., it was in the M state at some time in the past, but is not currently exclusively owned. From this state, the data will not be sourced to another L2 via intervention until the combined response is received and it is determined that no other L2 is sourcing data (that is, if no L2s have the data in the SL state. This state is entered when a snoop-read hit occurs while in the M state.

![Pasted image 20230913172331.png](/assets/images/power/Pasted image 20230913172331.png)

Included within the L2 subsystem are two noncacheable units (NCU), one per processor. The NCUs handle noncacheable loads and stores, as well as cache and synchronization operations. Each NCU is partitioned into two parts: the NCU master and the NCU snooper. The NCU master handles requests originating from processors on the chip, while the NCU snooper handles the snooping of translation lookaside buffer invalidate entry (tlbie) and instruction cache block invalidate (icbi) operations from the fabric. The NCU master includes a four-deep FIFO queue for handling cache-inhibited stores, including memory-mapped I/O store operations, and cache and memory barrier operations. It also contains a one-deep queue for handling cache-inhibited load operations. The return of data for a noncacheable load operation is via the L2 controller, using the same reload buses as for cacheable load operations. Cache-inhibited stores are routed through the NCU in order to preserve execution ordering of noncacheable stores with respect to one another. Cache and synchronization operations originating in a processor on the chip are handled in a manner similar to cache-inhibited stores, except that they do not have any data associated with them. These operations are issued to the fabric. Most will be snooped by an L2 controller. Included in this category are the icbi, tlbie, translation lookaside buffer synchronize (tlbsync), enforce in-order execution of I/O (eieio), synchronize (sync), page table entry synchronize (ptesync), lsync, data cache block flush (dcbf), data cache block invalidate (dcbi) instructions, and a processor acknowledgment that a snooped TLB has completed. The NCU snooper snoops icbi and tlbie operations from the fabric, propagating them upstream to the processors. These snoops are sent to the processor via the reload buses of the L2 controller. It also snoops sync, ptesync, lsync, and eieio. These are snooped because they may have to be retried because of an icbi or TLB that has not yet completed to the same processor.
## POWER4 L3 Cache
Belowshows a logical view of the L3 cache.

![Pasted image 20230912173114.png](/assets/images/power/Pasted image 20230912173114.png)
 The L3 consists of two components, the L3 controller and the L3 data array. The L3 controller is located on the POWER4 chip and contains the tag directory as well as the queues and arbitration logic to support the L3 and the memory behind it. The data array is stored in two 16MB eDRAM chips mounted on a separate module. A separate memory controller can be attached to the back side of the L3 module. To facilitate physical design and minimize bank conflicts, the embedded DRAM on the L3 chip is organized as eight banks at 2 MB per bank, with banks grouped in pairs to divide the chip into four 4MB quadrants. The L3 controller is also organized in quadrants. Each quadrant contains two coherency processors to service requests from the fabric, perform any L3 cache and/or memory accesses, and update the L3 tag directory. Additionally, each quadrant contains two processors to perform the memory cast-outs, invalidate functions, and DMA writes for I/O operations. Each pair of quadrants shares one of the two L3 tag directory SRAMs. The L3 cache is eight-way set-associative, organized in 512-byte blocks, with coherence maintained on 128-byte sectors for compatibility with the L2 cache. Five coherency states are supported for each of the 128-byte sectors, as follows:
 * I (invalid state): The data is invalid.
 * S (shared state): The data is valid. In this state, the L3 can source data only to L2s for which it is caching data.
 * T (tagged state): The data is valid. The data is modified relative to the copy stored in memory. The data may be shared in other L2 or L3 caches.
 * Trem (remote tagged state): This is the same as the T state, but the data was sourced from memory attached to another chip.
 * O (prefetch data state): The data in the L3 is identical to the data in memory. The data was sourced from memory attached to this L3. The status of the data in other L2 or L3 caches is unknown.
The L3 caches data, either from memory that resides beneath it or from elsewhere in the system, on behalf of the processors attached to its processor module. When one of its processors issues a load request that misses the L3 cache, the L3 controller allocates a copy of the data in the S (shared) state. Inclusivity with the L1 and L2 is not enforced. Hence, when the L3 deallocates data, it does not invalidate any L1 or L2 copies. The L3 enters the T or Trem state when one of its local L2 caches does a castout from the M or T state. An address decode is performed at snoop time to determine whether the address maps to memory behind the L3 or elsewhere in the system, and this causes the L3 to transition to the T or Trem state as appropriate. This design point was chosen to avoid the need for a memory address-range decode when the L3 performs a cast-out operation. The L3 can use the T/Trem distinction to determine whether the data can be written to the attached memory controller, or whether the castout operation must be issued as a fabric bus transaction. When in the T or Trem state, the L3 sources data to any requestor in the system. However, when in the S state, the L3 will source data only to its own L2s. This minimizes data traffic on the buses between processor modules, since, whenever possible, data is sourced by an L3 cache on the requesting processor module. When in the O state, the L3 sources data to any requestor using the same rules that determine when it is permitted to send data from its attached memory controller; i.e., no cache is sourcing data and no snooper retried the request.
## POWER4 Memory System
A logical view of the memory subsystem is shown in Figure. Each POWER4 chip can have an optional memory controller attached behind the L3 cache. Memory controllers are packaged two to a memory card and support two of the four processor chips on a module. A module can attach a maximum of two memory cards. Memory controllers can have either one or two ports to memory. The memory controller is attached to the L3 eDRAM chips, with each chip having two 8-byte buses, one in each direction, to the data interface in the memory controller. These buses operate at one-third processor speed using the synchronous wave pipeline interface to operate at high frequencies. Each port to memory has four 4-byte bidirectional buses operating at a fixed frequency of 400 MHz connecting load/store buffers in the memory controller to four system memory interface (SMI) chips used to read and write data from memory. When two memory ports are available, they each work on 512-byte boundaries. The memory controller has a 64-entry read command queue, a 64-entry write command queue, and a 16-entry write cache queue. The memory is protected by a single-bit error correct, double-bit error detect ECC. Additionally, memory scrubbing is used in the background to find and correct soft errors. Each memory extent has an extra DRAM to allow for transparent replacement of one failing DRAM per group of four DIMMs using chip-kill technology. Redundant bit steering is also employed.
![Pasted image 20230912173158.png](/assets/images/power/Pasted image 20230912173158.png)

## POWER4 IO
Below figure shows the I/O structure in POWER4 systems. Attached to a POWER4 GX bus is a remote I/O (RIO) bridge chip. This chip transmits the data across two 1-byte wide RIO buses to PCI host bridge (PHB) chips. Two separate PCI buses attach to PCI–PCI bridge chips that further fan the data out across multiple PCI buses. When multiple nodes are interconnected to form clusters of systems, the RIO bridge chip is replaced with a chip that connects with the switch. This provides increased bandwidth and reduced latency over switches attached via the PCI interface.
![Pasted image 20230913093144.png](/assets/images/power/Pasted image 20230913093144.png)
## System Building Block - MCM
Below figure shows the logical interconnection of four POWER4 chips across four logical buses to form an eight way SMP. Each chip writes to its own bus, arbitrating among the L2, I/O controller, and L3 controller for the bus. Each of the four chips snoops all of the buses, and if it finds a transaction that it must act on, it takes the appropriate action. Requests for data from an L2 are snooped by all chips to see
* whether it is in their L2 and in a state that permits sourcing it from the holding chip’s L2 to the requesting chip
* whether it is in its L3 or in memory behind its L3 cache based on the real address of the request.
If it is, the sourcing chip returns the requested data to the requesting chip on its bus. The interconnection topology appears like a bus-based system from the perspective of a single chip. From the perspective of the module, it appears like a switch.
![Pasted image 20230913100716.png](/assets/images/power/Pasted image 20230913100716.png)
## 32-way SMP
Below figure shows the interconnection of multiple four-chip MCMs to form larger SMPs. One to four MCMs can be interconnected. When interconnecting multiple MCMs, the intermodule buses act as repeaters, moving requests and responses from one module to another module in a ring topology. As with the single MCM configuration, each chip always sends requests/commands and data on its own bus but snoops all buses.

![Pasted image 20230913092752.png](/assets/images/power/Pasted image 20230913092752.png)
The multi-MCM configuration provides a worst-case memory access latency of slightly greater than 10% more than the best-case memory access latency maintaining the flat memory model, simplifying programming.

## POWER4 RAS
The L1 caches are parity-protected. Errors encountered in the L1 data cache are reported as a synchronous machine-check interrupt. To support error recovery, the machine-check interrupt handler is implemented in systemspecific firmware code. When the interrupt occurs, the firmware saves the processor-architected states and examines the processor registers to determine the recovery and error status. If the interrupt is recoverable, the system firmware removes the error by invalidating the L1 datacache line and incrementing an error counter. If the L1 data-cache error counter is greater than a predefined threshold, which is an indication of a solid error, the system firmware disables the failing portion of the L1 data cache. The system firmware then restores the processorarchitected states and “calls back” the operating system machine-check handler with the “fully recovered” status. The operating system checks the return status from firmware and resume execution.
The L3 tag directory is ECC-protected to support single-bit error correct and double-bit error detect. Uncorrectable errors result in a system checkstop. If a directory access results in a correctable error, the access is stalled while the error is corrected. After correction, the original access takes place. When an error is corrected, a recovered attention message is sent to the service processor for thresholding purposes
The L3 address, memory address, and control buses have parity bits for single-bit error detection. The L3 and memory data buses, as well as the L3-cache-embedded DRAMs, have ECC to support single-bit error correct and double-bit error detect. Uncorrectable errors are flagged and delivered to the requesting processor with an error indication, resulting in a machine-check interrupt.
If an L3 tag directory develops a stuck fault, or L3- cache-embedded DRAMs develop more stuck faults than can be handled with the line-delete control registers, the L3 cache on the failing processor chip can be reconfigured and logically removed from the system without removing other L3 caches in the system and without reconfiguring the memory attached to that L3. Memory accesses continue to pass through the reconfigured L3 module, but that L3 controller no longer performs cache operations.
# 7. POWER 5
The POWER5 has moved the L3 to directly connect to the L2 controller as victim cache, so that L2 fills no longer cause transactions on the fabric. In addition to moving the L3 cache, IBM designers added an on–chip memory controller to improve the speed of main memory access. These two changes greatly decrease the latency to main memory of a cache miss and also enhances system reliability by reducing system chip count. The processor cores each support two logical threads. To the operating system, the chip appears as a four-way symmetric multiprocessor. A 1.875-MB L2 cache is shared by the two processor cores. There are three partitions, or slices, of the L2, each of which is ten-way set-associative, with 512 congruence classes of 128-byte lines. The L3 directory for the off-chip 36-MB L3 is also integrated onto the POWER5 chip. The L3 is also implemented as three slices, with each slice acting as a victim cache for one of the L2 slices. Each slice is 12-way set-associative, with 4,096 congruence classes of 256-byte lines managed as two 128-byte sectors to match the L2 line size.
Accesses between the POWER5 chip and the L3 are across two unidirectional 16-byte-wide buses operating at half processor frequency. Access between memory and the on-chip memory controllers is via two unidirectional buses operating at twice the dual in-line memory module (DIMM) frequency. The data memory read bus is 16 bytes wide, while the write memory bus is 8 bytes wide.

![Pasted image 20230914085010.png](/assets/images/power/Pasted image 20230914085010.png)
## Multi-threading Evolution
* SMT easily added to Superscalar Micro-architecture
	* Second Program Counter (PC) added to share I-fetch bandwidth
	* GPR/FPR rename mapper expanded to map second set of registers (High order address bit indicates thread)
	* Completion logic replicated to track two threads
	* Thread bit added to most address/tag buses
![Pasted image 20230809154930.png](/assets/images/power/Pasted image 20230809154930.png)
In the PowerPC architecture, address translation is performed in two steps. First, the effective address is translated to the virtual address. In the POWER5 processor, the segment table is cached in a fully associative 64-entry segment lookaside buffer (SLB), one per thread. Next, the virtual address is translated to the real address using a hashed page table that is also maintained in memory. In the POWER5 processor, the page table is cached in a 1,024-entry, four-way set associative translation lookaside buffer (TLB). To facilitate fast translation, two first-level translation tables are used, one for instructions and one for data, to provide a fast, effective address to a real address translation.
The first-level data translation table is a fully associative 128-entry array. The first-level instruction translation table is a two-way set-associative 128-entry array. Entries in both first-level translation tables are tagged with the thread number and are not shared between threads. The BHT and count cache are also unchanged to support SMT. The return address stack was duplicated, since it contains ordering that must be maintained within a thread. The four instruction prefetch buffers are split between the two threads; each thread can independently process instruction cache misses and instruction prefetches.
The GCT design was changed to support SMT by implementing it as a linked list so that each thread can be independently allocated and deallocated. A fully shared GCT allows the number of entries to remain the same as in POWER4 systems. Register renaming is changed only slightly from the POWER4 implementation. Each logical register number has a thread bit appended, and these are then mapped as usual. Because the second thread comes with its set of architected registers, the number of register renames of each type was increased. In POWER5 systems, the size of the issue queues remains the same as in POWER4 systems, with the exception of the floating point issue queues, which were increased from a total of 20 entries to 24. In the POWER4 design, both the LRQ and the SRQ contain 32 entries. Entries are allocated and de-allocated in program order. They are allocated at dispatch. The LRQ is de-allocated at completion, and the SRQ is deallocated after completion once the store has been sent to the L2. For SMT, program order must be maintained within a thread, but the threads must be able to independently allocate and de-allocate entries. Because the address checking to ensure memory consistency occurs on a thread basis, it was simpler to split the LRQ and SRQ into two halves, one per thread; this resulted in cases in which one thread could run out of LRQ or SRQ entries. Rather than increase the physical size of these queues, each was extended by providing 32 additional virtual queue entries, 16 per thread. A virtual queue entry contains sufficient information to identify the instruction but not the address specified for the load or store instruction or the data to be stored for a store instruction. This mechanism provides a low-cost method of extending the LRQ and SRQ sizes and not stalling instruction dispatch. At instruction dispatch, if a real LRQ or SRQ entry is not available and a virtual entry is available, the instruction can dispatch with the virtual entry. As real entries become available, virtual entries are converted to real entries, with the virtual queue entry returned to the pool for possible use by younger instructions. Before a load or a store can issue, its LRQ or SRQ entry, respectively, must be associated with a real entry. The size of the BIQ remains at 16 entries as in POWER4 systems. In SMT mode it is split in two, with eight entries per thread. The POWER4 eight-entry load miss queue (LMQ) was changed in the POWER5 design by adding a thread bit that allows the LMQ to be dynamically shared between the two threads.
![Pasted image 20230804094425.png](/assets/images/power/Pasted image 20230804094425.png)
Below table summarizes changes of rename registers and issue queue sizes.
![Pasted image 20230914112838.png](/assets/images/power/Pasted image 20230914112838.png)
## Multithreaded Instruction Flow in Processor Pipeline
The POWER5 instruction pipeline is identical to the POWER4 instruction pipeline. All pipeline latencies, including the branch misprediction penalty and load-touse latency with an L1 data cache hit in POWER5, are the same as in POWER4.
![Pasted image 20230914085221.png](/assets/images/power/Pasted image 20230914085221.png)
In SMT mode, two separate program counters are used, one for each thread. Instruction fetches alternate between the two threads. After fetching (before pipeline stage D1), instructions are placed in separate instruction buffers for the two threads. These buffers contain 24 instructions each, slightly smaller than the single queue in a POWER4 microprocessor. On the basis of thread priority, up to five instructions are fetched from one of the instruction buffers (D0 pipeline stage), and a group is formed (pipeline stages D1 through D3). Instructions in a group are all from the same thread. All instructions in the group are decoded in parallel.
![Pasted image 20230914085156.png](/assets/images/power/Pasted image 20230914085156.png)


* **Dynamic resource balancing**. The objective of dynamic resource balancing is to ensure that the two threads executing on the same processor flow smoothly through the system. Dynamic resource-balancing logic monitors resources such as the GCT and the load miss queue to determine if one thread is hogging resources. For example, if one thread encounters multiple L2 cache load misses, dependent instructions can back up in the issue queues, preventing additional groups from dispatching and slowing down the other thread. To prevent this, resource-balancing logic detects that a thread has reached a threshold of L2 cache misses and throttles that thread. The other thread can then flow through the machine without encountering congestion from the stalled thread. The Power5 resourcebalancing logic also monitors how many GCT entries each thread is using. If one thread starts to use too many GCT entries, the resourcebalancing logic throttles it back to prevent its blocking the other thread. Depending on the situation, the Power5 resource-balancing logic has three threadthrottling mechanisms:
	* **Reducing the thread’s priority** is the primary mechanism in situations where a thread uses more than a predetermined number of GCT entries.
	* **Inhibiting the thread’s instruction decoding until the congestion clears** is the primary mechanism for throttling a thread that incurs a prescribed number of L2 cache misses.
	* **Flushing all the thread’s instructions that are waiting for dispatch and holding the thread’s decoding until the congestion clears** is the primary mechanism for throttling a thread executing a long-executing instruction, such as a synch instruction. (A synch instruction orders memory operations across multiple processors.)
* **Adjustable thread priority**. Adjustable thread priority lets software determine when one thread should have a greater (or lesser) share of execution resources. Reasons for choosing an imbalanced thread priority include the following:
	* **A thread is in a spin loop waiting for a lock**. Software would give the thread lower priority, because it is not doing useful work while spinning.
	* **A thread has no immediate work to do and is waiting in an idle loop**. Again, software would give this thread lower priority.
	* **One application must run faster than another**. For example, software would give higher priority to real-time tasks over concurrently running background tasks.
The Power5 microprocessor supports eight software-controlled priority levels for each thread. Level 0 is in effect when a thread is not running. Levels 1 (the lowest) through 7 apply to running threads.
The Power5 supports two types of ST operation:
* In the dormant state, the operating system boots up in SMT mode but instructs the hardware to put the thread into the dormant state when there is no work for that thread. To make a dormant thread active, either the active thread executes a special instruction, or an external or decrementer interrupt targets the dormant thread.
* When a thread is in the null state, the operating system is unaware of the thread’s existence.
## POWER5 Memory System
POWER5 uses a synchronous memory interface (SMI) to DDR-I or DDR-II SDRAM. In the two-SMI mode, each SMI chip is configured for an 8-byte read and 2-byte write interface on the controller side, and two independent 8-byte ports to the DIMMs. In the four-SMI mode, each SMI chip is configured for a 4-byte read and a 2-byte write interface on the controller side and two independent 8-byte ports to the DIMMs. The SMI chips contain buffers to match the differences in interface widths between the controller side and the DIMMs.
A logical view of the memory subsystem is shown in Figure. The interface between the POWER5 chip and the SMI chips is made up of three buses. All of these buses operate at twice the DIMM speed. The three buses are the address/command bus, the unidirectional write data bus, and the unidirectional read data bus. The write data bus is 8 bytes wide, with each of up to four SMI chips receiving 2 bytes point to point. In configurations with two SMI chips per POWER5 chip, four of the 8-byte write data bus pins are left unconnected. The read data bus consists of 16 bytes. When four SMI chips are used, each SMI drives four of the 16-byte read data bus inputs. In configurations with two SMI chips per POWER5 chip, each SMI chip drives eight of the 16-byte read data bus inputs.
![Pasted image 20230914090521.png](/assets/images/power/Pasted image 20230914090521.png)

## POWER5 SMP
Information flow between POWER5 chips in a system is managed in a distributed manner by a fabric bus controller (FBC) located on each POWER5 chip. This interconnect topology, referred to as a distributed switch, functions in a similar manner across all configurations.
High-end POWER5 systems use the MCM package. The basic building block is eight POWER5 chips packaged with eight L3 chips on two MCMs. The MCMs, associated memory, and bridge chips to connect to I/O drawers comprise a book.Below shows the POWER5 16-way building block.
![Pasted image 20230914090918.png](/assets/images/power/Pasted image 20230914090918.png)
Connections from each POWER5 chip to L3, memory, and I/O are made through separate pairs of dedicated unidirectional buses. The buses to the L3 cache, 16 bytes wide in each direction, operate at half the processor frequency and scale with the processor frequency. The I/O bus, 4 bytes wide in each direction, referred to as the GX bus, operates at one third of the processor frequency. The memory bus operates at twice the DRAM frequency and scales with DRAM frequency. POWER5 chips on the MCM are interconnected with two buses in a ring configuration, with data flowing in opposite directions on the two buses. Each of these buses is 8 bytes wide and operates at processor frequency.
The MCM configuration is replicated with an identical module on a processor board to form a book. Four pairs of unidirectional data buses, each 8 bytes wide and operating at half processor frequency, interconnect the POWER5 chips on the two modules. A POWER5 book is a 16-way symmetric multiprocessor; with simultaneous multithreading, this presents a 32-way image to software.
To build systems larger than 16-way, multiple books are interconnected as shown in Figure. Up to four books can be interconnected to build up to a 64-way symmetric multiprocessor. POWER5 chips in each book are connected to corresponding chips in adjacent books, as shown with an 8-byte-wide bus operating at half processor frequency.
![Pasted image 20230914091104.png](/assets/images/power/Pasted image 20230914091104.png)
An alternate configuration based on a dual-chip module is used to form systems with one to eight POWER5 chips. DCMs are interconnected similarly to MCMs, with the exception that there is only one POWER5 chip and its associated L3 chip. A 16-way DCM-based POWER5 system is shown in Figure. Two DCMs serve as the basic building blocks. Systems can be constructed with one, two, four, six, or eight DCMs.
![Pasted image 20230914091243.png](/assets/images/power/Pasted image 20230914091243.png)
### Fabric bus controller
A primary element enhancing POWER5 system scalability is the flexibility and scalability of the fabric bus controller on each POWER5 chip. The FBC buffers and sequences operations among the L2/L3, the functional units of the memory subsystem, the fabric buses that interconnect POWER5 chips on the MCM, and the fabric buses that interconnect multiple MCMs. The fabric bus has separate address and data buses that run independently to allow split transactions. Transactions are tagged to allow out-of-order replies.
* **Address bus** The inter-MCM address buses are 8 bytes wide, operating at half the processor frequency, while the intra-MCM address buses are 4 bytes wide, operating at full processor frequency. Every address transfer uses the bus for four processor cycles. Address bus operations are protected by ECC that provides for SECDED. Address transfers include a 50-bit real address, an address tag, transfer type, and other relevant data that uniquely identifies each address as it appears on the bus. There are twelve point-to-point address buses interconnecting the four POWER5 chips on an MCM. The fabric broadcasts addresses from MCM to MCM using a ring structure. The address propagates serially through all eight MCMs in a 64-processor system. Each chip on the ring that initiates the address or receives it from another module is responsible for forwarding the address to the next MCM in the ring and to the other chips on its own MCM. Once the originating chip receives the address it transmitted, it does not continue propagating it.
* **Response bus** Similarly to the address buses, the inter-MCM response buses run at half the processor frequency. The intra-MCM response buses operate at the processor frequency. The response bus is essentially a delayed version of the address bus. It includes information related to the cache coherency actions that must be taken after the memory subsystem units on each processor chip have snooped the address against its corresponding queues and directories. The response bus phase is parity-protected. It follows the address phase delayed by a fixed number of cycles on the same set of buses; i.e., once a chip snoops an address, the chip must return a snoop response to the source chip or the chip on the source ring in a fixed number of cycles, with the actual number of cycles dependent on system size. Once the source chip on the MCM receives the snoop responses from the other three chips on the MCM, it combines these responses with its own response and with the collective response obtained from the previous MCM in the ring and forwards the merged snoop response to the next MCM in the ring for further accumulation. When the originating chip receives the snoop response from the system for the address it initiated, it generates a combined response that is broadcast throughout the system just like an address phase. The combined response details the ultimate action to be taken on the corresponding address (for example, what state the master can now go to, which chip is to supply the data to the master for a read operation, and any invalidate scenarios that must be performed in the background). To reduce cache-to-cache transfer latency, POWER5 systems add the notion of an early combined response mechanism, which allows a remote chip to send a line from its L2 (or L3 as the case may be) in shared state to the requesting processor soon after it receives the address. The early combined response is determined by comparing an MCM snoop response with the cumulative snoop response of all previous MCMs in the ring, in order to find the first coherent snooper that can supply the read data. This early combined response mechanism allows the initiation of the data-transfer phase by a coherent snooper soon after it receives the address. When such a coherent snooper is found, the early combined snoop response sent to the downstream snoopers also indicates that the source of data has already been found.
* **Data bus**. The data bus services all data-only transfers, such as cache interventions. It also services the data portion of address-initiated operations that require data to be sent, such as cast-outs, snoop pushes, and DMA writes. The data bus phase, which is ECC (SECDED)protected, has been enhanced for POWER5 systems. In POWER4 systems, the data propagated in a simple ring structure and followed a broadcast model on the destination MCM. In the POWER5 design, the number of data buses within a module has been doubled to eight. Between modules the data buses have also been doubled to eight buses. In addition, in POWER5, the fabric routes data to the specific chip that requested it. Also, the vertical-node data buses, in addition to the traditional node-to-node data buses, were added to the system for POWER5 to provide a shorter path with reduced latency.
## POWER5 RAS
On POWER5 systems, many firmware upgrades can be done on a running machine without any reboot or impact to the system. ECC has been implemented on all system interconnects. Single-bit interconnect failures are dynamically corrected. If the failure is persistent, a deferred repair action can be scheduled. As with POWER4 systems, first failure data capture capabilities are used to identify the source of a nonrecoverable failure. When a nonrecoverable error is encountered, the system is taken down, the book containing the fault is taken offline, and the system rebooted, all without human intervention.
Memory is protected by a single-bit error correct, double-bit error detect error-correction code (ECC). Additionally, memory scrubbing7 is used in the background to find and correct soft errors. Each memory extent has an extra DRAM to allow for transparent replacement of one failing DRAM per group of four DIMMs using chipkill technology.
# Power 6

## 8. POWER 6
POWER6 is a dual-core chip where each core can be run in a two-way SMT mode. POWER6 is an in-order processor with limited out-of-order execution for floating point operations. Additional virtualization functions, decimal arithmetic, and vector multimedia arithmetic were added. Checkpoint retry and processor sparing were implemented. The recovery unit (RU) contains the data representing the state of the processor that is protected by ECC so that the state of the processor can be restored when an error condition is detected. The POWER6 core implements a vector unit to support the PowerPC VMX instruction set architecture (ISA) and a decimal execution unit to support the decimal ISA.
![Pasted image 20230726195941.png](/assets/images/power/Pasted image 20230726195941.png)

## POWER6 Core Pipeline
Instruction fetching and branch handling are performed in the instruction fetch pipe. Instructions from the L2 cache are decoded in precode stages P1 through P4 before they are written into the L1 I-cache. Branch prediction is performed using a 16K-entry branch history table (BHT) that contains 2 bits to indicate the direction of the branch. Up to eight instructions are then fetched from the L1 I-cache and sent through the instruction decode pipeline, which contains a 64-entry instruction buffer (I-buffer) for each thread. Instructions from both threads are merged into a dispatch group of up to seven instructions and sent to the appropriate execution units.
decimal and vector multimedia extension instructions are issued through the FP issue queue (FPQ) and are executed in the decimal and vector multimedia execution unit. Data generated by the execution units is staged through the checkpoint recovery (CR) pipeline and saved in an error-correction code (ECC)-protected buffer for recovery. FX load/store instructions are executed in order with respect to each other, while FP instructions are decoupled from the rest of the other instructions and allowed to execute while overlapping with subsequent load and FX instructions. Additional emphasis was put in the design to minimize the memory effect: prefetching to multiple levels of caches, speculative execution of instructions to prefetch data into the L1 D-cache, speculative prefetching of instructions into the L1 I-cache, providing a load-data buffer for FP load instructions, hardware stride prefetching, and software-directed prefetching. Buffered stages are added between the dispatch stage and the execution stage to optimize certain execution latency between categories of instructions. The following are examples:
* The FX instructions are staged for two additional cycles prior to execution in order to achieve a one-cycle load-to-use between a load instruction and a dependent FX instruction.
* Branch instructions are staged for two additional cycles to line up with FX staging instructions in order to avoid an additional branch penalty on an incorrect guess.
* FP instructions are staged through the FPQ for six cycles to achieve a zero-cycle load-to-use instruction between a load instruction and the dependent FP instruction.
![Pasted image 20230915084655.png](/assets/images/power/Pasted image 20230915084655.png)
* IFU. The IFU fetches instructions from the L2 cache into the L1 I-cache. These fetches are either demand fetches that result from I-cache misses or prefetches that result from fetching up to two sequential cache lines after a demand fetch. Fetch requests are made for a cache line (128 bytes), and data is returned as four sectors of 32 bytes each. Demand and prefetch requests are made for both instruction threads, and data may return in any order, including interleaving of sectors for different cache lines. Up to 32 instruction fetch requests can be initiated from the core to the L2 cache. Instructions are fetched from the I-cache and written into the I-buffer in the IDU. One major difference between the design of the POWER6 and that of the POWER4 and POWER5 processors [1, 2] is to push many of the decode and group formation functions into pre-decode and thus out of the critical path from the I-cache to the instruction dispatch. The POWER6 processor also recodes some of the instructions in the pre-decode stages to help optimize the implementation of later pipeline stages. The POWER6 processor requires four cycles of pre-decode and group formation before writing instructions into the I-cache, thus increasing the L2 access latency. The POWER6 processor still accomplishes a significant performance improvement by removing these functions from the much more critical I-cache access path.The IFU performs the following major functions:
	* Instruction recoding—Instructions can be recoded as part of the pre-decode function and before writing them into the I-cache. An example of useful recoding is switching register fields in some class of instructions in order to make the instructions appear more consistent to the execution units and save complexity and multiplexers in critical paths. The most extensive instruction recoding on the POWER6 processor occurs for branches. Relative branches contain an immediate field that specifies an offset of the current program counter as a target address for the branch. Pre-decode adds the immediate field in the branch to the low-order bits of the program counter, thus eliminating the need for an adder in the critical branch execution path. Pre-decode also determines whether the high-order bits of the program counter have to be incremented, decremented, or used as is to compute the branch target and that information is encoded in the recoded branch. Absolute branches can also be handled by this approach by signaling to use the sign-extended offset as a branch target. Information about conditional branch execution is also encoded in the branch to enable branch execution to achieve the cycle-time target for this type of critical instruction. Instruction grouping—Instructions are grouped in the pre-decode stages and the group information is stored in the L1 I-cache. Grouping allows ease of dispatch determination in the critical dispatch stage. Only instructions of the same thread are allowed in the same group. In general, the number of instructions in the group matches the dispatch bandwidth (five instructions per thread) as well as the execution resources. Except for FP instructions, instructions within a group are independent of one another. A taken branch terminates a group. Each instruction in the I-cache carries a start bit that controls dispatch group formation in the IDU. If the start bit associated with an instruction is set, that instruction starts a new group. Otherwise, it is part of a larger group of instructions that started earlier in the instruction stream. The POWER6 processor dispatch groups contain, at most, five instructions. An extensive set of rules from very simple to rather complex ones determine whether the start bit of a particular instruction is set on the basis of the instruction itself and the instructions immediately preceding it.A rule states what is not allowed to occur within a group; consequently, the first instruction that would violate a rule starts a new group.
		* A group cannot use more resources than are available in the POWER6 processor. Available resources are two FXUs, two LSUs, two FP or VMX units, and one branch unit.
		* A group cannot have a write-after-write dependency on any general-purpose register (GPR), FP register (FPR), CR field, or the FX exception register (XER).
		* A group cannot have a read-after-write dependency on the XER.
		* A group cannot have a read-after-write dependency on a GPR, except for an FX instruction or an FX load, followed by an FX store of the same register.
		* A group cannot have a read-after-write dependency on a CR field except for an FX instruction setting the CR field, followed by a conditional branch dependent on the CR field. Thus, the most important combination of a compare followed by a dependent conditional branch can be in one group.
		* A group cannot have an FP multiply–add (FMA) followed by an instruction reading the target FPR of the FMA.
		* A group cannot have an FP load followed by an FP store of the same FPR.
	* Branch execution—The POWER6 processor can predict up to eight branches in a cycle (up to the first taken branch in a fetch group of up to eight instructions). Prediction is made using a 16K-entry, 2-bit BHT; 8-entry, fully associative count cache; and a 6-entry link stack. The BHT and the count cache are shared between the two threads. Up to ten taken branches per thread are allowed to be pending in the machine. The POWER6 processor is optimized to execute branches as early as possible and as fast as possible. The pre-decode assists by precomputing the branch target and providing encoded information for the evaluation of conditional branches. Another unique design technique is allowing conditional branch execution to occur in any of three stages in the POWER6 processor execution pipeline,whereas instruction execution normally occurs in a fixed pipeline stage.
* IDU. Instruction dispatching, tracking, issuing, and completing are handled by the IDU. At the dispatch stage, all instructions from an instruction group of a thread are always dispatched together. Both threads can be dispatched simultaneously if all of the instructions from both threads do not exceed the total number of available execution units. The POWER6 processor can dispatch up to five instructions from each thread and up to seven instructions from both threads. In order to achieve high dispatch bandwidth, the IDU employs two parallel instruction dataflow paths, one for each thread. Each thread has an I-buffer that can receive up to eight instructions per cycle from the I-cache. Both I-buffers are read at the same time, for a maximum of five instructions per thread. Instructions from each thread then flow to the next stage, in which the non-FP unit (FPU) and VMX dependency-tracking and resource-determination logic is located. If all of the dependencies for instructions in the group are resolved, then the instruction group is dispatched. Otherwise, the group is stalled at the dispatch stage until all of the dependencies are resolved. Tracking of non-FPU and VMX instruction dependencies is performed by a target table, one per thread. The target table stores the information related to the whereabouts of a particular instruction in the execution pipe. As instructions flow from the I-buffer to the dispatch stage, the target information of those instructions is written into the target table. Subsequent FX instructions access the target table to obtain dependency data so that they can be dispatched appropriately. FPU and VMX instruction dependencies are tracked by the FPQ located downstream from the dispatch stage. FPU and VMX arithmetic instructions are dispatched to the FPQ. Each FPQ can hold eight dispatch groups, and each group can have two FPU or VMX arithmetic instructions. The FPQ can issue up to two arithmetic FPU or VMX instructions per cycle. In order to achieve zero-cycle load-to-use for load floats feeding arithmetic FPU instructions, the FPU instructions are staged six cycles after the dispatch stage through the FPQ to line up with load data coming from the LSU. If the load data cannot be written into the FPR upon arrival at the FPU, it is written into a 32-entry load target buffer (16 per thread). The load target buffer allows up to 16 load instructions per thread to execute ahead of arithmetic FP instructions, thus eliminating the effect of the six-cycle FP pipe stages. Arithmetic FPU instructions can also be issued out of order with respect to other FPU instructions. At most, eight FPU instructions can be issued out of order from the FPQ. A completion table is employed by the IDU to track a high number of instructions in flight. Each thread uses a ten-entry completion table. Each completion table entry holds the information necessary to track a cache-line’s worth of instructions (up to 32 sequential instructions). When a taken branch is detected in the fetching instruction stream, a new completion table entry is allocated for the instructions after the predicted taken branch. A new completion table entry is also allocated when an I-cache line is crossed. In effect, the completion table can track up to 320 instructions, or ten taken branches, per thread.
* FXU. The POWER6 core implements two FXUs to handle FX instructions and generate addresses for the LSUs. The FXU executes most instructions in a single pipeline stage. One of the signature features of the POWER6 processor FXU is that it supports back-to-back execution of dependent instructions with no intervening cycles required to bypass the data to the dependent instruction.
* BFU. The POWER6 core includes two BFUs, essentially mirrored copies, which have their register files next to each other to reduce wiring. To offset some of the cycle-time differences, an additional pipeline stage is used, but it is designed so that it does not have an impact on performance. New bypass paths are added to make up for the additional latency. Rather than waiting for the result to be rounded, an intermediate result is fed back to the beginning of the pipeline prior to rounding and prior to normalization.
* LSU. The LSU contains several subunits: the load/store address generation and execution; the L1 D-cache array; and the cache array supporting set-predict and directory arrays, address translation, store queue, load miss queue (LMQ), and data prefetch engine, which perform the following functions:
	* Load/store execution—In support of the POWER6 processor high-frequency design, the LSU execution pipeline employs a relatively simple dataflow with minimal state machines and hold states. Most load/store instructions are handled by executing a single operation. A hardware state machine is employed to assist in the handling of load/store multiple and string instructions and in the handling of a misaligned load or store. As a result, unaligned data within the 128-byte cache-line boundary is handled without any performance penalty. In the case of data straddling a cache line, the instruction is handled with two internal operations, with the partial data from each stitched together to provide the desired result.
	* L1 D-cache organization—The POWER6 core contains a dedicated 64-KB, eight-way, set-associative L1 D-cache. The cache-line size is 128 bytes, consisting of four sectors of 32 bytes each. The reload data bus from the L2 cache is 32 bytes. The cache line is validated on a sector basis as each 32-byte sector is returned. Loads can hit against a valid sector before the entire cache line is validated. The L1 D-cache has two ports that can support either two reads (for two loads) or one write (for a store or cache-line reload). Writes due to cache-line reloads have the highest priority and they block load/store instructions from being dispatched. Reads for executing loads have the next priority. Finally, if there are no cache-line reloads or load reads occurring, completed stores can be written from the store queue to the L1 D-cache. The L1 D-cache is a store-through design: All stores are sent to the L2 cache, and no L1 castouts are required.
	* Set predict—To meet the cycle time of the access path, a set-predict array is implemented. The set-predict array is based on the EA and is used as a minidirectory to select which one of the eight L1 D-cache sets contains the load data. Alternatively, the L1 D-cache directory array could be used, but it would take more time, as it is based on the real address (RA) and, thus, would require translation results from the ERAT. The set-predict array is organized as the L1 D-cache: indexed with EA(51:56) and eight-way set associative. Each entry or set contains 11 EA hash bits, 2 valid bits (one per thread), and a parity bit. The 11-bit EA hash is generated as follows: (EA(32:39) XOR EA(40:47)) plus EA(48:50). When a load executes, the generated EA(51:56) is used to index into the set-predict array, and EA(32:50) is hashed as described above and compared with the contents of the eight sets of the indexed entry. When an EA hash match occurs and the appropriate thread valid bit is active, the match signal is used as the set select for the L1 D-cache data.
## POWER6 Coherency Protocol
With a broadcast-based snooping protocol such as that found in the POWER5 processor, coherence traffic and the associated bandwidth required grow proportionally with the square of the system size. As system-packaging cost implications of this bandwidth become more important, alternatives to globally snooped, broadcastbased protocols become more attractive. Approaches such as directory-based NUMA (nonuniform memory access) schemes have become popular because they localize broadcasts to small nodes with directories that indicate when regions of memory owned by a given node are checked out to other nodes. This can greatly restrict traffic flow outside the node. For POWER6 technology, it was necessary to develop a single design that incorporates a robust, global broadcast-based protocol while also integrating a capability styled after directory-based NUMA. Significant innovations have been incorporated into the coherence protocol to address this challenge. In addition to the globally broadcast request, response, and notification transport, with its distributed management using specialized cache states, a localized (or scopelimited) broadcast transport mechanism is also integrated. Thus, a given request can be broadcast globally or locally.
Below table summarizes POWER6 processor coherency protocol of cache states.
![Pasted image 20230915100402.png](/assets/images/power/Pasted image 20230915100402.png)
![Pasted image 20230915100432.png](/assets/images/power/Pasted image 20230915100432.png)
Below tables summarizes the POWER6 processor cache states and scope-state implications.
![Pasted image 20230915103531.png](/assets/images/power/Pasted image 20230915103531.png)
The scope-state bit in memory is integrated into the redundant content for error correction already stored in memory, so no cost is added. For each 128-byte cache line, the bit indicates whether the line might be in use outside of the local scope where the memory resides. Since it is stored with the data bits, the state bit is automatically read or written whenever the data is read or written. The four new cache states provide a means of caching the scope-state bit in the L2 and L3 caches, either by itself or along with the data it covers. Note that when cached scope state is deallocated, it is typically cast out (i.e., written back) to memory. For cases in which the implied scope state might be global, the castout is functionally required to ensure that coherence is maintained. For cases in which the implied scope state is known to be local, the castout is optional, as it is desirable but not necessary to localize the broadcast scope for subsequent operations.
The combination of the scope-state bit in memory and the four new cache states provides a low-cost alternative to a NUMA directory and integrates cleanly into the nonblocking-broadcast distributed-coherence protocol. As some workloads localize well and others do not, the design of the POWER6 processor incorporates a number of predictors to determine whether a given coherence request should make a local attempt or immediately broadcast globally. For workloads that exhibit a high degree of processor-to-memory localization, and for workloads that have varying mixtures of locally resolvable traffic, laboratory results show that scope limited speculative snoop resolution is highly effective.
## POWER6 Cache hierarchy
The POWER6 processor cache hierarchy consists of three levels, L1, L2, and L3. The 64-KB L1 I-cache and 64-KB L1 D-cache are integrated in the core, within the IFU and the LSU, respectively. Each core is supported by a private 4-MB L2 cache, which is fully contained on the POWER6 chip. The two cores on a given chip may also share a 32-MB L3 cache. The controller for the L3 cache is integrated in the POWER6 chip, but the data resides off-chip. Below Table summarizes the organizations and characteristics of these caches.
![Pasted image 20230914170540.png](/assets/images/power/Pasted image 20230914170540.png)
The private 4-MB POWER6 processor L2 cache, which comprises 128-byte cache lines and is eight-way set associative, is organized differently than the POWER5 processor L2 cache. Instead of dividing into three independent address-hashed slices, the POWER6 processor L2 cache is operated by a single, centralized controller. The cache data arrays are organized as four interleaves, with each interleave containing a 32-byte sector of every cache line in the cache. Line fetch operations initiated by the core, castout read operations, and intervention read operations utilize all four interleaves in concert, but readmodify-write operations (performed on behalf of accumulated store-through operations initiated by the core) and writeback operations (which occur when the L2 cache is reloaded) operate only upon one or more 32-byte sectors within a given cache line and, therefore, utilize only the interleaves that are affected by the operations. The directories and coherence management resources are divided into two address-hashed slices. Each directory slice can accept either a core request, a snoop request, or an update operation every other core cycle. Unlike the POWER5 processor L2, which used separate read ports for core and snoop requests, the POWER6 L2 incorporates a sliding window to schedule snoop requests such that they do not collide with core requests, thereby eliminating the need for a second access port in order to reduce power and area.
Because the L1 D-cache is a store-through design in order to reduce accesses to the cache arrays, the L2 accumulates individual core store operations by employing an eight-entry, 128-byte-wide queue per directory slice. All stores gathered by a given entry are presented to the L2 cache with a single read-modify-write operation. This operation uses only the cache interleaves that are affected. To handle all fetch operations initiated by the core and read-modify-write operations initiated by the L2 store queue, the L2 can employ one of 16 read/claim (RC) machines per directory slice. RC machines manage coherence transactions for all core-initiated cacheable operations. The 32 total machines are needed to enable a sufficient number of outstanding prefetch operations to drive the memory interface to saturation while still handling fetch and store traffic. If a fetch operation hits in the L2 cache, the cache interleaves are read, and data is forwarded to the core. If a read-modify-write operation hits in the L2 cache, the impacted cache interleaves are updated.
If either of these operations misses the L2 cache, the L2 waits for a response from the L3 cache. If the operation hits in the L3, data is returned to the L2 (and possibly independently to the core). If the operation misses the L3, the L2 cache sends a request to the SMP coherence interconnect fabric, and data is eventually returned from another L2, L3, or memory via the SMP data interconnect fabric. For cases in which these operations result in the deallocation of a cache line from the L2 in order to install the newly requested cache line, the L2 must additionally employ one of four castout machines per directory slice to move data or state to the L3 victim cache or to memory. The eight total machines are needed to enable sufficient outstanding castout operations to drive the L3 cache write interface to saturation.
If data is returned from the L3 cache or via the SMP data interconnect fabric, in the case of a fetch, it is forwarded to the core and written to the L2 cache; in the case of a read-modify-write, it is written directly to the nonimpacted L2 cache interleaves and merged with store data prior to writing to the impacted L2 cache interleaves. To handle incoming snoop requests from the SMP coherence interconnect fabric, the L2 first consults the directory to determine whether it is required to take any action. If so, it can employ one of four snoop machines per directory slice to perform the required task. The eight total machines are needed to enable enough outstanding interventions to drive the datapath that lies between a likely pair of L2 caches to saturation. A snoop machine task might involve reading the data from the cache to send it, via the SMP data interconnect fabric, to another processor and possibly updating the directory state; or reading the data from the cache to send it to memory and invalidating the directory state; or simply updating or invalidating the directory state.
Since the L2 is private and there are two such L2 caches on a chip, it is possible that the data requested by a given core may be found in the L2 cache associated with the other core, leading to an intervention of data between two L2 caches on the same chip. To improve latency in such a scenario, a high-speed cache-to-cache interface was created. Whenever a fetch operation initiated by a core checks its own L2 directory, the operation is forwarded to the other L2 as well. If the operation misses in its own L2, the other L2 directory is checked, and if the line is found, it is read from the other L2 cache and forwarded on a high-speed interface back to the requesting core and L2. This significantly reduces the latency as compared with a normal intervention case.
The 32-MB POWER6 processor L3 cache, which is shared by both cores on a given chip, is 16-way associative and is composed of 128-byte lines. To handle all fetch and store operations initiated by the core that miss the L2 and must check the L3 cache, the L3 can employ one of eight read machines per directory slice, private to each L2 cache. The 16 total machines per L2 cache are needed to enable enough outstanding L3 fetch hit operations to drive the L3 cache read interface to saturation. If the data resides in the L3 cache, the read machine retrieves it and routes it back to the requesting L2 cache and core. Otherwise, a response is sent to the requesting L2 cache indicating that it should route the request to the SMP coherent interconnect fabric. To handle L2 castout write operations, the L3 can employ one of eight write machines per slice, shared by both L2 caches. The number of write machines is tied to the number of castout machines in order to reduce complexity and functional overhead. For cases in which an L2 castout write to the L3 results in the deallocation of a cache line from the L3 and a copy of the line must be moved to memory, the L3 must additionally employ an L3 castout machine associated with the write machine to move data and state to memory. The 16 total machines are needed to enable enough outstanding L3 castout operations to drive the memory write interface to saturation. Since the rule of exclusivity governing the relationship between the POWER5 processor L2 and L3 caches does not carry over to the POWER6 processor design, there are cases in which an L2 castout write must merge state information with a copy of the same cache line in the L3 cache. To handle incoming snoop requests from the SMP coherence interconnect fabric, the L3 first consults the directory to determine whether it is required to take any action with respect to the snoop operation. If so, it can employ one of four snoop machines per directory subslice to perform the required task. The reason for the number of machines and the possible tasks were described above in the previous section.
## POWER6 Memory subsystem
Each POWER6 chip includes two integrated memory controllers. A memory controller supports up to four parallel channels, each of which can be connected through an off-chip interface to a buffer chip. A channel supports a 2-byte read datapath, a 1-byte write datapath, and a command path that operates four times faster than the DRAM frequency, the fastest of which is 800-MHz DDR2 (double data rate 2) DRAM. Depending on the system configuration, one or both memory controllers may utilize two or four channels each. Each channel may have from one to four buffer chips daisy-chained together. For some system configurations, buffer chips are mounted on the system board, and industry-standard DIMMs (dual inline memory modules) are used. For other configurations, specialized DIMMs directly integrate the buffer chip.

## POWER6 I/O subsystem

The POWER6 processor I/O controller is built on the same architectural foundation as that of the POWER4 and POWER5 chips. A 4-byte off-chip read interface and a 4-byte off-chip write interface connect the POWER6 chip to an I/O hub chip. These interfaces operate at onehalf of the core frequency but can also run at lower frequency integer divisions of the core frequency to support previous-generation I/O hub chips. To facilitate concurrent maintenance, the POWER6 processor interrupt presentation function was redesigned to make it decentralized and it was mapped onto the SMP coherence interconnect fabric. A pipelined I/O highthroughput mode was added whereby DMA write operations initiated by the I/O controller are speculatively pipelined. This ensures that in the largest systems, inbound I/O throughput is not limited by the tenure of the coherence phase of the DMA write operations. Partial cache-line DMA write transactions have been redesigned to allow any arbitrary transfer size, from 1 byte to 128 bytes, to complete in a single coherence transaction.
## POWER6 SMP
Below shows the SMP links.
![Pasted image 20230726201124.png](/assets/images/power/Pasted image 20230726201124.png)
the POWER6 processor first-level nodal structure is composed of up to four POWER6 chips. Relying on the traffic reduction afforded by the innovations in the coherence protocol to reduce packaging overhead, coherence and data traffic share the same physical links by using a time-division-multiplexing (TDM) approach. With this approach, the system can be configured either with 67% of the link bandwidth allocated for data and 33% for coherence or with 50% for data and 50% for coherence. Within a node, the shared links are fully connected such that each chip is directly connected to all of the other chips in the node. There are five 8-byte off-chip SMP interfaces on the POWER6 chip (which operate at half the processor frequency). Three are dedicated for interconnecting the first-level nodal structure, which is composed of up to four POWER6 chips.
![Pasted image 20230914163617.png](/assets/images/power/Pasted image 20230914163617.png)
Instead of using parallel rings, POWER6 processor based systems can connect up to eight nodes with a fully connected topology, in which each node is directly connected to every other node. This provides perfect isolation, since any two nodes can interact without involving any other nodes. Also, system latencies do not increase as a system grows from two to eight nodes, yet aggregate system bandwidth increases faster than system size. with a four-chip node, eight such links are available for direct node-to-node connections. Seven of the eight are used to connect a given node to the seven other nodes in an eight-node 64-way system. The five off-chip SMP interfaces on the POWER6 chip protect both coherence and data with SECDED ECCs.
![Pasted image 20230914163644.png](/assets/images/power/Pasted image 20230914163644.png)
## POWER6 RAS
The POWER6 core RU contains a copy of the designed state of the processors. Checkpoints of the data in the core are constantly saved in the RU. The data is protected by ECC. The L2 cache includes the L1 I-cache and D-cache. As a result, the L1 contains temporary data that is protected by parity. Persistent data is in the L2 and L3 caches and is protected by ECC. Major instruction flow, dataflow, and some control logic are checked using parity, residue, or duplication. When an error is detected, the core is stopped and is prevented from communicating with the rest of the chip. Checkpoint data associated with the last successfully saved instruction from the RU is read and re-stored to the affected core. The L1 I- and D-caches are cleared because the persistent data is protected and stored outside of the core. All translation tables in the core are cleared, as snooping actions are not seen by the core after it has stopped. The processor then resumes at the re-stored checkpoint. If it is determined that the core failure is a hard error (persistent), the content of the RU can be transferred to another idle core in the system, and the task can be restarted on the new core from the checkpoint.

# 9. Power 7
Below Figure shows the Power7 chip, including the eight processor cores, each having 12 execution units capable of running four-way SMT. To feed these eight high-performance cores, the Power7 processor has two memory controllers, one on each side of the chip. Each memory controller supports four channels of DDR3 memory. These eight channels together provide 100 Gbytes per second (GB/s) of sustained memory bandwidth. At the top and bottom of the chip are the SMP links, providing 360 GB/s of coherence bandwidth, which lets the Power7 chip efficiently scale up to 32 sockets.
![Pasted image 20230915155142.png](/assets/images/power/Pasted image 20230915155142.png)

## POWER7 Core
POWER7 has advanced branch prediction and prefetching capabilities, as well as deep out-of-order execution capability for significant ST performance gain. At the same time, it has sufficient resources to efficiently support four threads per core. The core can dynamically change mode among ST, two-way SMT (or SMT2), and SMT4 modes.
The core consists primarily of the following six units: instruction fetch unit (IFU), instruction-sequencing unit (ISU), LSU, FXU, VSU, and decimal FPU (DFU). The IFU contains a 32-KB instruction cache (I-cache), and the LSU contains a 32-KB data cache (D-cache), which are each backed up by a tightly integrated 256-KB unified L2 cache. In a given cycle, the core can fetch up to eight instructions, decode and dispatch up to six instructions, and issue and execute up to eight instructions. There are 12 execution units within the core, i.e., two fixed point, two LS, four double-precision (DP) floating-point pipelines, one vector, one branch, one condition register (CR) logical, and one decimal floating-point (DFP) pipeline. The two LS pipes have the additional capability to execute simple FX operations. Each of the four floating-point pipelines is capable of executing DP multiply–add operations, accounting for eight FLOPs per cycle per core. The DFU, which is first introduced in POWER6, accelerates many commercial applications.
![Pasted image 20230915155118.png](/assets/images/power/Pasted image 20230915155118.png)
Below shows the POWER7 processor core instruction flow.
![Pasted image 20230915155003.png](/assets/images/power/Pasted image 20230915155003.png)
To reduce power and area, a partitioned approach to the SMT4 design was incorporated. With this approach, a pair of threads is supported from one physical general-purpose register (GPR) file that feeds one fixed-point unit (FXU) pipeline and one load/store unit (LSU) pipeline, and another pair of threads is supported from a separate physical GPR file that feeds a separate FXU pipeline and LSU pipeline. With this approach, POWER7 can efficiently rename registers for twice as many threads with a total of physical GPR file entries that is less than that of POWER5*, which only supported SMT2.
In earlier out-of-order machines, such as POWER4 and POWER5, the register rename structure for the GPR, floating-point register (FPR), and vector register (VR) was separate, which required a large number of entries. In POWER7, these were all merged into one unified rename structure with a total of 80 entries, matching the maximum number of outstanding nonbranch instructions between instruction dispatch and completion. This significantly reduces the area and power of the out-of-order machine. In addition, in earlier machines, the issue queues for floating-point instructions and fixed-point (FX) (along with load and store) instructions were separate. In POWER7, these have been combined to reduce the area and power. The new issue queue is called unified issue queue (UQ). To achieve high frequency, the large UQ is physically implemented as two 24-entry queues, i.e., UQ0 and UQ1.
The floating-point unit (FPU) and the vector media extension (VMX) unit were separate in the POWER6 design. In POWER7, these two units are merged into one unit called the vector and scalar unit (VSU), which also incorporates the new vector and scalar extension (VSX) architecture that allows two-way single-instruction multiple-data (SIMD) FLOPs out of a 64-entry architected register file, with 128 bits per entry. POWER7 did not increase the number of issue ports over POWER6 but still supports the new VSX instruction sets and can execute four FX operations and eight FLOPs per cycle.
Both of the Level 1 (L1) instruction and data caches are highly banked, which allows concurrent read and write accesses to the cache, whereas an individual bank can only support either two reads or one write in a given cycle. This significantly reduces the area and power for the caches, while most of the time reads and writes (as long as they go to different banks) can occur concurrently.
Each POWER7 chiplet (i.e., a POWER7 core with its Level 2 (L2) and local L3 cache) is designed to be on a separate power domain, with asynchronous boundary with the PowerBus to which it is connected. This allows each chiplet to have independent voltage and frequency slewing for advanced power management.

### POWER7 instruction fetch and decode pipe stages
The POWER7 core has a dedicated 32-KB four-way set-associative I-cache. It is a 16-way banked design to avoid read and write collisions. Late select of the four ways is predicted using a 64-entry instruction effective address directory (IEADIR), which provides fast prediction for way selection to choose a fetch line from the four ways. A traditional full I-cache directory (IDIR) is also accessed in parallel to confirm the set selection prediction in the next cycle. Fast address translation is supported by a 64-entry instruction effective-to-real-address translation (IERAT) table. The IERAT supports threads 0 and 2 in the first 32 entries and threads 1 and 3 in the bottom 32 entries. The IERAT directly supports 4 and 64 KB, and larger pages (64 MB and 16 GB) are supported by dynamically mapping them into 64-KB pages as needed.
The IFU fetches instructions into the L1 I-cache from the L2 unified cache. Each fetch request for instructions from the L2 returns as four sectors of 32 bytes each. These fetches are either demand fetches that result from L1 I-cache misses or instruction prefetches. For each demand fetch request, the prefetch engine initiates up to two additional L2 prefetches for the two sequential cache lines following the demand fetch. Demand and prefetch requests are made for all four instruction threads independently, and data may return in any order, including interleaving of sectors for different cache lines. Up to four instruction fetch requests can be outstanding from the core to the L2 cache. Instruction prefetching is supported in the ST and SMT2 modes only. Up to two sequential lines are allowed to be prefetched in the ST mode and one per thread in the SMT2 mode.
When instructions are fetched from the memory subsystem, two cycles are taken to create predecode bits and parity for each of the instructions, before the instructions are written into the L1 I-cache. The predecode bits are used to scan for taken branches, help group formation, and denote several exception cases. Branch instructions are modified in these stages to help generate target addresses during the branch scan process. The modified branch instruction, with a partially computed target address, is stored in the L1 I-cache. Three cycles after the data arrives on the L2 interface, the 32 bytes are written into the I-cache. If the requesting thread is waiting for these instructions, they are bypassed around the cache to be delivered to the instruction buffers (IBUFs) and the branch scan logic.
Instruction fetch address registers (IFARs) track program counter addresses for each thread. On each cycle, the IFAR for one of the threads is selected to provide the fetch address to the I-cache complex and the branch prediction arrays. The I-cache fetch process reads up to eight instructions per cycle from the I-cache and writes them into the IBUFs where they are later formed into dispatch groups. Thread priority, cache miss pending, IBUF fullness, and thread balancing metrics are used to determine which thread is selected for fetching in a given cycle.
The direction of a conditional branch is predicted using a complex of branch history tables (BHTs), consisting of an 8-K entry local BHT (LBHT) array, a 16-K entry global BHT (GBHT) array, and an 8-K entry global selection (GSEL) array. These arrays together provide branch direction predictions for all the instructions in a fetch group in each cycle. A fetch group can have up to eight instructions, all of which can be branches. These arrays are shared by all active threads. The local array is directly indexed by 10 bits from the instruction fetch address. The GBHT and GSEL arrays are indexed by the instruction fetch address hashed with a 21-bit global history vector (GHV) folded down to 11 bits, one per thread. The value in the GSEL entry is used to choose between the LBHT and the GBHT for the direction prediction of each individual branch. All the BHT entries consist of 2 bits, with the higher order bit determining direction (taken or not taken) and the lower order bit providing hysteresis.
Branch target addresses are predicted using the following two mechanisms: 1) Indirect branches that are not subroutine returns are predicted using a 128-entry count cache, which are shared by all active threads. The count cache is indexed using an address obtained by doing an XOR of 7 bits, each from the instruction fetch address and the GHV. Each entry in the count cache contains a 62-bit predicted address along with two confidence bits. The confidence bits are used to determine when an entry is replaced if an indirect branch prediction is incorrect. 2) Subroutine returns are predicted using a link stack, one per thread. Whenever a branch-and-link instruction is scanned, the address of the next instruction is pushed down in the link stack for that thread. The link stack is popped whenever a branch-to-link instruction is scanned. The POWER7 link stack allows for one speculative entry to be saved in the case where a branch-and-link instruction is scanned and then flushed due to a mispredicted branch that appeared earlier in the program order. In the ST and SMT2 modes, each thread uses a 16-entry link stack. In the SMT4 mode, each thread uses an eight-entry link stack. In the ST mode, when a taken branch is encountered, the three-cycle branch scan causes two dead cycles where no instruction fetch takes place. To mitigate the penalty incurred by taken branches, a BTAC was added to track the targets of direct branches. The BTAC uses the current fetch address to predict the fetch address two cycles in the future. When correct, the pipelined BTAC will provide a seamless stream of fetch addresses that can handle a taken branch in every cycle. If the effect of a conditional branch is only to conditionally skip over a subsequent FX or LS instruction and the branch is highly unpredictable, POWER7 can often detect such a branch, remove it from the instruction pipeline, and conditionally execute the FX or LS instruction. The conditional branch is converted to an internal resolve operation, and the subsequent FX or LS instruction is made dependent on the resolve operation. When the condition is resolved, depending on the taken or not-taken determination of the condition, the FX or LS instruction is either executed or ignored. This may cause a delayed issue of the FX or LS instruction, but it prevents a potential pipeline flush due to a mispredicted branch.
Fetched instructions go to the branch scan logic and to the IBUFs. An IBUF can hold up to 20 entries, each four instructions wide. In the SMT4 mode, each thread can have five entries, whereas, in ST and SMT2 modes, a thread can have ten entries. Special thread priority logic selects one thread per cycle for group formation. Groups are formed by reading a maximum of four nonbranches and two branches from the IBUF of the thread. Unlike the POWER4 and POWER5 processors, branches do not end groups in POWER7.
After group formation, the instructions are either decoded or routed to special microcode hardware that breaks complex instructions into a series of simple internal operations.
![Pasted image 20230915154836.png](/assets/images/power/Pasted image 20230915154836.png)
### POWER7 ISU overview
Below shows the ISU which dispatches instructions, renames registers, issues instructions, completes instructions, and handles exception conditions.
![Pasted image 20230915154927.png](/assets/images/power/Pasted image 20230915154927.png)
Which copy of the issue queue, physical register file, and functional unit will be used by an operation depends on the multithreading mode of the processor core. In the ST and SMT2 modes, the two physical copies of the GPR have identical contents. Instructions from the thread(s) can be dispatched to either one of the issue queue halves (UQ0 or UQ1) in these modes. Load balance across the two issue queue halves is maintained by dispatching alternate instructions of a given type from a given thread to a UQ half. In an SMT4 mode, the two copies of the GPR have different contents. FX and load/store (LS) operations from threads T0 and T1 can only be placed in UQ0, can only access GPR0, and can only be issued to FX0 and LS0 pipelines. FX and LS operations from threads T2 and T3 can only be placed in UQ1, can only access GPR1, and can only be issued to FX1 and LS1 pipelines.
most VSU operations can be dispatched to either UQ0 or UQ1 in all modes (single thread, SMT2, SMT4), with the following exceptions: 1) VMX floating point and simple and complex integer operations can only be dispatched to UQ0; 2) permute (PM), decimal floating point, and 128-bit store operations can only be dispatched to UQ1; 3) VSU operations dispatched to UQ0 always execute on vector scalar pipeline 0 (VS0); and 4) VSU operations dispatched to UQ1 always execute on VS1 pipeline.
The POWER7 processor dispatches instructions on a group basis and can dispatch a group from one thread at a time to the ISU. Register renaming is done using the mapper logic (see Figure 4) before the instructions are placed in the issue queues. The following registers are renamed in POWER7: GPR, vector and scalar register (VSR), exception register (XER), CR, floating-point status and control register (FPSCR), link, and count. The GPR and VSR share a pool of 80 rename entries. The CRs are mapped onto 56 physical registers. The XERs are mapped onto 40 physical registers, and one nonrenamed register. The Link and Count registers are mapped onto 24 physical registers. The FPSCR is renamed using a 20-entry buffer to keep the state of the FPSCR associated with each group of instructions. Each of the aforementioned resources has a separate rename pool that can be independently accessed and shared by all active threads. Instructions that update more than one destination register are broken into subinstructions.
The ISU also assigns a load tag (LTAG) and a store tag (STAG) to manage load and store instruction flow. The LTAG corresponds to a pointer to the load-reorder-queue (LRQ) entry assigned to a load instruction. The STAG corresponds to a pointer to the store-reorder-queue (SRQ) entry assigned to a store instruction. This is also used to match the store data instruction with the store address instruction in the SRQ. A virtual STAG/LTAG scheme is used to minimize dispatch holds due to running out of physical SRQ/LRQ entries. When a physical entry in the LRQ is freed, a virtual LTAG will be converted to become a real LTAG. When a physical entry in the SRQ is freed, a virtual STAG will be converted to become a real STAG. Virtual STAGs or LTAGs are not issued to the LSU until they are subsequently marked as being real in the issue queue. The ISU can assign up to 63 LTAGs and 63 STAGs to each thread.
POWER7 employs three separate issue queues: a 48-entry UQ, a 12-entry branch issue queue (BRQ), and an 8-entry CR queue (CRQ). Dispatched instructions are saved in the issue queues and then issued to the execution unit one cycle after dispatch at the earliest for the BRQ or CRQ and two cycles after dispatch at the earliest for the UQ. The BRQ and CRQ are shifting queues, where dispatched instructions are placed at the top of the queue and then trickle downward toward the bottom of the queue. To save power, the UQ is implemented as a nonshifting queue and managed by queue position pointers. The queue position pointers are shifted, but the UQ entries are not shifted, which significantly reduces the switching power in the large UQ. Instructions can issue in order or out of order from all of these queues, with higher priority given to the older ready instructions for maximizing performance. An instruction in the issue queue is selected for issuing when all source operands for that instruction are available. In addition, the STAG and the LTAG must have real entries for a load or store instruction before it can be issued. For the BRQ and CRQ, instruction dependences are checked by comparing the destination physical pointer of the renamed resource against all outstanding source physical pointers. For the UQ, dependences are tracked using queue pointers via a dependence matrix. The issue queues together can issue a total of eight instructions per cycle, i.e., one branch, one CR logical, two FX instructions to the FXU, two LS or two simple FX instructions to the LSU, and two vector–scalar instructions to the VSU.
The BRQ contains only branch instructions, and it receives two branches per cycle from the dispatch stage and can issue one branch instruction per cycle for execution to the IFU. The CRQ contains the CR logical instructions and moves from SPR instructions, for the IFU, the ISU, and the pervasive control unit. The CRQ can receive two instructions per cycle and can issue one instruction per cycle to the IFU. The UQ is implemented as a 48-entry queue that is split into two halves of 24 entries each. It contains all instructions that are executed by the FXU, LSU, VSU, or DFUs. The top half of the queue contains instructions for FX0, LS0, and VS0 pipelines including VMX integer instructions. The bottom half of the queue contains instructions for FX1, LS1, and VS1 pipelines including DFP, VMX PM, and the VSU 128-bit store instructions. Appropriate instructions are steered at the dispatch stage to the appropriate half of the UQ. The UQ can receive up to four instructions per cycle per UQ half. The 64-bit VSU store instructions are split into an address generation (AGEN) operation and a data steering operation during instruction dispatch, and a total of eight such operations can be written into UQ in a given cycle. The relative age of the instructions in the UQ is determined by an age matrix since the UQ is a nonshifting queue, which is written at dispatch time. Each half of the UQ can issue one FX, one LS, and one VS instruction per cycle for a total of six instructions per cycle. Speculative issues can occur, for example, when an FX operation dependent on a load operation is issued before it is known that the load misses the D-cache or the data effective-to-real-address translation (D-ERAT). On a misspeculation, the instruction is rejected and reissued a few cycles later. Simple FX instructions may be selected for issue to the LSU for improved FX throughput, with the same latency as a load operation from L1 D-cache. The ISU is responsible to track and complete instructions. POWER7 employs a global completion table (GCT) to track all in-flight instructions after dispatch. Instructions in the core are tracked as groups of instructions and, thus, will dispatch and complete as a group. The GCT has 20 entries, which are dynamically shared by all active threads. Each GCT entry corresponds to a group of instructions, with up to four nonbranch and up to two branch instructions. This allows the GCT to track a maximum of 120 in-flight instructions after dispatch. Each GCT entry contains finish bits for each instruction in the group. At dispatch, the finish bits are set to reflect the valid instructions. Instructions are issued out of order and speculatively executed. When an instruction has successfully executed (without a reject), it is marked as finished. When all the instructions in a group are marked finished, and the group is the oldest for a given thread, the group can complete. When a group completes, the results of all its instructions are made architecturally visible, and the resources held by its instructions are released. The POWER7 core can complete one group per thread pair (threads 0 and 2 form one pair, whereas threads 1 and 3 form the other pair) per cycle, for a maximum total of two group completions per cycle. When a group is completed, a completion group tag (GTAG) is broadcasted so that resources associated with the completing group can be released and reused by new instructions. Flush generation for the core is handled by the ISU. There are many reasons to flush out speculative instructions from the instruction pipeline such as branch misprediction, LS out-of-order execution hazard detection, execution of a context synchronizing instruction, and exception conditions. The completion unit combines flushes for all groups to be discarded into a 20-bit mask, i.e., 1 bit for each group. The completion unit also sends out the GTAG for partial-group flushes, which occurs when the first branch is not the last instruction in the group, and it mispredicts, causing a need to flush all subsequent instructions from the thread. A 4-bit slot mask accompanies the partial flush GTAG to point out which instructions in the group need to be partially flushed. All operations related to the canceled groups are discarded.
### LSU microarchitecture
LSU contains two symmetric LS execution pipelines (LS0 and LS1), each capable to execute a load or a store operation in a cycle. Below shows the microarchitecture for an LSU pipeline, which contains several subunits, i.e., LS AGEN and execution, SRQ and store data queue (SDQ), LRQ, load miss queue (LMQ), address translation mechanism, which includes the D-ERAT, ERAT miss queue, segment lookaside buffer (SLB) and TLB, and the L1 D-cache array with its supporting set predict and data directory (DDIR) arrays, and the data prefetch request queue (PRQ) engine.
![Pasted image 20230915154725.png](/assets/images/power/Pasted image 20230915154725.png)
* LS execution. In the ST and SMT2 modes, a given LS instruction can execute in either pipeline. In the SMT4 mode, instructions from threads 0 and 1 execute in pipeline 0, whereas instructions from threads 2 and 3 execute in pipeline 1. Instructions are issued to the LSU out of order, with a bias toward the oldest operations first. Stores are issued twice; an AGEN operation is issued to the LSU, whereas a data steering operation is issued to the FXU or the VSU. Main dataflow buses into and out of the LSU include 32-byte reload data from the L2 cache and 16-byte store data to the L2 cache, 16-byte load data per execution pipeline to the VSU (with a tap off of 8-byte load data per execution pipeline to the FXU), one 16-byte store data from the VSU and 8-byte store data per execution pipeline from the FXU. POWER7 L1 D-cache size is 32 KB, which resulted in a reduction in the D-cache access latency. FX loads have a two-cycle load-to-use latency, that is, only one cycle of bubble (which is a cycle in the pipeline during which no useful work is done) is introduced between a load and a dependent FXU operation. The VSU loads have a three-cycle load-to-use latency, that is, two cycles of bubbles are introduced between a load and a dependent VSU operation. Each LSU pipeline can also execute FX add and logical instructions, allowing more FX execution capability for the POWER7 core and greater flexibility to the ISU in the issuing of instructions.
* LS ordering. The LSU must ensure the effect of architectural program order of execution of the load and store instructions, although the instructions can be issued and executed out of order. To achieve that, LSU employs two main queues: the SRQ and the LRQ. The SRQ is a 32-entry real-address-based content-addressable memory (CAM) structure. Each thread has 64 virtual entries that are available, allowing 64 outstanding stores to be dispatched per thread. A total of 32 outstanding stores may be issued since a real physical SRQ entry is required for the store to be issued. The SRQ is dynamically shared among the active threads. An SRQ entry is allocated at issue time and deallocated after the completion point when the store is written to the L1 D-cache or sent to the L2 cache. For each SRQ entry, there is a corresponding SDQ entry of 16 bytes. Up to 16 bytes of data for a store instruction can be sent to the L2 cache (and also written to the L1 D-cache on a hit) in every processor cycle. Store forwarding is supported, where data from an SRQ entry is forwarded to an inclusive subsequent load, even if the store and load instructions are speculative. Like the SRQ, the LRQ is a 32-entry real-address-based CAM structure. Sixty-four virtual entries per thread are available to allow a total of 64 outstanding loads to be dispatched per thread. A total of 32 outstanding loads may be issued since a real physical LRQ entry is required for the load to be issued. The LRQ is dynamically shared among the threads. The LRQ keeps track of out-of-order loads, watching for hazards. Hazards generally exist when a younger load instruction executes out of order before an older load or store instruction to the same address (in part or in whole). When such a hazard is detected, if specific conditions exist, the LRQ initiates a flush of the younger load instruction and all its subsequent instructions from the thread, without having an impact on the instructions from other threads. The load is then refetched from the I-cache and reexecuted, ensuring proper LS ordering.
* Address translation. During program execution, the EAs are translated by the first level translation into 46-bit real addresses that are used for all addressing in the cache and memory subsystem. The first level translation consists of two 64-entry D-ERAT cache and a 64-entry IERAT. In case of a miss in the ERAT cache (data or instruction), the second level translation is invoked to generate the translation. The second level translation consists of a 32-entry-per-thread SLB and a 512-entry TLB that is shared by all active threads.Effective addresses are first translated into 68-bit virtual addresses using the segment table, and the 68-bit virtual addresses are then translated into 46-bit real addresses using the page frame table. While segment table and page frame tables are large and reside in main memory, a 32-entry-per-thread SLB is maintained to keep entries from the segment table to translate from effective to virtual address, and a 512-entry TLB is maintained to keep the recently used entries from the page frame table to translate from virtual to real addresses. POWER7 supports two segment sizes, i.e., 256 MB and 1 TB, and four page sizes, i.e., 4 KB, 64 KB, 16 MB, and 16 GB. The D-ERAT is a 64-entry fully associative CAM-based cache. Physically, there are two identical copies of the D-ERAT, associated with the two LSU pipelines. In the ST and SMT2 modes, since instructions from the thread(s) can go to either LS0 or LS1 pipeline, the two copies of the D-ERAT are kept in sync with identical contents. Therefore, in the ST and SMT2 modes, logically, there are a total of 64 entries available. In the SMT2 mode, the entries are dynamically shared between the two threads. In the SMT4 mode, since the two LSU pipelines are split between the two thread pairs, the two physical copies of the D-ERAT have different contents, i.e., threads 0 and 1 dynamically share one physical 64-entry D-ERAT (associated with LS0 pipe), and threads 2 and 3 dynamically share the other physical 64-entry D-ERAT (associated with LS1 pipe), for a total of 128 logical entries. Each D-ERAT entry translates 4-KB, 64-KB, or 16-MB pages. Pages of 16 GB are installed as multiple 16-MB pages. The D-ERAT employs a binary tree least recently used (LRU) replacement policy. The SLB is a 32-entry-per-thread fully associative CAM-based buffer. Each SLB entry can support 256 MB or 1 TB segment sizes. The multiple pages per segment (MPSS) extension of PowerPC architecture is supported in POWER7. With MPSS, a segment with a base page size of 4 KB can have 4-KB, 64-KB, and 16-MB pages to be concurrently present in the segment. For a segment base page size of 64 KB, the segment can have 64-KB and 16-MB pages concurrently. The SLB is managed by the operating system, with the processor generating a data or instruction segment interrupt when an SLB entry needed for translation is not found. The TLB is a 512-entry four-way set-associative buffer. The TLB is managed by hardware and employs a true LRU replacement policy. There can be up to two concurrent outstanding table-walks for TLB misses. The TLB also provides a hit-under-miss function, where the TLB can be accessed, and it returns translation information to the D-ERAT, while a table-walk is in progress. In POWER7, each TLB entry is tagged with the logical partition (LPAR) identity. For a TLB hit, the LPAR identity of the TLB entry must match the LPAR identity of the active partition running on the core.
* L1 data cache organization. POWER7 contains a dedicated 32-KB eight-way set-associative banked L1 D-cache. The cache line size is 128 bytes consisting of four sectors of 32 bytes each. There is a dedicated 32-byte reload data interface from the L2 cache, which can supply 32 bytes of data in every processor cycle. The cache line is validated on a sector basis as each 32-byte sector is returned from memory subsystem. Loads can hit against a valid sector before the entire cache line is validated. The L1 D-cache has three ports––two read ports (for two load instructions) and one write port (for a store instruction or a cache line reload). A write has higher priority over a read, and a write for a cache line reload has higher priority than a write for a completed store instruction. The L1 D-cache consists of four physical macros organized by data bytes, each macro partitioned into 16 banks based on the EA bits, for a total of 64 banks. The cache banking allows for one write and two reads to occur in the same cycle, as long as the reads are not to the same bank(s) as the write. If a read has a bank conflict with a write, the load instruction is rejected and reissued. A 32-byte cache line reload spans eight banks, whereas a completed store instruction spans from one to four banks, depending on data length. The L1 D-cache is a store-through design; all stores are sent to the L2 cache, and no L1 cast-outs are required. The L1 D-cache is not allocated on a store miss; the store is just sent to the L2 cache. The L1 D-cache is inclusive of the L2 cache. The L1 D-cache has byte-write capability of up to 16 bytes within a given 32-byte sector in support of store instructions. The L1 D-cache is indexed with the EA bits. The L1 D-cache directory employs a binary tree LRU replacement policy. Being 32 KB and eight-way set-associative results in 4 KB per set, requiring up to EA bit 52 to be used to index into the L1 D-cache. A set predict array is used to reduce the L1 D-cache load hit latency. The set predict array is based on EA and is used as a minidirectory to select which one of the eight L1 D-cache sets contains the load data. The set predict array is organized as the L1 D-cache: indexed with EA(52:56) and eight-way set-associative. Each entry contains 11 hash bits obtained from hashing bits EA(33:51), valid bits per thread, and a parity bit. When a load executes, the generated EA(52:56) is used to index into the set predict array, and EA(33:51) is hashed and compared with the contents of the eight sets of the indexed entry. When an EA hash match occurs and the appropriate thread valid bit is active, the match signal is used as the set select for the L1 D-cache data. If there is no EA hash match, it indicates a cache miss. However, an EA hash match does not necessarily mean a cache hit. For cache hit determination, the EA is used to look up in the L1 data cache directory for the real address and then compare this real address with the real address obtained from the ERAT for the given EA. When a cache line is validated, the default is to enter in a shared mode where all thread valid bits for the line are set. A nonshared mode is dynamically entered on an entry-by-entry basis to allow only one thread valid bit to be active. This is beneficial to avoid thrashing among the threads, allowing the same EA hash to exist for each thread at the same time.
* Load miss handling. Loads that miss the L1 D-cache initiate a cache line reload request to the L2 cache, release the issue queue entry, and create an entry in the LMQ to track the loading of the cache line into the L1 D-cache and also to support the forwarding of the load data to the destination register. When the load data returns from the L2 cache, it gets higher priority in the LSU pipeline, and the data is transferred to the destination register. The LMQ is real address based and consists of eight entries, dynamically shared among the active threads. The LMQ tracks all cache line misses that result in reload data to the L1 D-cache, which also includes data prefetch requests and data touch instructions, in addition to load instructions. The LMQ supports load merging, where up to two load instructions (of the same or different threads) can be associated with a given LMQ entry and cache line reload request. The LMQ can support multiple misses (up to eight) to a given L1 D-cache congruence class.
### POWER7 FXU overview
The FXU comprises of two identical pipelines (FX0 and FX1). As shown in below figure, each FXU pipeline consists of a multiport GPR file; an arithmetic and logic unit (ALU) to execute add, subtract, compare, and trap instructions; a rotator to execute rotate, shift, and select instructions; a count (CNT) leading zeros unit; a bit-select unit (BSU) to execute bit PM instruction; a divider (DIV); a multiplier (MULT); and a miscellaneous execution unit (MXU) to execute population count, parity, and binary-coded decimal assist instructions. All SPRs that are local to the FXU pipeline are stored in SPR. Certain resources such as the FX XER file are shared between the two pipelines.
![Pasted image 20230915154552.png](/assets/images/power/Pasted image 20230915154552.png)
The most frequent FX instructions are executed in one cycle, and dependent operations may issue back to back to the same pipeline, if they are dispatched to the same UQ half (otherwise, one cycle bubble is introduced). Other instructions may take two, four, or a variable number of cycles. At the heart of each FXU pipeline is a GPR file with 112 entries, which holds the architected registers and the renamed registers for up to two threads. The GPR has four read ports, two supplying operands for the FX pipeline, and two supplying AGEN operands to the attached LSU pipeline. Two physical write ports are clocked twice per cycle (double-pumped), giving four logical write ports, two capturing results from the two FX pipelines, and the other two from the two data cache read ports in the LSU. Double pumping the write ports reduces power consumption and the size of the GPR macro, which is important for shortening the length of critical wires that must traverse it. Contents of the two GPR files in each pipeline are managed by the ISU to be identical in the ST and SMT2 modes but distinct in the SMT4 mode. That is, in the SMT4 mode, the GPR in one pipeline contains the architected and renamed registers for one pair of threads, whereas the GPR in the other pipeline contains the registers for the other pair of threads.The latency between a compare instruction and a dependent branch instruction is often a significant performance detractor for many workloads. To reduce this latency, each FXU pipeline has a fast compare custom macro that calculates the condition code from a compare instruction faster than the ALU, resulting in a back-to-back issue in most cases for a compare, followed by a branch instruction.
* Vector and scalar instruction execution. The POWER7 VSU implements the new VSX architecture introducing 64 architected registers. With dual issue of two-way SIMD floating-point DP instructions, the performance in FLOPs per cycle per core is doubled in comparison to POWER6. In addition, the VSU of the POWER7 processor merges the previously separate VMX unit and binary FPU (BFU) into a single unit for area and power reduction. Furthermore, the POWER6 DFU is attached to the VSU as a separate unit, sharing the issue port with the VS1 pipeline.
Below shows the VSU pipeline diagram.
![Pasted image 20230918150731.png](/assets/images/power/Pasted image 20230918150731.png)
## POWER7 Cache hierarchy
the POWER7 cache hierarchy includes a shared 32-MB L3 cache comprised of the 4-MB local L3 regions from the eight cores that reside on the processor chip. The eDRAM requires only one-fifth of the standby energy used by a traditional SRAM cell, while using less than one-third the area.
![Pasted image 20230915155341.png](/assets/images/power/Pasted image 20230915155341.png)
POWER7 L2 and L3 caches support the same 13-cache-state protocol as the POWER6 design point. While no new cache states have been added for POWER7, new coherent operations are supported. One such operation is cache injection. An I/O device performing a direct memory access (DMA) write operation may target the operation to the cache, instead of to memory. If a given core’s L2 cache or local L3 region owns a copy of the targeted cache line (i.e., holds the line in an M, ME, MU, T, TE, TN, or TEN cache state), the data will be installed into the local L3 region. Additionally, new heuristics have been developed, which further exploit the semantic content reflected by the existing cache states. One of these, which is called the partial victim cache management policy, reduces energy usage as data moves between a given L2 cache and its associated local L3 region.
Below table summarizes the POWER7 cache states.
![Pasted image 20230915154407.png](/assets/images/power/Pasted image 20230915154407.png)
the barrier synchronization register (BSR) facility originally implemented in POWER5 and POWER6 processors has been virtualized in the POWER7 processor. Within each system, multiple megabytes of main storage may be classified as BSR storage and assigned to tasks by the virtual memory manager. The BSR facility enables low-latency synchronization for parallel tasks. Writes to BSR storage are instantaneously broadcast to all readers, allowing a designated master thread to orchestrate the activities of workers threads in a low-latency fine-grained fashion. This capability is particularly valuable for improving parallel speedups in certain HPC environments.
* L2 Cache. Store-through traffic from the core represents the bulk of the traffic managed by the L2 cache. A fully associative 16-deep 32-byte entry store cache absorbs every individual store executed in the core or up to 16 bytes of store traffic every core cycle. Up to four of the 32-byte store cache entries, comprising updates to the same 128-byte coherence granule, can be grouped together into a single simultaneous coherence dispatch by the L2 scheduling logic.
* L3 Cache. A 4-MB L3 region is comprised of 32 ultradense high-speed eDRAM macros. The eDRAM macro has access latency and cycle time characteristics slightly worse than conventional 6T SRAM. As such, the combined effects of the eDRAM access latency, the overhead of waiting on the directory result before accessing the cache (to reduce energy usage), and the overhead of traversing the L2 cache prior to accessing the L3 cache are negligible. They are more than counterbalanced by the beneficial latency of the 256-KB L2 cache. Likewise, the slightly higher cycle time and reduction in overall bandwidth per unit of capacity is counterbalanced by the traffic reduction afforded by the 256-KB L2 cache. The refresh overhead, typically associated with DRAM, is hidden by a parallel engine that refreshes unused subarrays within each macro whenever operations exercise the macro. The centralized L3 region controller provides a single core/L2 dispatch port, a single lateral L3 region dispatch port, dual snoop dispatch ports (for even and odd cache lines), and a single pool of operational resources. Storage accesses that miss the L2 cache access the 4-MB local L3 region via the core/L2 dispatch port. Those that hit in the 4-MB local L3 region are managed by a pool of eight read machines. Prefetches and some L2 store misses also access the L3 region via the core/L2 dispatch port and are managed by a pool of 24 lightweight RC machines. When prefetched data is staged to the L3 cache, it is managed by a pool of ten write machines. Each write machine has an associated cast-out machine to manage the eviction of lines displaced by writes. Note that these write machines are also utilized by other operations such as cast-outs and cache-injections. Storage accesses that miss both the L2 cache and the 4-MB local L3 region are broadcast to the coherence fabric and snooped by the memory controller, other L2 caches, possibly other L3 caches, and by the seven remote 4-MB L3 regions that comprise the remaining 28 MB of the on-chip L3 cache. Therefore, operations that miss the 4-MB local L3 region but hit in the remaining 28 MB of the L3 cache access the cache via the snoop dispatch ports. L2 cast-out operations access the 4-MB local L3 region via the core/L2 dispatch port, whereas lateral cast-outs access a given 4-MB L3 region via the lateral L3 region dispatch port or via the snoop dispatch ports.
## POWER7 SMP
In order to continue to provide the high-scalability low-latency characteristics of earlier POWER server processors, the POWER7 processor utilizes a similar nonblocking-broadcast-based coherence-transport mechanism, based upon the same distributed management relaxed-order-optimized multiscope enablement provided by the POWER6 platform. The on-chip coherence interconnect routes two sets (even and odd cache line) of coherence requests through the horizontal trunk, inward toward the even/odd arbitration logic located at the center of the chip. Up to one even request and one odd request may be granted each on-chip bus cycle. Once granted, the requests are broadcast within the chip on the even/odd snoop buses outward toward the left and right edges of the chip. Requests that will be routed to other chips are also sent to the multichip SMP interconnect (discussed in the multichip interconnect section) via a central vertical spine toward the top and bottom edges of the chip. Requests that have arrived from other chips are managed by the even/odd arbitration logic and broadcast to the snoop buses. Coherence responses from the snoopers are routed inward along the horizontal trunk toward the even/odd coherence decision logic located at the center of the chip. For requests that have been routed to other chips, additional responses from the off-chip snooper are fed into the coherence decision logic. Once a final coherence decision is made in response to a given request, a notification is broadcast within the chip on the even/odd notification buses outward from the center toward the left and right edges of the chip. Notifications that will be routed to other chips are also sent to the multichip SMP interconnect via the central vertical spine toward the top and bottom edges of the chip.
Because the coherence flow is nonblocking, the rate at which requests may be scheduled onto the snoop buses is restricted by the snooper with the lowest possible snoop processing rate. The central coherence arbitration logic must insure that requests (whether sourced from the chip containing the slowest snooper or from another chip in the system) do not overrun the slowest snooper. To accommodate this, system firmware negotiates a Bfloor frequency. As individual processor frequencies are adjusted upward and downward, none will ever fall beneath the floor frequency. The coherence arbitration logic throttles the rate at which requests are granted to insure that a snooper operating at the floor frequency or higher can process all the requests.
The on-chip data interconnect consists of eight 16-byte buses that span the horizontal trunk. Four flow from left to right, and the other four flow from right to left. These buses are bracketed by memory controllers found at the left and right edges of the chip. They are divided into multiple segments, such that multiple 16-byte data packets may be pipelined within the multiple segments of the same bus at any given time. The buses operate at the on-chip bus frequency. Each memory controller has two 16-byte on-ramps and two 16-byte off-ramps that provide access to the eight buses. Each core’s associated L2 cache and local L3 region share one 16-byte on-ramp/off-ramp pair, as does the pair of I/O controllers. The multichip data interconnect ports, found in the central vertical spines have a total of seven 16-byte on-ramp/off-ramp pairs. In total, there are twenty 16-byte on-ramps and twenty 16-byte off-ramps that provide access to and from the eight horizontal 16-byte trunk buses. Each ramp pair is associated with a bus segment. Note that a source-to-destination on-ramp/off-ramp route may consume only a subset of the segments in the horizontal trunk, depending upon the physical locations of the source and destination. Data transfers are managed by centralized arbitration logic that takes into account source and destination locations, allocates available bus segments to plot one of several possible routes, allocates the on- and off-ramp resources, and manages destination data buffering resources. Note that since transfers may use only a subset of the segments in a given trunk bus, multiple noninterfering source-to-destination transfers may utilize the same horizontal trunk bus simultaneously. The arbitration logic must also account for the differing operating frequencies of the processor cores. For example, a source core operating at a lower frequency will send data via its on-ramp to the trunk buses at a slower rate. Likewise, a destination core operating at a lower frequency will consume data via its off-ramp from the trunk buses at a slower rate. To manage this, the arbitration logic controls speed-matching buffers in all of the on-ramps/off-ramps.
Below depicts a first-level nodal structure, which combines up to four POWER7 chips. Each chip has four 10-B/s on-node SMP links associated with the vertical spine that emanates from the center of the chip toward the top edge. In this manner, the four chips are fully connected.
![Pasted image 20230918164544.png](/assets/images/power/Pasted image 20230918164544.png)
A second-level system structure combines up to eight of the nodes. Each chip has two 10B/s off-node SMP links associated with the vertical spine that emanates from the center of the chip toward the bottom edge. As shown in Figure, in a standard commercial system, up to seven of the eight off-node SMP links (coming from the four POWER7 chips comprising a node) are connected to each of the seven other nodes that comprise an eight-node system. Unlike POWER6 systems, which enforce a strict 50% coherence to 50% data ratio or 33% coherence to 67% data ratio, the POWER7 SMP links enable a dynamic free-form allocation of coherence and data traffic, enabling higher effective utilization of the links. Additionally, to maximize SMP link bandwidth, the on- and off-node SMP links do not operate at the on-chip frequency. They are independently tuned, typically operating at speeds ranging from 2.5 to 3.3 GHz, depending upon system packaging characteristics, and provide increased flexibility over the POWER6 SMP links. POWER7 systems heavily exploit the speculative localized scope coherence broadcast capability introduced in the POWER6 design. The localized regions make use of enhanced scope prediction heuristics to partition the coherence traffic, such that each region has full access to its SMP link and snooper coherence bandwidth. In cases where the speculation is successful, this has the effect of multiplying the link and snooper bandwidths by the number of regions. For example, dividing a large 256-way system into eight nodal regions has the effect (to the degree of successful speculation) of enabling each 32-way region to privately enjoy the SMP link and snooper bandwidth that would otherwise be shared across the whole system.
![Pasted image 20230725175956.png](/assets/images/power/Pasted image 20230725175956.png)

## POWER7 RAS
POWER7 reliability and availability features are highlighted in below Figure.
![Pasted image 20230918170442.png](/assets/images/power/Pasted image 20230918170442.png)
When an error is detected and reported by a core unit, the POWER7 core quickly blocks all instruction completion, along with blocking all instruction fetch and dispatch. If the error condition is not severe enough to cause a checkstop, the core initiates the recovery process. The recovery process flushes all the instructions in the pipeline for each thread to put the core in an architected state that existed sometime before the error condition occurred, fence the core from the rest of the system (L2 and nest), run an automatic built-in self-test to clear and test the core SRAM cells, reset each core unit to clean up any errors and reset the state machines, refresh the GPR and VSR files by initiating a state machine that does a read/correct/write operation to each entry in the register files to correct any single bit error through ECC correction mechanism, drop the fence to L2 and nest, and then restart instruction fetching and enable dispatch and completion.
To facilitate error detection and recovery in POWER7, the big register files (such as GPR and VSR) are ECC protected, whereas the smaller register files are protected through parity; all SRAM cells have error detection and recovery mechanisms. The I-cache is parity protected and recoverable. The floating-point pipelines implement residue checking mechanism, and numerous logic units implement additional control checking mechanism. In addition, POWER7 core uses RAS-hardened latches for various SPRs and core configuration latches.
The L1 D-cache is protected by byte parity. Hardware recovery is invoked on detection of a parity error while reading the L1 D-cache for a load instruction. The load instruction in error is not completed but rather flushed, the L1 D-cache contents are invalidated, and the instructions are refetched and reexecuted from the group of the load instruction in error. Additionally, when a persistent hard error is detected either in the L1 D-cache array or in its supporting directory or set predict array, a set delete mechanism is used to prohibit the offending set from being validated again. This allows the processor core to continue execution with slightly degraded performance until a maintenance action is performed.

# 10. Power 8
To instead exploit the parallelism across cloud instances, the POWER8 core can be put in a split-core mode, so that four partitions can run on one core at the same time, with up to two hardware threads per partition. Also supporting emerging workloads, the POWER8 processor includes an optimized implementation of hardware TM (Transactional Memory). This implementation has a low overhead to start a transaction and additional features that support the exploitation of transactions in Java and other programming languages, in many cases without any changes to the user code.
![Pasted image 20230718154255.png](/assets/images/power/Pasted image 20230718154255.png)
On soft-error detection, the core automatically uses its out-of-order execution features to flush the instructions in the pipeline and re-fetch and re-execute them, so that there is no loss of data integrity.
## Power 8 Core
Below Figure shows the POWER8 core floorplan. The core consists primarily of the following six units: instruction fetch unit (IFU), instruction sequencing unit (ISU), load-store unit (LSU), fixed-point unit (FXU), vector and scalar unit (VSU) and decimal floating point unit (DFU). The instruction fetch unit contains a 32 KB I-cache (instruction cache) and the load-store unit contains a 64 KB D-cache (data cache), which are both backed up by a tightly integrated 512 KB unified L2 cache. In a given cycle, the core can fetch up to eight instructions, decode and dispatch up to eight instructions, issue and execute up to ten instructions, and commit up to eight instructions.
There are sixteen execution pipelines within the core: two fixed-point pipelines, two load/store pipelines, two load pipelines, four double-precision floating-point pipelines (which can also act as eight single-precision floating-point pipelines), two fully symmetric vector pipelines that execute instructions from both the VMX (Vector eXtensions) and VSX (Vector-Scalar eXtensions) instruction categories in the Power ISA, one cryptographic pipeline, one branch execution pipeline, one condition register logical pipeline, and one decimal floating-point pipeline. The two load/store pipes and the two load pipes have the additional capability to execute simple fixed-point operations. The four floating-point pipelines are each capable of executing double precision multiply-add operations, accounting for eight double-precision, 16 single-precision, floating-point operations per cycle per core. In addition, these pipelines can also execute 64-bit integer SIMD operations.
![Pasted image 20230718155035.png](/assets/images/power/Pasted image 20230718155035.png)

## POWER8 Core Instruction Flow
Figure shows the instruction flow in POWER8 processor core. Instructions flow from the memory hierarchy through various issue queues and then are sent to the functional units for execution. Most instructions (except for branches and condition register logical instructions) are processed through the Unified Issue Queue (UniQueue), which consists of two symmetric halves (UQ0 and UQ1). There are also two copies (not shown) of the general-purpose (GPR0 and GPR1) and vector-scalar (VSR0 and VSR1) physical register files. One copy is used by instructions processed through UQ0 while the other copy is for instructions processed through UQ1. The fixed-point, floating-point, vector, load and load-store pipelines are similarly split into two sets (FX0, FP0, VSX0, VMX0, L0, LS0 in one set, and FX1, FP1, VSX1, VMX1, L1, LS1 in the other set) and each set is associated with one UniQueue half. Which issue queue, physical register file, and functional unit are used by a given instruction depends on the simultaneous multi-threading mode of the processor core at run time. In ST mode, the two physical copies of the GPR and VSR have identical contents. Instructions from the thread can be dispatched to either one of the UniQueue halves (UQ0 or UQ1). Load balance across the two UniQueue halves is maintained by dispatching alternate instructions of a given type to alternating UniQueue halves. In the SMT modes (SMT2, SMT4, SMT8), the two copies of the GPR and VSR have different contents. The threads are split into two thread sets and each thread set is restricted to using only one UniQueue half and associated registers and execution pipelines. Fixed-point, floating-point, vector and load/store instructions from even threads (T0, T2, T4, T6) can only be placed in UQ0, can only access GPR0 and VSR0, and can only be issued to FX0, LS0, L0, FP0, VSX0, and VMX0 pipelines. Fixed-point, floating-point, vector and load/store instructions from odd threads (T1, T3, T5, T7) can only be placed in UQ1, can only access GPR1 and VSR1, and can only be issued to FX1, LS1, L1, FP1, VSX1, and VMX1 pipelines. Cryptographic and decimal floating-point instructions from a thread can only be placed in the corresponding UniQueue half, but since there is only one instance of each of these units, all instructions are issued to the same unit. Branches and condition register logical instructions have their own dedicated issue queues and execution pipelines, which are shared by all threads.
![Pasted image 20230918172554.png](/assets/images/power/Pasted image 20230918172554.png)

![Pasted image 20230919095043.png](/assets/images/power/Pasted image 20230919095043.png)
## POWER8 IFU
The POWER8 IFU has several new features relative to the POWER7 processor IFU. Support for SMT8 and additional concurrent LPARs (logical partitions) required changes in sizes for many resources in the IFU. In addition, the following changes were made to improve the overall performance of the POWER8 core: First, instruction cache alignment improvements result in a higher average number of instructions fetched per fetch operation. Second, branch prediction mechanism improvements result in more accurate target and direction predictions. Third, group formation improvements allow more instructions per dispatch group, on average. Fourth, instruction address translation hit rates were improved. Fifth, instruction fusion is used to improve performance of certain common instruction sequences. Finally, better pipeline hazard avoidance mechanisms reduce pipeline flushes.
![Pasted image 20230918172656.png](/assets/images/power/Pasted image 20230918172656.png)
* Instruction fetching and predecoding. The POWER8 core has a dedicated 32 KB, 8-way set associative L1 I-cache. It is based on a 16-way banked design to avoid read and write collisions. A 32 x 8-entry Instruction Effective Address Directory (IEAD) provides fast prediction for way selection to choose one fetch line from the eight ways. A traditional full I-cache directory (I-dir) is accessed in parallel to confirm the way selection prediction in the next cycle. The I-cache can be addressed on any 16-byte boundary within the 128-byte cache line. Fast instruction address translation for instruction fetch is supported by a fully associative 64-entry Instruction Effective to Real Address translation Table (IERAT). The IERAT is shared among all threads. The IERAT directly supports 4 KB, 64 KB, and 16 MB page sizes. Other page sizes are supported by storing entries with the next smaller supported page size. The IFU reads instructions into the I-cache from the L2 unified cache. Each read request for instructions from the L2 returns four sectors of 32 bytes each. These reads are either demand loads that result from I-cache misses or instruction prefetches. For each demand load request, the prefetch engine initiates additional prefetches for sequential cache lines following the demand load. Demand and prefetch requests are made for all instruction threads independently, and instructions may return in any order, including interleaving of sectors for different cache lines. Up to eight instruction read requests can be outstanding from the core to the L2 cache. Instruction prefetching is supported in ST, SMT2, and SMT4 modes only. Up to three sequential lines are prefetched in ST mode and one sequential line per thread in SMT2 and SMT4 modes. There is no instruction prefetching in SMT8 mode to save on memory bandwidth. Prefetches are not guaranteed to be fetched and depending on the congestion in the POWER8 processor nest, some prefetches may be dropped. When instructions are read from the L2 cache, the IFU uses two cycles to create predecode and parity bits for each of the instructions, before they are written into the I-cache. The predecode bits are used to scan for taken branches, help group formation, and denote several exception cases. Branch instructions are modified in these stages to help generate target addresses during the branch scan process that happens during the instruction fetch stages of the pipeline. The modified branch instruction, with a partially computed target address, is stored in the I-cache. Three cycles after a 32-byte sector of instructions arrives on the I-cache/L2 interface, the sector is written into the I-cache. If the requesting thread is waiting for these instructions, they are bypassed around the I-cache to be delivered to the instruction buffers and the branch scan logic. Instruction Fetch Address Registers (IFARs) track program counter addresses for each thread. On each cycle, the IFAR register for one of the threads is selected to provide the fetch address to the I-cache complex and the branch prediction arrays. The I-cache fetch process reads quad-word aligned block of up to eight instructions per cycle from the I-cache and writes them into the instruction buffers where they are later formed into dispatch groups. Quadword-aligned fetch ensures that for a non-sequential fetch at least one instruction from the first quadword and four instructions from the second quadword are fetched as long as there is a cache hit and both quadwords are within the cache line. Thread priority, pending cache misses, instruction buffer fullness, and thread balancing metrics are used to determine which thread is selected for instruction fetching in a given cycle. The IFU allocates fetch cycles within threads of the same partition based on the priorities associated with each thread.
* Group formation. Fetched instructions are processed by the branch scan logic and are also stored in the instruction buffers (IBUF) for group formation. The IBUF can hold up to 32 entries, each four instructions wide. Each thread can have four entries in SMT8 mode, eight entries in SMT4 mode and 16 entries in SMT2 and ST modes. Instructions are retrieved from the IBUF and collected into groups. Thread priority logic selects one group of up to six non-branch and two branch instructions in ST mode or two groups (from two different threads) of up to three non-branch and one branch instructions in SMT modes per cycle for group formation.
* Instruction decode. After group formation, the instructions are either decoded or routed to microcode hardware that breaks complex instructions into a series of simple internal operations. Simple instructions are decoded and sent to dispatch. Complex instructions that can be handled by two or three simple internal operations are cracked into multiple dispatch slots. Complex instructions requiring more than three simple internal operations are handled in the microcode engine using a series of simple internal operations. Microcode handling continues until the architected instruction is fully emulated. The decode and dispatch section of the IFU also handles illegal special-purpose register (SPR) detection, creation of execution route bits and marking of instructions for debugging and performance monitoring purposes.
* Instruction fusion. For select combinations of instructions, the POWER8 core is capable of fusing two adjacent architected instructions into a single internal operation.
* Branch prediction. The POWER8 IFU supports a three-cycle branch scan mechanism that fetches 32 bytes (corresponding to eight instructions) from the I-cache, scans the fetched instructions for branches that are predicted taken, computes their target addresses (or predicts the target address for a branch-to-link or branch-to-count instruction), determines if any of these branches (in the path of execution) is unconditional or predicted taken and if so, makes the target address of the first such branch available for next fetch for the thread. It takes three cycles to obtain the next fetch address when there is a taken branch, and for two of these cycles there is no fetch for the thread. However, in SMT mode, those two cycles will normally be allocated to other active threads, and thus not lost. If the fetched instructions do not contain any branch that is unconditional or predicted taken, the next sequential address is used for the next fetch for that thread and no fetch cycles are lost. The direction of a conditional branch is predicted using a complex of Branch History Tables (BHT), consisting of a 16K-entry local BHT array (LBHT), a 16K-entry global BHT array (GBHT) and a 16K-entry global selection array (GSEL). These arrays are shared by all active threads and provide branch direction predictions for all the instructions in a fetch sector in each cycle. A fetch sector can have up to eight instructions, all of which can be branches. The LBHT is directly indexed by 14 bits from the instruction fetch address. The GBHT and GSEL arrays are indexed by the instruction fetch address hashed with a 21-bit Global History Vector (GHV) folded down to 11 bits. The value in the GSEL entry is used to choose between the LBHT and GBHT, for the direction prediction of each individual branch. All BHT entries consist of two bits with the higher order bit determining direction (taken or not taken), and the lower order bit providing hysteresis. There is one GHV for every thread in the POWER8 core to track the past branch history for that particular thread.
* Pipeline hazards. The POWER8 IFU also implements mechanisms to mitigate performance degradation associated with pipeline hazards. A Store-Hit-Load (SHL) is an out-of-order pipeline hazard condition, where an older store executes after a younger overlapping load, thus signaling that the load received stale data. The POWER8 IFU has logic to detect when this condition exists and provide control to avoid the hazard by flushing the load instruction which received stale data (and any following instructions). When a load is flushed due to detection of a SHL, the fetch address of the load is saved and the load is marked on subsequent fetches allowing the downstream logic to prevent the hazard. When a marked load instruction is observed, the downstream logic introduces an explicit register dependency for the load to ensure that it is issued after the store operation.
## POWER8 ISU
The Instruction Sequencing Unit (ISU) dispatches instructions to the various issue queues, renames registers in support of out-of-order execution, issues instructions from the various issues queues to the execution pipelines, completes executing instructions, and handles exception conditions. Below Figure illustrates the logical flow of instructions in the ISU.
![Pasted image 20230918172731.png](/assets/images/power/Pasted image 20230918172731.png)
The POWER8 processor dispatches instructions on a group basis. In ST mode, it can dispatch a group of up to eight instructions per cycle. In SMT mode, it can dispatch two groups per cycle from two different threads and each group can have up to four instructions. All resouces such as the renaming registers and various queue entries must be available for the instructions in a group before the group can be dispatched. Otherwise, the group will be held at the dispatch stage. An instruction group to be dispatched can have at most two branch and six non-branch instructions from the same thread in ST mode. If there is a second branch, it will be the last instruction in the group. In SMT mode, each dispatch group can have at most one branch and three non-branch instructions.
The ISU employs a Global Completion Table (GCT) to track all in-flight instructions after dispatch. The GCT has 28 entries that are dynamically shared by all active threads. In ST mode, each GCT entry corresponds to one group of instructions. In SMT modes, each GCT entry can contain up to two dispatch groups, both from the same thread. This allows the GCT to track a maximum of 224 in-flight instructions after dispatch. Each GCT entry contains finish bits for each instruction in the group. At dispatch, the finish bits are set to reflect the valid instructions. Instructions are issued out of order and executed speculatively. When an instruction has executed successfully (without a reject), it is marked as finished. When all the instructions in a group are marked finished, and the group is the oldest for a given thread, the group can complete. When a group completes, the results of all its instructions are made architecturally visible and the resources held by its instructions are released. In SMT modes, the POWER8 core can complete one group per thread set per cycle, for a maximum total of two group completions per cycle. In ST mode, only one group, consisting of up to eight instructions, can complete per cycle. When a group is completed, a completion group tag (GTAG) is broadcast so that resources associated with the completing group can be released and reused by new instructions.
## POWER8 LSU
The Load/Store Unit (LSU) is responsible for executing all the load and store instructions, managing the interface of the core with the rest of the systems through the unified L2 cache and the Non-Cacheable Unit (NCU), and implementing address translation as specified in the Power ISA. The POWER8 LSU contains two symmetric load pipelines (L0 and L1) and two symmetric load/store pipelines (LS0 and LS1). Below figure illustrates the microarchitecture of the POWER8 LS0 pipeline.
![Pasted image 20230918172837.png](/assets/images/power/Pasted image 20230918172837.png)
Each of the LS0 and LS1 pipelines are capable of executing a load or a store operation in a cycle. Furthermore, each of L0 and L1 pipelines are capable of executing a load operation in a cycle. In addition, simple fixed-point operations can also be executed in each of the four pipelines in the LSU, with a latency of three cycles. The LSU contains several subunits, including the load/store address generation (AGEN) and execution subunits, the store reorder queue (SRQ), the store data queue (SDQ), the load reorder queue (LRQ), the load miss queue (LMQ), and the L1 data cache array (D-cache) with its supporting set predict and directory arrays (DDIR), and the data prefetch engine (PRQ). The address translation mechanism in the LSU includes the Effective-to-Real Address Translation for data (DERAT), the Effective-to-Real Address Translation (ERAT) Miss Queue (EMQ), the Segment Lookaside Buffer (SLB), and TLB.
* Load/store execution. In ST mode, a given load/store instruction can execute in any appropriate pipeline: LS0, LS1, L0 and L1 for loads, LS0 and LS1 for stores. In SMT2, SMT4, and SMT8 mode, instructions from half of the threads execute in pipelines LS0 and L0, while instructions from the other half of the threads execute in pipelines LS1 and L1. Instructions are issued to the load/store unit out-of-order, with a bias towards the oldest instructions first. Stores are issued twice; an address generation operation is issued to the LS0 or LS1 pipeline, while a data operation to retrieve the contents of the register being stored is issued to the L0 or L1 pipeline. Main dataflow buses into and out of the LSU include a 64-byte reload data bus from and a 16-byte store data bus to the L2 cache, two 16-byte load data buses (one per execution pipeline) to and two 16-byte store data buses from the VSU, and two 8-byte store data buses (one per execution pipeline) from the FXU. The load data buses to the VSU have each a tap off of 8-byte load data to a corresponding FXU execution pipeline. Fixed-point loads have a three-cycle load-to-use latency on a L1 D-cache hit. That is, two cycles of bubbles are introduced between a load and a dependent FXU operation. VSU loads have a five-cycle load-to-use latency on a L1 D-cache hit. That is, four cycles of bubbles are introduced between a load and a dependent VSU operation. Each of the four LSU pipelines can also execute fixed-point add and logical instructions (simple fixed-point), allowing more fixed-point execution capability for the POWER8 core and greater flexibility to the ISU in the issuing of instructions.
* Load/store ordering. The LSU must ensure the effect of architectural program order of execution of the load and store instructions, even though the instructions can be issued and executed out-of-order. To achieve that, the LSU employs two main queues: the store reorder queue (SRQ) and the load reorder queue (LRQ). The SRQ is a 40-entry, real address based CAM structure. Whereas 128 virtual entries per thread are available to allow a total of 128 outstanding stores to be dispatched per thread, only a total of 40 outstanding stores may be issued, since a real, physical SRQ entry is required for the store to be issued. The SRQ is dynamically shared among the active threads. An SRQ entry is allocated at issue time and de-allocated after the completion point when the store is written to the L1 D-cache and/or sent to the L2 Cache. For each SRQ entry, there is a corresponding store data queue (SDQ) entry of 16 bytes. Up to 16 bytes of data for a store instruction can be sent to the L2 Cache (and also written to the L1 D-Cache on a hit) in every processor cycle. Store forwarding is supported, where data from an SRQ entry is forwarded to an inclusive, subsequent load, even if the store and load instructions are speculative. Similar to the SRQ, the LRQ is a 44-entry, real address based, CAM structure. Again, 128 virtual entries per thread are available to allow a total of 128 outstanding loads to be dispatched per thread, but only a total of 44 outstanding loads may be issued, since a real, physical LRQ entry is required for the load to be issued. The LRQ is dynamically shared among the threads. The LRQ keeps track of out-of-order loads, watching for hazards. Hazards generally exist when a younger load instruction executes out-of-order before an older load or store instruction to the same address (in part or in whole). When such a hazard is detected, the LRQ initiates a flush of the younger load instruction and all its subsequent instructions from the thread, without impacting the instructions from other threads. The load is then re-fetched from the I-cache and re-executed, ensuring proper load/store ordering.
* Address translation. During program execution, 64-bit effective addresses are translated by the first level translation into 50-bit real addresses that are used for all addressing in the cache and memory subsystem. The first level translation consists of a primary Data Effective-to-Real Address Translation (DERAT), a secondary DERAT, and an Instruction Effective-to-Real Address Translation (IERAT). When a data reference misses the primary DERAT, it looks up the address translation in the secondary DERAT. If the translation is found in the secondary DERAT, it is then loaded into the primary DERAT. If the translation is not found in either the primary or the secondary DERAT, the second-level translation process is invoked to generate the translation. When an instruction reference misses the IERAT, the second-level translation is also invoked to generate the translation. The second-level translation consists of a per-thread Segment Lookaside Buffer (SLB) and a TLB that is shared by all active threads. Effective addresses are first translated into 78-bit virtual addresses using the segment table and the 78-bit virtual addresses are then translated into 50-bit real addresses using the page frame table. While the architected segment and page frame tables are large and reside in main memory, the SLB and TLB serve as caches of the recently used entries from the segment table and page frame table, respectively. The POWER8 processor supports two segment sizes, 256 MB and 1 TB, and four page sizes: 4 KB, 64 KB, 16 MB, and 16 GB. The primary DERAT is a 48-entry, fully-associative, Content Addressed Memory (CAM) based cache. Physically, there are four identical copies of the primary DERAT, associated with the two load/store pipelines and two load pipelines. In ST mode, the four copies of the primary DERAT are kept synchronized with identical contents. So, in ST mode, logically there are a total of 48 entries available. In the SMT modes, two synchronized primary DERATs (in LS0 and L0 pipes) contain translation entries for half of the active threads while the two other synchronized primary DERATs (in LS1 and L1 pipes) contain translation entries for the other half of the active threads. In the SMT modes, the first two paired primary DERATs contain addresses that can be different from the other two paired primary DERATs, for a total of 96 logical entries. Each Primary DERAT entry translates either 4 KB, 64 KB, or 16 MB pages. The 16 GB pages are broken into 16 MB pages in the primary DERAT. The primary DERAT employs a binary tree Least Recently Used (LRU) replacement policy. The secondary DERAT is a 256-entry, fully associative, CAM-based cache. In single thread mode, all 256 entries are available for that thread. In SMT mode, the secondary DERAT is treated as two 128-entry arrays, one for each thread set. The secondary DERAT replacement policy is a simple First-In First-Out (FIFO) scheme. The SLB is a 32-entry-per-thread, fully associative, CAM-based buffer. Each SLB entry can support 256 MB or 1 TB segment sizes. The Multiple Pages Per Segment (MPSS) extension of Power ISA is supported in the POWER8 processor. With MPSS, a segment with a base page size of 4 KB can have 4 KB, 64 KB, and 16 MB pages concurrently present in the segment. For a segment with a base page size of 64 KB, pages of size 64 KB and 16 MB are allowed concurrently. The SLB is managed by supervisor code, with the processor generating a data or instruction segment interrupt when an SLB entry needed for translation is not found. The TLB is a 2,048-entry, 4-way set associative buffer. The TLB is managed by hardware, and employs a true LRU replacement policy. A miss in the TLB causes a table-walk operation, by which the TLB is reloaded from the page frame table in memory. There can be up to four concurrent outstanding table-walks for TLB misses. The TLB also provides a hit-under-miss function, where the TLB can be accessed and return translation information to the DERAT while a table-walk is in progress. In the POWER8 LSU, each TLB entry is tagged with the LPAR (logical partition) identity. For a TLB hit, the LPAR identity of the TLB entry must match the LPAR identity of the active partition running on the core. When a partition is swapped in, there is no need to explicitly invalidate the TLB entries. If a swapped-in partition has run previously on the same core, there is a chance that some of its TLB entries are still available which reduces TLB misses and improves performance.
## POWER8 FXU
The Fixed-Point Unit (FXU) is composed of two identical pipelines (FX0 and FX1). As shown in Figure, each FXU pipeline consists of a multiport General Purpose Register (GPR) file, an arithmetic and logic unit (ALU) to execute add, subtract, compares and trap instructions, a rotator (ROT) to execute rotate, shift and select instructions, a count unit (CNT) to execute count leading zeros instruction, a bit select unit (BSU) to execute bit permute instruction, a miscellaneous execution unit (MXU) to execute population count, parity and binary-coded decimal assist instructions, a multiplier (MUL), and a divider (DIV).
![Pasted image 20230918172918.png](/assets/images/power/Pasted image 20230918172918.png)
At the heart of each FXU pipeline is a GPR file with 124 entries which holds all the rename and a subset of the architected registers for up to four threads. Additional architected registers are kept in the SAR register files. The GPR has eight read ports, two supplying operands for the fixed-point pipeline, two supplying operands to the load/store pipeline, two supplying operands to the load pipeline, and two supplying register data to the SAR. The GPR has six write ports: two for the fixed-point pipelines, two for the load/store pipelines, and two for the load pipelines. (Updates to a particular GPR can come from either set of fixed-point, load/store and load pipelines when the core is in ST mode.) The write ports from the remote fixed-point and load/store pipelines are shared with write ports from the SAR. In SMT modes, writes from remote pipelines are disabled and the ports can be used exclusively to load data from the SAR. The POWER8 core implements a VSU extract bus which is routed to the result multiplexer of each FXU pipe. The extract bus significantly reduces latency for VSR to GPR transfers. The contents of the two GPR register files in each pipeline are managed by the ISU to be identical in ST mode, but distinct in SMT2, SMT4, and SMT8 modes. That is, in SMT2, SMT4, or SMT8 mode the GPR in one pipeline contains the registers for one set of threads, while the GPR in the other pipeline contains the registers for the other set of threads. The POWER8 FXU supports Transactional Memory (TM) by doubling the register space to hold a backup copy of all the architected registers. Rather than doubling the size of the GPR, the SAR was added to expand the state space of the architected GPR registers. The XER, which is the other architected register in the FXU, had to grow for TM support. The XER is implemented as a Reorder Buffer (ROB) and Architected Register File (ARF) structure to accommodate the increase in state space.
## POWER8 VSU
The POWER8 processor Vector-and-Scalar Unit (VSU), shown in below Figure has been completely redesigned from its initial implementation in the POWER7 processor to support the growing computation and memory bandwidth requirements of business analytics and big data applications.
![Pasted image 20230918172953.png](/assets/images/power/Pasted image 20230918172953.png)
The POWER8 VSU now supports dual issue of all scalar and vector instructions. Further improvements include a two-cycle VMX/VSX Permute (PM) pipeline latency, doubling of the store bandwidth to two 16-byte vectors/cycle to match the 32-byte/cycle load bandwidth, and execution of all floating-point compare instructions using the two-cycle Simple Unit (XS) pipeline to speedup branch execution. Other latencies remain unchanged from the POWER7 processor design point, supporting fast six-cycle bypass within the floating-point unit.
## Power8 On Chip Cache


![Pasted image 20230718155501.png](/assets/images/power/Pasted image 20230718155501.png)
## Power8 Memory Organization
 Up to 8 high speed channels, each running up to 9.6 Gb/s for up to 230 GB/s sustained
 Up to 32 total DDR ports yielding 410 GB/s peak at the DRAM
 Up to 1 TB memory capacity per fully configured processor socket
![Pasted image 20230718160154.png](/assets/images/power/Pasted image 20230718160154.png)

## Power8 CAPI - Coherence Attach Processor Interface
Virtual Addressing
• Accelerator can work with same memory addresses that the processors use
• Pointers de-referenced same as the host application
• Removes OS & device driver overhead
Hardware Managed Cache Coherence
• Enables the accelerator to participate in “Locks” as a normal thread Lowers Latency over IO communication model

# 11. Power 9
Built on a 14nm process, each CPU package can contain up to 24 SMT4 cores or 12 SMT8 cores. Each pair of SMT4 cores, or singleton SMT8 core, comprises a slice; each slice in turn contains 512kB L2 cache and 10MB L3 cache. Chips can be fused as SMT4 or SMT8 during manufacturing.
* SMT8 is optimized for IBM’s PowerVM (server virtualization) ecosystem
* SMT4 is optimized for the Linux Ecosystem
![Pasted image 20230907145848.png](/assets/images/power/Pasted image 20230907145848.png)

## POWER9 Core Execution Slice Microarchitecture
A **Slice** is the basic 64-bit computing block incorporating a single Vector and Scalar Unit(**VSU**) coupled with **Load/Store Unit** (**LSU**). VSU has a heterogeneous mix of computing capabilities including integer and floating point supporting scalar and vector operations. IBM claims this setup allows for higher utilization of resources while providing efficient exchanges of data between the individual slices. Two slices coupled together make up the **Super-Slice**, a 128-bit POWER9 physical design building block. Two super-slices together along with an **Instruction Fetch Unit** (**IFU**) and an **Instruction Sequencing Unit** (**ISU**) form a single POWER9 SMT4 core. The SMT8 variant is effectively two SMT4 units.
![Pasted image 20230907160728.png](/assets/images/power/Pasted image 20230907160728.png)
## POWER9 Core Pipeline
POWER9 modular design allowed IBM to reduce fetch-to-compute latency by 5 cycles. Additional 3 cycles were shorten from map-to-retire for floating point instructions. POWER9 furthered increased fusion and reduced the number of instructions cracked (POWER handles complex instructions by 'cracking' them into two or three simple µOPs). Instruction grouping at dispatch that was done in POWER8 has also been entirely removed from POWER9.
![Pasted image 20230907162021.png](/assets/images/power/Pasted image 20230907162021.png)
Below is a more detailed pipeline diagram
![Pasted image 20230907180319.png](/assets/images/power/Pasted image 20230907180319.png)
## POWER9 – Core Compute
* Fetch / Branch
	* 32kB, 8-way Instruction Cache
	* 8 fetch, 6 decode
	* 1x branch execution
* Slices issue VSU and AGEN
	* 4x scalar-64b / 2x vector-128b
	* 4x load/store AGEN
* Vector Scalar Unit (VSU) Pipes
	* 4x ALU + Simple (64b)
	* 4x FP + FX-MUL + Complex (64b)
	* 2x Permute (128b)
	* 2x Quad Fixed (128b)
	* 2x Fixed Divide (64b)
	* 1x Quad FP & Decimal FP
	* 1x Cryptography
* Load Store Unit (LSU) Slices
	* 32kB, 8-way Data Cache
	* Up to 4 DW load or store
![Pasted image 20230907161654.png](/assets/images/power/Pasted image 20230907161654.png)
the core microarchitecture diagram is shown in below figure
![Pasted image 20230907180048.png](/assets/images/power/Pasted image 20230907180048.png)
## POWER9 – Premier Acceleration Platform
**POWERAccel** is the collective name for all the interfaces and acceleration protocols provided by the POWER microarchitecture. POWER9 offers two sets of acceleration attachments: 
* PCIe Gen4 which offers 48 lanes at 192 GB/s duplex bandwidth
* 25G link which offers 96 lanes delivering up to 600 GB/s of duplex bandwidth.
On top of the two physical interfaces are a set of open standard protocols that integrated onto those signaling interfaces. The four prominent standards are:
- CAPI 2.0 - POWER9 introduces CAPI 2.0 over PCIe which quadruples the bandwidth offered by the original CAPI protocol offered in POWER8
- OpenCAPI 4.0- A new interface that runs on top of the POWER9 25G link (300 GiB/s) interface, designed for CPU-Accelerators applications
- NVLink 2.0 - High bandwidth and integration between the GPU and CPU.
- OMI - serial high bandwidth memory interface
- On-Chip Acceleration
    - 1x GZip
    - 2x 842 Compression
    - 2x AES/SHA
![Pasted image 20230907163735.png](/assets/images/power/Pasted image 20230907163735.png)

## POWER9 Variations
* POWER9 Scale out - use direct attach memory and support 2 socket SMP
* POWER9 Scale up  - use DMI Buffered Memory and support 16 socket SMP
* POWER9 Advanced IO - use OMI buffered memory and support 16 socket SMP
![Pasted image 20230907165748.png](/assets/images/power/Pasted image 20230907165748.png)

## POWER9 SMP
* 16 Gbps X-Bus Fully connected fabric within a central electronics complex drawer
* 25Gbps O-Bus fabric for Drawer to Drawer interconnect
A Power E980 logical system architecture is shown as below
![Pasted image 20230907174711.png](/assets/images/power/Pasted image 20230907174711.png)
A drawer is consisted of 4 POWER9. Below shows the symmetric multiprocessing (SMP) connections between nodes for 2-, 3-, and 4-drawer configurations
![Pasted image 20230907175136.png](/assets/images/power/Pasted image 20230907175136.png)
![Pasted image 20230907175155.png](/assets/images/power/Pasted image 20230907175155.png)

# 12. Power 10
The Power10 processor is a superscalar symmetric multiprocessor that is manufactured in samsung 7-nm with 18 layers of metal. The processor contains up to 15 cores that support eight simultaneous multithreading (SMT8) independent execution contexts. Each core has private access to 2 MB L2 cache and local access to 8 MB of L3 cache capacity. The local L3 cache region of a specific core also is accessible from all other cores on the processor chip. The cores of one Power10 processor share up to 120 MB of latency optimized non-uniform cache access (NUCA) L3 cache.
The modular design of the Power10 core provides for two core variants. An “SMT8 core” supports up to eight simultaneously active threads and has double the resources of an “SMT4 core” which supports up to four simultaneously active threads. Each Power10 chip is fabricated to support either SMT8 or SMT4 cores. The SMT8 cores are designed to operate with the PowerVM firmware and hypervisor. The SMT8 cores can operate with either one partition per thread or one partition per core. The SMT4 cores can operate with one partition per thread.
![Pasted image 20230906201517.png](/assets/images/power/Pasted image 20230906201517.png)

## Power10 Core Microarchitecture (per SMT4-Core-Resource)
Below shows a block diagram of major Power10 core organizational features. Relative sizes and capacities compared with the POWER9 core are highlighted for reference.
* Instructions are fetched from the L2 cache, 64 bytes to 32 bytes per cycle, then pre-decoded and stored in a 48 KB, 6-way, L1 instruction cache at a rate of up to 32 bytes per cycle. In each cycle, up to eight instructions are read from the L1 instruction cache or bypassed on write. Fetched instructions are scanned for branches and access a set of advanced predictors for both direction and target address prediction. Predicted taken branches redirect subsequent fetches.
	* When a fetch misses the L1 cache, the request is serviced by the L2 cache after under-going address translation. Cache misses conditionally generate up to seven cache-line prefetches based on a prefetch predictor.
	* Fetched instructions are bypassed into the decode pipeline when available. When decode is stalled or another thread is being selected for decode, the fetched instructions are buffered in a 128-entry instruction buffer.
* Instruction decode processes up to eight instructions per cycle. The 8-byte prefixed instructions each use two of the eight decode lanes. A subset of instructions are cracked into two, three, or four internal operations. A limited set of instructions are expanded with a micro-code engine that generates internal operations across multiple cycles of the decode pipeline. Instructions flagged for fusion by pre-decode are processed at instruction decode. Fused instructions can be in one of several categories.
* Up to eight internal operations or instructions are processed per cycle. Dispatch assigns execution resources as needed for each internal operation. A NOP is finished directly at dispatch and does not execute in a computational pipeline. The following resources are allocated at dispatch and when not available can cause dispatch holds.
	* Issue queue entry and slice assignment
	* Register renaming and dependency ordering
	* Load/store queue virtual entry
* Dispatched instructions are tracked in the Instruction Completion Table (ICT) until each operation has finished execution. The ICT holds up to 512 operations per SMT4-core-resource. Operations can be marked as finished either at dispatch or as they are executed in the pipelines. Operations are completed up to 64 instructions per cycle and on the granularity of two entries, called an “instruction-pair”.
![Pasted image 20230908111004.png](/assets/images/power/Pasted image 20230908111004.png)

### Issue Queue
Each slice has an issue queue with 20 entries. Per SMT4-core-resource, there are four issue queues supporting a total of up to 80 operations awaiting dependency resolution.
Each issue queue is associated with a native computation slice and paired with a second slice to form a super-slice. The super-slice contains an additional set of computation pipelines as well as a load port and a store/branch port. Each cycle, the issue queue in each slice can issue three instructions
* a computational operation
* a load
* a store/branch/simple operation.
Each super-slice selects a single load and a single store/branch/simple operation for execution.
![Pasted image 20230908113518.png](/assets/images/power/Pasted image 20230908113518.png)
## POWER10 Core Pipeline
The Power10 core pipeline stages show the nominal path through the load and store unit (LSU), arithmetic and logical unit (ALU), and floating-point unit (FPU) pipelines including the shortest latency data forwarding paths.
* Branch Pipeline
	* Branches are issued from an issue port shared with both store address generation and simple addition operations. Each SMT4-core-resource can resolve one branch per cycle. Branches are issued after the branch target-address source register (__ LNK__ , __ CNT__ , or __ TAR__ ), if any, is ready; even when a __ condition register (CR)__  source is still awaiting resolution. These partially executed branches awaiting __ CR__  results are held in the branch condition queues (BCQ). This enables target register dependent branches to resolve target register dependencies and extends the effective capacity of the main issue queues. Move-to and move-from operations between the target registers and GPRs are optimized for latency. The nominal latency of these operations has been reduced by sharing the physical register file between the LNK, CNT, TAR, and the GPRs. Further optimizations include dependency bypass at dispatch to completely eliminate the dependent latency between a target producing instruction and the consuming branch in some scenarios.
* Simple Pipeline
	* Add immediate instructions, such as those used for address manipulation, are supported on either the main ALU pipelines or share the simple pipeline used for some branch instructions by issuing to the store/branch/simple issue port. These operations can use either of the two simple ports per SMT4-core resource to produce a result with a nominal 2-cycle latency. A dynamic policy steers the add immediate instructions to the simple pipelines or the main ALU pipelines.
* Local/Store Pipeline
	* Load and store instructions are issued and released from the slice issue queues once operand dependencies are met and once the assigned entry is available in the load or store queue respectively. Store instructions issue with the address generation first and subsequently issue store data. Load and store pipeline hazards that require re-entry into the pipeline are handled locally by the load and store pipelines and queues. A load hazard such as a read-write cache bank conflict can be accommodated by a single cycle pipeline delay. Other hazards that require pipeline re-entry are managed by the load and store queues and are fully pipelined with operations from the main issue queues.
![Pasted image 20230908111733.png](/assets/images/power/Pasted image 20230908111733.png)
### Instruction flow
The high-level pipeline segments for instruction flow are depicted. The front end of the Power10 core operates with aggressive speculation and an in-order pipeline; decoding and dispatching up to eight instructions per cycle. After instruction execution resources are assigned at dispatch, internal operations (iops) are executed out-of-order in multi-slice, super-scalar compute and load/store pipelines.
![Pasted image 20230908111421.png](/assets/images/power/Pasted image 20230908111421.png)

## Thread and LPAR Management
With SMT8 cores, SMT4-core-resource_0 supports the even logical threads (0, 2, 4, 6) and SMT4-core resource_1 supports the odd logical threads (1, 3, 5, 7). All external interrupt lines and internal interrupts, such as decrementer, hypervisor, and door-bell interrupts, are steered to the correct resources to wake up the correct logical threads.
Within an SMT4-core-resource, the following SMT modes are supported:
* Single-thread (ST) mode: one thread active.
* SMT2 mode: two threads active.
* SMT4 mode: three or four threads active.
When the number of active threads per SMT4-core-resource moves between SMT2 and SMT4 modes, an SMT mode change procedure is executed by the hardware to re-balance resources. Threads become active via enabled interrupts and are de-activated by the stop instruction.
![Pasted image 20230908140921.png](/assets/images/power/Pasted image 20230908140921.png)
a subset of the core resources are dynamically partitioned between threads based on the SMT mode. A number of resources are shared between pairs of active threads (thread-pair). A summary of thread partitioning follows:
• Fetch is toggled between active threads while honoring dynamic thread priority.
• In SMT4 mode, decode and dispatch are split to support up to four instructions per thread-pair per cycle. Each four-instruction pipe flows independently from the others except in the case of the microcode engine, which is shared within an SMT4-core-resource.
• In SMT4 mode, issue queues, execution units, load and store execution units are divided by thread-pair, with each thread-pair using a single super-slice.
• The load miss queue and L1 caches are shared dynamically by threads within an SMT4-core-resource.
• Prefetch queues are shared dynamically for hardware detected streams and are statically partitioned based on the active thread count for software-initiated streams.

## L2 Cache
The SMT4 L2 cache features are summarized as follows:
* 1 MB private cache per SMT4-core-resource:
	* 128-byte line, 8-way set associative.
	* Both instruction side (I-side) and data side (D-side) inclusive for a Power10 core.
	* Quad-banked cache design interleaved across a four consecutive cache-line boundary.
	* L2 cache can perform a read from one cache bank while writing to one of the other cache banks.
* 8-way directory, quad-banked multi-ported:
	* One processor read port, two snoop read ports, and one write port per physical bank.
	* The processor port operates at ½ the processor clock rate into a given bank (initiated on a 2:1 clock boundary).
	* The snoop port into a given bank operates at ½ the processor clock rate (initiated on a 2:1 clock boundary) allowing for up to four snoops per 2:1 clock across the four banks.
	* The quad-banked directory can initiate:
		* Up to five directory reads in a given 2:1 cycle (four snoop ports and one on processor port).
		* One write in a given processor clock cycle (where directory writes are scheduled on the second half of a 2:1 cycle, such that they never conflict with directory reads).
* 1024 × 13-bit LRU arrays (logical configuration).
	* 2 × 4 LRU vector tracking tree with cache invalidate state biasing.
	* Supports LRU, direct map, single-member, and pseudo-random modes.
* Point of global coherency.
* Reservation stations: one per processor thread.
* Support for Power10 Synthetic_TM core mode.
* Four snoop-bus ports selected by the cache-line “real-address” bits [55:56].
* Hardware directory line delete capabilities to support faulty L2 cache elements.

## L3 Cache
SMT4 L3 cache features:
* Private 4 MB L3 cache; shared L3.1
* 16-way set associativity.
* 128-byte cache lines.
* Data cache consists of 4 banks of high-efficiency SRAMs with interleaving for read/write overlapping.
* 64-byte wide data bus to the L2 cache for reads.
* 64-byte wide data bus from the L2 cache for L2 castouts.
* Sixty-six, 600 kb SRAM macros; two of the SRAM macros are for redundancy.
* All cache accesses have the same latency.
* 16-way directory organized as four banks, with up to four reads or two reads and two writes every 2 pclks to differing banks. Physically implemented as eight 512 × 8 way × 50-bit SRAMs.
* LRU algorithm for victim selection using 4-bit per member utilization tracking using data type and re-use awareness.
The L3 cache has four dispatch pipes that handle incoming reflected-command (snoop) requests from the fabric. The snoop dispatch pipes perform an L3 directory read to determine how to handle the request. If the request requires the sending of intervention data, the executing of a snoop push, or the invalidating of the associated cache line, then the snoop dispatch pipe forwards the request to one of 16 snoop (SN) state machines. If the request is an incoming lateral cast-out (LCO) or a cache-injection that is accepted, the request is forwarded to both a snoop state machine (SN) and a write inject (WI) state machine.

## SMP Interconnect
* General Features
	* Master command/data request arbitration.
	* Command requests are tagged and broadcast using a snooping protocol that enables high-speed cache-to-cache transfers.
	* Multiple command scopes are used to reduce the bus-utilizations system wide. The SMP interconnect architecture uses cache states indicating the last known location of a line (sent off chip), information maintained in the system memory (memory domain indicator [MDI] bits), a coarse grained directory that indicates when a line has gone off the chip, and combined response equations that indicate if the scope of the command is sufficient to complete the command or if a larger scope is necessary.
	* The command snoop responses specified by the SMP interconnect implementation are used to create a combined response that is broadcast to maintain system cache state coherency. Combined responses are not tagged. Instead, the order of commands from a chip source, using a specific command-broadcast scope, is the same order that combined responses are issued from that source. The order is also affected by the snoop bus usage as well.
	* Data is tagged and routed along a dynamically selected path using staging/buffering along the way to overcome data routing collisions.
	* Command throttling and retry command back-off mechanisms for livelock prevention.
	* Multiple data links between chips are supported (link aggregation).

### Power10 Fabric SMP Topology
The Power10 off-chip SMP interconnect is a highly scalable, multi-tiered, fully-connected topology. The off-chip links use 18-bit high-speed differential links running up to 32 Gbps and can be configured in either a 1-hop or 2-hop configuration.
* 1-Hop SMP Topology
In the 1-hop configuration, the Power10 processor chip can fully connect up to seven other processor chips to create an eight-chip SMP system. Each chip is a group using up to seven inter-group A-links for a maximum system of eight processor chips.
![Pasted image 20230909093241.png](/assets/images/power/Pasted image 20230909093241.png)
* 2-Hop SMP Topology
In the 2-hop configuration, the Power10 processor chip can fully connect up to three other processor chips to create a four-chip group. The intra-group links are designated as __ X-links__ . Each Power10 processor in a group connects to its corresponding processor chip in each other group. Three of the inter-group __ A-links__  are provided per chip supporting a total of four groups, each containing four processor chips. A full four-group system of four chips per group comprises a maximum system of 16 processor chips.
![Pasted image 20230909093315.png](/assets/images/power/Pasted image 20230909093315.png)
#### Protocol and Data Routing in Multi-Chip Configurations
The SMP ports configured for coherency are used for both data and control information transport. The buses are used as follows:
1. The chip containing the master that is the source of the command issues the reflected command and the combined response to all other chips in the SMP system. Partial responses are collected and returned to the chip containing the master.
2. Data is moved point-to-point. For read operations, the chip containing the source of the data directs the data to the chip containing the master. For write operations, the chip containing the master directs the data to the LPC that performs the write operation. The routing tag contains the chip and unit identifier information for this purpose.

### Power10 Coherency Flow
1-Hop Broadcast Scope Definition
![Pasted image 20230909193458.png](/assets/images/power/Pasted image 20230909193458.png)
2-Hop Broadcast Scope Definition
![Pasted image 20230909193512.png](/assets/images/power/Pasted image 20230909193512.png)
Power10 System Real-Address Map
![Pasted image 20230909193549.png](/assets/images/power/Pasted image 20230909193549.png)

## NCU
The Power10 Non-Cacheable Unit (NCU) is responsible for processing noncacheable load and store operations, word and doubleword load and store atomic instructions (lwat, ldat, stwat, stdat), and certain other uncacheable operations such as __ tlbie__  and portions of the various __ sync__  and __ ptesync__  instructions. One NCU unit is instantiated per SMT4-core resource.
![Pasted image 20230909194305.png](/assets/images/power/Pasted image 20230909194305.png)
The Power10 NCU provides one dedicated cache-inhibited load station (LDS) per thread to process cache inhibited loads and load word or doubleword atomics (lwat, ldat). Cache-inhibited loads (whether guarded or not) and load atomics are neither gathered nor are they reordered in the Power10 implementation.
For cache-inhibited stores and store word and doubleword atomics (stwat, stdat), a store queue (STQ) consisting of sixteen 64-byte store gather stations is provided. The store gather stations are shared across the four core threads and hardware prevents any thread from blocking other threads in the store queue. A pair of 64-byte stations can “pair” together to gather up to 128 bytes.
The Power10 NCU supports gathering and reordering for cache-inhibited stores in the unguarded caching inhibited (IG = ‘10’) space. In caching-inhibited, but guarded space (IG = ‘11’), cache-inhibited stores are neither reordered nor gathered as required by the architecture. Similarly, atomic word and doubleword stores (stwat, stdat) are never gathered, but might be re-ordered.
The Power10 NCU provides eight store address machines (SAM) that manage the address tenure of the store allowing for up to eight outstanding cache-inhibited or store atomic word or doubleword instructions (stwat, stdat).
Finally, the NCU provides eight snoop queues (TLBS) to process snooped TLBIE operations and four snoop queues (SLBS) to process SLBIE operations and forward these to the core.

## Memory Controller
The Power10 memory controller unit (MCU) provides the system memory interface between the on-chip SMP interconnect fabric and the OpenCAPI memory interface (OMI) links. These OMI links can be attached to external memory buffer chips that conform to the OpenCAPI 3.1 specification (Memory Interface Class only).

The buffer chips, in turn, directly connect to industry standard memory DIMM interfaces or other memory media (such as, storage-class memory). Each memory channel supports two OMI links. Physically, the MCUs are grouped into four instances of an extended memory OMI (EMO) unit. Each EMO unit contains two MCU channels. The MCUs process 128-byte read requests and 64-byte or 128-byte write requests from processor cores, caches, and I/O host bridges; 1 - 128-byte partial-line writes; and atomic memory operations (AMOs). The MCU also handles address-only operations for the purpose of address protection, acting as the lowest-point of coherency (LPC).

The eight MCUs on the chip can be configured into one or more address interleave groups. Within each group, the address space is divided into portions, such that each sequential portion is handled by a different MCU in a round-robin fashion. The maximum memory addressing per Power10 chip is 128 TB.
Within a single MCU channel, the two OMI sub-channels are always address interleaved on a 128-byte boundary (assuming both sub-channels are populated with memory).
below is the Power10 System Memory High-Level Diagram
![Pasted image 20230909195300.png](/assets/images/power/Pasted image 20230909195300.png)
To improve memory RAS for large systems, the memory controller supports:
* Selective Memory Mirroring - In this configuration, memory sub-channels are grouped into mirrored pairs. Separate mirrored and non-mirrored BAR registers enable memory access to be targeted to either mirrored or non-mirrored space
* Whole Memory Encryption - The memory controller supports Advanced Encryption Standard (AES) encryption/decryption of all traffic to system memory. Encryption is enabled via configuration bits accessible to firmware. Accesses to OMI configuration and MMIO spaces are never encrypted, because they are not part of the system memory media. Other than that, all traffic to system memory is encrypted (if enabled) or not.


## On-Chip Accelerators
The Nest Accelerator unit (NX) consists of cryptographic and memory compression/decompression engines(coprocessors) with support hardware. Below figure shows a block diagram of the NX.
![Pasted image 20230911083955.png](/assets/images/power/Pasted image 20230911083955.png)
To support coprocessor invocation by user code, use of effective addresses, high-bandwidth storage accesses, and interrupt notification of job completion, NX includes the following support hardware:
* SMP interconnect unit (SIU)
	* Interfaces to SMP interconnect and direct memory access (DMA) controller, Provides 16-bytes per cycle data bandwidth per direction to both
	* Employs SMP interconnect common queue (SICQ) multiple parallel read and write machine architecture to maximize bandwidth
	* User-mode access control (UMAC) coprocessor invocation block
		* After the Virtual Accelerator Switchboard (VAS) accepts a CRB that was initiated by a copy/paste instruction, the UMAC snoops the VAS’s notification for an available coprocessor request block (CRB or job).
		* Supports one high- and one low-priority queue per coprocessor type
		* Retrieves CRBs from queues and dispatches CRBs to the DMA controller
	* Effective-to-real address translation (ERAT) table stores 32 recently used translations
* DMA Controller - Decodes CRB to initiate coprocessor and move data on behalf of coprocessors

below figure shows the Flow for NX Invocation through the VAS.
![Pasted image 20230911090237.png](/assets/images/power/Pasted image 20230911090237.png)
1. After a send window has been established, the user process can begin using the NX accelerator. First, it must create a coprocessor request block (CRB). The NX specification defines the format of the CRB. This CRB is sent to the VAS unit by using the Power10 copy/paste instructions. The copy instruction places the CRB into the copy buffer. The user process then issues a paste instruction using the effective address given by the operating system during send window creation to store the copy data to the VAS. The copy data contains the 128-byte CRB. The effective address is translated to a real address by translation hardware in the core. The store to the real address is issued to the SMP interconnect as a remote memory access write (RMA_write) command and has the send window identifier embedded within the real address. The 128-byte RMA_write payload (the CRB) is stored into one of 64 VAS data buffers. The VAS has the ability to hold 128 unique window contexts. Upon snooping the RMA_write, the VAS uses the send window identifier to fetch the Send Window Table Entry from memory if not already resident with the VAS window cache logic.
2. The VAS reads the Receive Window Identifier field in the send window context to determine which receive window the send window from the RMA_write points to. Each NX coprocessor type (CT) has a unique receive window corresponding to a unique FIFO for each of the accelerators. If the receive window is not cached, it will be fetched from memory.
3. Using the FIFO address from the receive window context, VAS stores the RMA_write payload to memory, thereby placing the CRB onto the NX accelerator FIFO. VAS stamps or overlays a portion of the CRB with the send and receive window identifiers. NX uses this information when processing the CRB. In particular, the send window identifier in the CRB is used by NX to fetch the send window and obtain translation information for the addresses contained within the CRB. The receive FIFOs are implemented as circular queues. After reaching the end of the FIFO, VAS wraps back to the beginning of the FIFO and writes the next entry.
4. After writing the CRB to the FIFO, VAS sends an ASB_notify command on the SMP interconnect. The ASB_notify contains a logical partition identifier (LPID), process identifier (PID), and thread identifier (TID).
5. Each NX FIFO has a particular LPID:PID:TID combination associated with it. When NX snoops an ASB_notify that matches its programmed LPID:PID:TID, it increments the corresponding counter for the associated FIFO, indicating a new work item has been placed on the accelerator FIFO.
6. When an NX CT queue is empty and its counter is nonzero, NX reads the next CRB from the receive FIFO. As soon as the CRB is read from the FIFO, NX does a memory mapped (MMIO) store to the VAS unit to return a credit. VAS ensures that the receive FIFO does not overflow by managing credits. The hypervisor initializes the receive window with credits equal to the number of CRBs that can be stored to the receive FIFO based on the size of the FIFO. VAS decrements the receive credit count when it stores a CRB to the receive FIFO and increments the count when NX returns a credit via MMIO store after NX pulls the CRB off of the FIFO. NX uses the stamped information from the CRB to read the send window context from memory and decrements its internal counter.
7. NX dispatches the job to the associated CT, which can have multiple acceleration engines, and executes the CRB.
8. Upon completion of the job, NX returns a send window credit to VAS via an MMIO store. Each send window, when created by the hypervisor, is assigned a number of send credits. This allows the hypervisor to implement quality of service by managing numerous users sharing the same accelerator resource, and preventing one process from using more than its share. When an RMA_write command is received by VAS, VAS decrements the send credit for the associated send window. VAS increments the count when NX completes the CRB and returns a send credit with an MMIO store.
9. NX writes a __ coprocessor status block (CSB)__  and can optionally send an interrupt, which notifies the user that the job has completed. NX also updates the __ accelerator processed byte count (XPBC)__  in the send window indicating the number of bytes that were processed on behalf of the user.

## OpenCAPI Processing in the POWERAccel Unit


## Nest MMU

![Pasted image 20230911094531.png](/assets/images/power/Pasted image 20230911094531.png)

## Interrupt Controller
The Power10 interrupt controller (INT) consists of three major units:
* the virtualization controller (P3VC)
* the presentation controller (P3PC)
* the Power10 Fabric bus interface common queue (P3CQ).
These units work together to take triggers from interrupt sources and deliver exceptions to the appropriate processor thread. This section provides an overview of the interrupt architecture, describes the INT units and their interfaces, and also describes how they operate with the interrupt sources and software in the Power10 infrastructure.
The high-level diagram depicts the conceptual interaction among sources and the controller blocks in interrupt signaling and notification. The individual elements are interconnected and communicate via the Power10 Fabric bus.
* The P3VC receives notification triggers from interrupt source controllers (P3SCs) via a Power10 Fabric bus store operation (for example, a cache-inhibited write: ci_wr). It processes the notification using information contained in the event assignment entry (EAE) that is located in main memory and associated with the specific trigger. This processing can include updating an event queue entry and then forwarding the notification to the P3PC, which signals an exception to one of the processor threads. The P3VC also handles notification redistribution if a state change to the assigned processor thread preclude it from handling the interrupt or notification escalation, if there is no processor thread that is currently capable of handling the interrupt.
* The P3PC has an exception bus towards the cores to notify the individual processor threads. Three signals are created for each thread, one to generate hypervisor interrupts, another to generate operating-system interrupts, and a third to generate an event-based branch. Associated with each of the exception-notification signals in the P3PC is prioritization and exception-queuing logic that prevents less favored events from preempting more favored ones or from loss due to dropping an event. Associated with each of the exception notification wires is one or more logical server numbers stored in CAM-like lines. This structure is also referred to as the thread interrupt management area (TIMA). These logical server numbers identify which software entities are currently dispatched on the specific physical processor thread. When the P3VC issues fabric bus operations to route an event notification, these CAM-like lines are searched to identify candidate processor threads. In addition to the CAM-like lines, priority and exception-queuing logic mentioned previously, each interrupt-generating exception has logic to track how much interrupt work has been handled by the associated processor thread. This information is used to evenly distribute interrupt processing load among the candidates.
* The P3CQ serves as the Power10 Fabric bus interface controller between the interrupt logic and the rest of the Power10 chip. This unit is responsible for sequencing the appropriate fabric bus protocol when the interrupt controller drives or receives commands. It performs compares to determine if the interrupt controller is the destination of a command (for example, a store operation used for an interrupt trigger). It is also responsible for driving the fabric bus histogram, poll, and assign commands to find the correct presentation controller for an interrupt trigger. Another key P3CQ function is sending and receiving the AIB interface to the virtualization and presentation controllers.
![Pasted image 20230911102017.png](/assets/images/power/Pasted image 20230911102017.png)

# 13. 参考文献
1. Power ISA (Version 3.1B), 2021.
2. R. R. Oehler and R. D. Groves, "IBM RISC System/6000 processor architecture," in IBM Journal of Research and Development, vol. 34, no. 1, pp. 23-36, Jan. 1990, doi: 10.1147/rd.341.0023.
3. S. W. White and S. Dhawan. 1994. POWER2: next generation of the RISC System/6000 family. IBM J. Res. Dev. 38, 5 (Sept. 1994), 493–502. https://doi.org/10.1147/rd.385.0493
4. F. P. O'Connell and S. W. White, "POWER3: The next generation of PowerPC processors," in IBM Journal of Research and Development, vol. 44, no. 6, pp. 873-884, Nov. 2000, doi: 10.1147/rd.446.0873.
5. pSeries 690 Service Guide, n.d.
6. Andersson, S., Bell, R., Hague, J., Holthoff, H., Mayes, P., Nakano, J., Shieh, D., Tuccillo, J., n.d. RS/6000 Scientific and Technical Computing: POWER3 Introduction and Tuning Guide.
7. J. M. Tendler, J. S. Dodson, J. S. Fields, H. Le and B. Sinharoy, "POWER4 system microarchitecture," in IBM Journal of Research and Development, vol. 46, no. 1, pp. 5-25, Jan. 2002, doi: 10.1147/rd.461.0005.
8. Bossen, D.C., Kitamorn, A., Reick, K.F., Floyd, M.S., 2002. Fault-tolerant design of the IBM pSeries 690 system using POWER4 processor technology. IBM J. Res. & Dev. 46, 77–86. [https://doi.org/10.1147/rd.461.0077](https://doi.org/10.1147/rd.461.0077)
9. B. Sinharoy, R. N. Kalla, J. M. Tendler, R. J. Eickemeyer and J. B. Joyner, "POWER5 system microarchitecture," in IBM Journal of Research and Development, vol. 49, no. 4.5, pp. 505-521, July 2005, doi: 10.1147/rd.494.0505.
10. Kalla, R., 2003. IBM’s POWER5 Microprocessor Design and Methodology.
11. Kalla, R., Sinharoy, B., Tendler, J.M., 2004. IBM power5 chip: a dual-core multithreaded processor. IEEE Micro 24, 40–47. [https://doi.org/10.1109/MM.2004.1289290](https://doi.org/10.1109/MM.2004.1289290)
12. Clabes, J., Friedrich, J., Sweet, M., DiLullo, J., Chu, S., Plass, D., Dawson, J., Muench, P., Powell, L., Floyd, M., Sinharoy, B., Lee, M., Goulet, M., Wagoner, J., Schwartz, N., Runyon, S., Gorman, G., n.d. Design and Implementation of the POWER5 TM Microprocessor.
13. Jiménez, V., Cazorla, F.J., Gioiosa, R., Valero, M., Boneti, C., Kursun, E., Cher, C.-Y., Isci, C., Buyuktosunoglu, A., Bose, P., 2010. Power and thermal characterization of POWER6 system, in: Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques. Presented at the PACT ’10: International Conference on Parallel Architectures and Compilation Techniques, ACM, Vienna Austria, pp. 7–18. [https://doi.org/10.1145/1854273.1854281](https://doi.org/10.1145/1854273.1854281)
14. H. Q. Le et al., "IBM POWER6 microarchitecture," in IBM Journal of Research and Development, vol. 51, no. 6, pp. 639-662, Nov. 2007, doi: 10.1147/rd.516.0639.
15. IBM Power 570 and IBM Power 595 (POWER6) System Builder, n.d.
16. Kalla, R., Sinharoy, B., Starke, W.J., Floyd, M., 2010. Power7: IBM’s Next-Generation Server Processor. IEEE Micro 30, 7–15. [https://doi.org/10.1109/MM.2010.38](https://doi.org/10.1109/MM.2010.38)
17. B. Sinharoy et al., "IBM POWER7 multicore server processor," in IBM Journal of Research and Development, vol. 55, no. 3, pp. 1:1-1:29, May-June 2011, doi: 10.1147/JRD.2011.2127330.
18. POWER7 and POWER7+ Optimization and Tuning Guide, n.d.
19. B. Sinharoy et al., "IBM POWER8 processor core microarchitecture," in IBM Journal of Research and Development, vol. 59, no. 1, pp. 2:1-2:21, Jan.-Feb. 2015, doi: 10.1147/JRD.2014.2376112.
20. Caldeira, A., Kahle, M.-E., Saverimuthu, G., Vearner, K.C., n.d. IBM Power Systems S812LC Technical Overview and Introduction.
21. IBM Power System E980: Technical Overview and Introduction, n.d.
22. IBM Power System S822: Technical Overview and Introduction, n.d.
23. S. K. Sadasivam, B. W. Thompto, R. Kalla and W. J. Starke, "IBM Power9 Processor Architecture," in IEEE Micro, vol. 37, no. 2, pp. 40-51, Mar.-Apr. 2017, doi: 10.1109/MM.2017.40.
24. POWER9 Processor User’s Manual, 2019.
25. W. J. Starke, B. W. Thompto, J. A. Stuecheli and J. E. Moreira, "IBM's POWER10 Processor," in IEEE Micro, vol. 41, no. 2, pp. 7-14, March-April 2021, doi: 10.1109/MM.2021.3058632.
26. Power10 Processor Chip User’s Manual, 2021.
27. IBM Power E1050: Technical Overview and Introduction, n.d.
28. IBM Power S1014, S1022s, S1022, and S1024 Technical Overview and Introduction, n.d.